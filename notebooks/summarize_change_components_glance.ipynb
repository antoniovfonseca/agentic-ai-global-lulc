{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antoniovfonseca/agentic-ai-global-lulc/blob/main/notebooks/summarize_change_components_glance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmXZbuvE7iH2"
      },
      "source": [
        "# **Change Component Analysis (CCA)**\n",
        "---\n",
        "Author: Robert Gilmore Pontius Jr, Antonio Fonseca\n",
        "\n",
        "Institution: Clark University\n",
        "\n",
        "Email: antfonseca@clarku.edu\n",
        "\n",
        "Version: 3.0\n",
        "\n",
        "Updates:\n",
        "\n",
        "This version includes the following improvements:\n",
        "\n",
        "1.   Using windows\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NAsFaVLqTn4"
      },
      "source": [
        "## **1.Environment Setup**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QC87SS9LXyFS"
      },
      "source": [
        "### **1.1 Install Python Libraries**\n",
        "Run the cell below only if using Google Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z_MVe6_iWL5i",
        "outputId": "648f8170-5d1c-448c-8fd0-1ed35eb65d61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.4/63.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip -q install \\\n",
        "    rasterio \\\n",
        "    seaborn \\\n",
        "    xlsxwriter \\\n",
        "    matplotlib-scalebar \\\n",
        "    matplotlib-map-utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKgJK38S0XNi"
      },
      "source": [
        "### **1.2 Importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0ZpGgIxg8ZxP"
      },
      "outputs": [],
      "source": [
        "# Standard library\n",
        "import glob\n",
        "import os\n",
        "import re\n",
        "import pickle\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "from pathlib import Path\n",
        "\n",
        "# Typing for annotations\n",
        "from typing import Dict, List, Optional, Iterable, Tuple\n",
        "\n",
        "# Third-party\n",
        "import numba as nb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import rasterio\n",
        "import xlsxwriter\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import matplotlib.ticker as ticker\n",
        "import matplotlib.ticker as mticker\n",
        "from numba import prange\n",
        "from pyproj import Transformer\n",
        "from pyproj import CRS, Geod, Transformer\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Rasterio submodules\n",
        "from rasterio.plot import show\n",
        "from rasterio.mask import mask\n",
        "from rasterio.enums import Resampling\n",
        "from rasterio.transform import from_origin\n",
        "\n",
        "# Matplotlib extensions\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "from matplotlib_scalebar.scalebar import ScaleBar\n",
        "from matplotlib_map_utils import north_arrow\n",
        "from matplotlib.colors import (\n",
        "    ListedColormap,\n",
        "    BoundaryNorm,\n",
        "    Normalize,\n",
        "    LinearSegmentedColormap,\n",
        ")\n",
        "from matplotlib.patches import Patch, Rectangle, FancyArrowPatch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IP8buIeKZL-1"
      },
      "source": [
        "### **1.3 Mounting Google Drive in Colab**\n",
        "Run the cell below only if using Google Colab.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JUJHwe8pnXon",
        "outputId": "ae9b005a-3e64-46e7-f0f5-8ca941bc6eae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtzprFSP4D8N"
      },
      "source": [
        "## **2.Data Preparation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ8HWNifZdu5"
      },
      "source": [
        "### **2.1 Setting Paths to Image Files**\n",
        "\n",
        "The user must include the year in the raster map name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yS4eUbHsKggH",
        "outputId": "1d091bec-859e-4768-ee50-80953f022413",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All input files found.\n"
          ]
        }
      ],
      "source": [
        "# List of input raster paths\n",
        "image_paths = [\n",
        "    r\"/content/drive/MyDrive/Dataset/ecoregions/raster/glance/ecoregion567_glance_2001.tif\",\n",
        "    # r\"/content/drive/MyDrive/Dataset/ecoregions/raster/glance/ecoregion567_glance_2002.tif\",\n",
        "    # r\"/content/drive/MyDrive/Dataset/ecoregions/raster/glance/ecoregion567_glance_2003.tif\",\n",
        "    # r\"/content/drive/MyDrive/Dataset/ecoregions/raster/glance/ecoregion567_glance_2004.tif\",\n",
        "    # r\"/content/drive/MyDrive/Dataset/ecoregions/raster/glance/ecoregion567_glance_2005.tif\",\n",
        "    # r\"/content/drive/MyDrive/Dataset/ecoregions/raster/glance/ecoregion567_glance_2006.tif\",\n",
        "    # r\"/content/drive/MyDrive/Dataset/ecoregions/raster/glance/ecoregion567_glance_2007.tif\",\n",
        "    # r\"/content/drive/MyDrive/Dataset/ecoregions/raster/glance/ecoregion567_glance_2008.tif\",\n",
        "    # r\"/content/drive/MyDrive/Dataset/ecoregions/raster/glance/ecoregion567_glance_2009.tif\",\n",
        "    r\"/content/drive/MyDrive/Dataset/ecoregions/raster/glance/ecoregion567_glance_2010.tif\",\n",
        "    # r\"/content/drive/MyDrive/Dataset/ecoregions/raster/glance/ecoregion567_glance_2011.tif\",\n",
        "    # r\"/content/drive/MyDrive/Dataset/ecoregions/raster/glance/ecoregion567_glance_2012.tif\",\n",
        "    # r\"/content/drive/MyDrive/Dataset/ecoregions/raster/glance/ecoregion567_glance_2013.tif\",\n",
        "    # r\"/content/drive/MyDrive/Dataset/ecoregions/raster/glance/ecoregion567_glance_2014.tif\",\n",
        "    # r\"/content/drive/MyDrive/Dataset/ecoregions/raster/glance/ecoregion567_glance_2015.tif\",\n",
        "    # r\"/content/drive/MyDrive/Dataset/ecoregions/raster/glance/ecoregion567_glance_2016.tif\",\n",
        "    # r\"/content/drive/MyDrive/Dataset/ecoregions/raster/glance/ecoregion567_glance_2017.tif\",\n",
        "    # r\"/content/drive/MyDrive/Dataset/ecoregions/raster/glance/ecoregion567_glance_2018.tif\",\n",
        "    r\"/content/drive/MyDrive/Dataset/ecoregions/raster/glance/ecoregion567_glance_2019.tif\"\n",
        "]\n",
        "\n",
        "# Set no data value\n",
        "noData_value = 255\n",
        "\n",
        "# Check if all files exist\n",
        "missing_files = [f for f in image_paths if not os.path.exists(f)]\n",
        "\n",
        "if missing_files:\n",
        "    print(\"Missing files:\")\n",
        "    for f in missing_files:\n",
        "        print(f\" - {f}\")\n",
        "else:\n",
        "    print(\"All input files found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJTmYUaEuZzL"
      },
      "source": [
        "### **2.2 Setting Path to Output Files**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "h86vO8_FKggH",
        "outputId": "55a1c3aa-e0f2-4761-f72a-84b36e87bee5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder already exists: /content/drive/MyDrive/Assessments/cca/output/ecoregion567_glance/nativa_matrixa\n"
          ]
        }
      ],
      "source": [
        "# Output directory path\n",
        "output_path = r\"/content/drive/MyDrive/Assessments/cca/output/ecoregion567_glance/nativa_matrixa\"\n",
        "\n",
        "# Create folder if it does not exist\n",
        "if not os.path.exists(output_path):\n",
        "    os.makedirs(output_path)\n",
        "    print(\"Folder created:\", output_path)\n",
        "else:\n",
        "    print(\"Folder already exists:\", output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qx9wLzLCnRUC"
      },
      "source": [
        "### **2.3 Setting Time Points**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Be002lWxy_Ov"
      },
      "outputs": [],
      "source": [
        "# Years of input rasters\n",
        "# years = list(range(2001, 2020))\n",
        "years = [2001, 2010, 2019]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVsegVxtEvuu"
      },
      "source": [
        "### **2.4 Setting Classes Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GfUfN_BTKggH"
      },
      "outputs": [],
      "source": [
        "# Class ID, names, and colors\n",
        "class_labels_dict = {\n",
        "\n",
        "    6: {\"name\": \"Shrublands\",  \"color\": \"#28A765\"},\n",
        "    5: {\"name\": \"Tree Cover\",  \"color\": \"#336600\"},\n",
        "    2: {\"name\": \"Snow/Ice\",    \"color\": \"#FFFFFF\"},\n",
        "    3: {\"name\": \"Developed\",   \"color\": \"#FF0000\"},\n",
        "    4: {\"name\": \"Bare\",        \"color\": \"#68666B\"},\n",
        "    1: {\"name\": \"Water\",       \"color\": \"#386CB0\"},\n",
        "    7: {\"name\": \"Herbaceous\",  \"color\": \"#FFFF99\"}\n",
        "}\n",
        "\n",
        "# Sorted list of renamed class labels\n",
        "class_labels = [class_labels_dict[key][\"name\"] for key in sorted(class_labels_dict.keys())]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pm9M40oKdQ4"
      },
      "source": [
        "### **2.5 Display the Maps**\n",
        "\n",
        "This cell defines and executes a function to visualize a temporal series of classified rasters in a multi-panel grid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHbyNHcavJ9j"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import rasterio\n",
        "from rasterio.enums import Resampling\n",
        "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
        "from matplotlib.patches import Rectangle\n",
        "from matplotlib_scalebar.scalebar import ScaleBar\n",
        "from pyproj import Transformer, Geod\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "def compute_display_pixel_size_km(\n",
        "    raster_path: str,\n",
        "    downsample_divisor: float,\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Compute horizontal resolution in kilometers per displayed pixel.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    raster_path : str\n",
        "        Path to a raster file used to derive spatial extent and CRS.\n",
        "    downsample_divisor : float\n",
        "        Factor used to downsample the raster width for display.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        Pixel size in kilometers for the downsampled display grid.\n",
        "    \"\"\"\n",
        "    with rasterio.open(raster_path) as src:\n",
        "        left, bottom, right, top = src.bounds\n",
        "        lat_mid_src = (top + bottom) / 2.0\n",
        "\n",
        "        to_ll = Transformer.from_crs(\n",
        "            src.crs,\n",
        "            \"EPSG:4326\",\n",
        "            always_xy=True,\n",
        "        )\n",
        "        lon_l, lat_mid = to_ll.transform(\n",
        "            left,\n",
        "            lat_mid_src,\n",
        "        )\n",
        "        lon_r, _ = to_ll.transform(\n",
        "            right,\n",
        "            lat_mid_src,\n",
        "        )\n",
        "\n",
        "        geod = Geod(\n",
        "            ellps=\"WGS84\",\n",
        "        )\n",
        "        _, _, width_m = geod.inv(\n",
        "            lon_l,\n",
        "            lat_mid,\n",
        "            lon_r,\n",
        "            lat_mid,\n",
        "        )\n",
        "\n",
        "        # Ensure we don't divide by zero and handle floats\n",
        "        cols_disp = max(\n",
        "            1.0,\n",
        "            src.width / downsample_divisor,\n",
        "        )\n",
        "\n",
        "        return (width_m / cols_disp) / 1_000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4N-1FeLy_Ov"
      },
      "outputs": [],
      "source": [
        "def plot_classified_images(\n",
        "    class_map: Dict[int, Dict[str, str]],\n",
        "    years: List[int],\n",
        "    output_path: str,\n",
        "    image_paths_arg: List[str],\n",
        "    nodata_val: int,\n",
        "    max_display_size: int = 1500, # Replaced fixed divisor with dynamic max size\n",
        "    panel_size: tuple = (4.0, 6.0),\n",
        "    dx_km: Optional[float] = None,\n",
        "    resampling_method: Resampling = Resampling.nearest,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Plot classified rasters over time with legend, north arrow, and scale bar,\n",
        "    using dynamic downsampling to prevent out-of-memory errors.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    class_map : Dict[int, Dict[str, str]]\n",
        "        Mapping from class ID to metadata with at least\n",
        "        ``{\"name\": str, \"color\": str}`` for each class.\n",
        "    years : List[int]\n",
        "        List of years corresponding to each classified raster.\n",
        "    output_path : str\n",
        "        Directory where input rasters are stored and the figure is saved.\n",
        "    image_paths_arg : List[str]\n",
        "        List of paths to the raster files.\n",
        "    nodata_val : int\n",
        "        Value representing NoData pixels.\n",
        "    max_display_size : int, optional\n",
        "        Maximum width/height in pixels for each panel. Larger rasters will be\n",
        "        downsampled dynamically to this size to save RAM. Default is 1500.\n",
        "    panel_size : tuple, optional\n",
        "        Width and height in inches for a single panel, by default (4.0, 6.0).\n",
        "    dx_km : float, optional\n",
        "        Pixel size in kilometers for the scale bar. Computed if None.\n",
        "    resampling_method : Resampling, optional\n",
        "        Resampling method used when reading rasters, by default\n",
        "        ``Resampling.nearest``.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "        This function saves a multi-panel figure to disk and shows the plot.\n",
        "    \"\"\"\n",
        "    if not image_paths_arg:\n",
        "        raise ValueError(\"No image paths provided.\")\n",
        "\n",
        "    image_paths = sorted(image_paths_arg)\n",
        "\n",
        "    # 1. Calculate Grid and Dynamic Scale Factor\n",
        "    n_images = len(image_paths)\n",
        "    max_cols = 10\n",
        "    ncols = min(\n",
        "        max_cols,\n",
        "        n_images,\n",
        "    )\n",
        "    nrows = math.ceil(n_images / max_cols)\n",
        "\n",
        "    # Calculate dynamic downsampling based on the first image\n",
        "    with rasterio.open(image_paths[0]) as src:\n",
        "        max_dim = max(src.width, src.height)\n",
        "        # If image is smaller than max size, scale_factor is 1 (no scaling)\n",
        "        scale_factor = max(1.0, max_dim / max_display_size)\n",
        "\n",
        "        # Extent is needed so Matplotlib plots using real map coordinates\n",
        "        left, bottom, right, top = src.bounds\n",
        "        extent = [left, right, bottom, top]\n",
        "\n",
        "    # Derive km per displayed pixel using the calculated scale factor\n",
        "    if dx_km is None:\n",
        "        dx_km = compute_display_pixel_size_km(\n",
        "            raster_path=image_paths[0],\n",
        "            downsample_divisor=scale_factor,\n",
        "        )\n",
        "\n",
        "    # 2. Setup Figure\n",
        "    fig, axs = plt.subplots(\n",
        "        nrows,\n",
        "        ncols,\n",
        "        figsize=(\n",
        "            panel_size[0] * ncols,\n",
        "            panel_size[1] * nrows,\n",
        "        ),\n",
        "        sharey=True,\n",
        "        sharex=True, # Share X too since bounds are identical\n",
        "        constrained_layout=False,\n",
        "    )\n",
        "\n",
        "    plt.subplots_adjust(\n",
        "        left=0.02,\n",
        "        right=0.85,\n",
        "        top=0.95,\n",
        "        bottom=0.05,\n",
        "        wspace=0.04,\n",
        "        hspace=0.04,\n",
        "    )\n",
        "\n",
        "    if isinstance(axs, np.ndarray):\n",
        "        axes = axs.ravel()\n",
        "    else:\n",
        "        axes = [axs]\n",
        "\n",
        "    # 3. Setup Colormap\n",
        "    class_ids_sorted = sorted(class_map.keys())\n",
        "    cmap = ListedColormap(\n",
        "        [\n",
        "            class_map[k][\"color\"]\n",
        "            for k in class_ids_sorted\n",
        "        ],\n",
        "    )\n",
        "    norm = BoundaryNorm(\n",
        "        class_ids_sorted + [class_ids_sorted[-1] + 1],\n",
        "        cmap.N,\n",
        "    )\n",
        "\n",
        "    # 4. Read and Plot Data\n",
        "    print(f\"Plotting {n_images} maps. Dynamic downsample factor: {scale_factor:.2f}\")\n",
        "\n",
        "    for i, (path, year) in enumerate(\n",
        "        zip(\n",
        "            image_paths,\n",
        "            years,\n",
        "        ),\n",
        "    ):\n",
        "        ax = axes[i]\n",
        "\n",
        "        with rasterio.open(path) as src:\n",
        "            # Calculate new shape safely\n",
        "            new_height = max(1, int(src.height / scale_factor))\n",
        "            new_width = max(1, int(src.width / scale_factor))\n",
        "\n",
        "            # Read and resample on the fly\n",
        "            data = src.read(\n",
        "                1,\n",
        "                out_shape=(\n",
        "                    new_height,\n",
        "                    new_width,\n",
        "                ),\n",
        "                resampling=resampling_method,\n",
        "            )\n",
        "\n",
        "        # Apply NoData mask\n",
        "        data = np.ma.masked_where(data == nodata_val, data)\n",
        "\n",
        "        ax.imshow(\n",
        "            data,\n",
        "            cmap=cmap,\n",
        "            norm=norm,\n",
        "            interpolation=\"nearest\"\n",
        "        )\n",
        "        ax.set_title(\n",
        "            f\"{year}\",\n",
        "            fontweight=\"bold\",\n",
        "            fontsize=24,\n",
        "        )\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "    # Disable unused axes\n",
        "    for j in range(\n",
        "        n_images,\n",
        "        len(axes),\n",
        "    ):\n",
        "        axes[j].axis(\"off\")\n",
        "\n",
        "    # 5. Add Legend to the last valid axis\n",
        "    last_ax = axes[n_images - 1]\n",
        "\n",
        "    legend_elements = [\n",
        "        Rectangle(\n",
        "            (0, 0),\n",
        "            1,\n",
        "            1,\n",
        "            color=class_map[k][\"color\"],\n",
        "            label=class_map[k][\"name\"],\n",
        "        )\n",
        "        for k in class_map.keys()\n",
        "    ]\n",
        "\n",
        "    last_ax.legend(\n",
        "        handles=legend_elements,\n",
        "        loc=\"center left\",\n",
        "        bbox_to_anchor=(\n",
        "            1.02,\n",
        "            0.5,\n",
        "        ),\n",
        "        frameon=False,\n",
        "        fontsize=18,\n",
        "    )\n",
        "\n",
        "    # 6. Add Scale Bar\n",
        "    scalebar = ScaleBar(\n",
        "        dx=dx_km,\n",
        "        units=\"km\",\n",
        "        length_fraction=0.35,\n",
        "        location=\"lower right\",\n",
        "        scale_loc=\"bottom\",\n",
        "        color=\"black\",\n",
        "        box_alpha=0,\n",
        "    )\n",
        "    last_ax.add_artist(\n",
        "        scalebar,\n",
        "    )\n",
        "\n",
        "    # 7. Add North Arrow\n",
        "    try:\n",
        "        north_arrow(\n",
        "            last_ax,\n",
        "            location=\"upper left\",\n",
        "            shadow=False,\n",
        "            rotation={\n",
        "                \"degrees\": 0,\n",
        "            },\n",
        "        )\n",
        "    except NameError:\n",
        "        print(\"Note: north_arrow function not found, skipping north arrow.\")\n",
        "\n",
        "    # 8. Save Figure\n",
        "    out_fig = os.path.join(\n",
        "        output_path,\n",
        "        \"map_panel_input_maps.png\",\n",
        "    )\n",
        "    plt.savefig(\n",
        "        out_fig,\n",
        "        format=\"png\",\n",
        "        bbox_inches=\"tight\",\n",
        "        dpi=300,\n",
        "    )\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "# Execution of the plotting function with the defined parameters\n",
        "plot_classified_images(\n",
        "    class_map=class_labels_dict,\n",
        "    years=years,\n",
        "    output_path=output_path,\n",
        "    image_paths_arg=image_paths,\n",
        "    nodata_val=noData_value\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ut1boaFFKggH"
      },
      "source": [
        "## **3.Class distribution over time**\n",
        "\n",
        "\n",
        "---\n",
        "This cell processes raster data to calculate pixel counts per class over time, generating a stacked bar chart and exporting the statistical results to both a pivot table and a CSV file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAjNGqVyj_IY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import rasterio\n",
        "\n",
        "# 1. Processing and Exporting Function\n",
        "def process_and_export_pixel_counts(\n",
        "    image_paths: list[str],\n",
        "    years: list[int],\n",
        "    class_labels_dict: dict,\n",
        "    output_dir: str,\n",
        "    no_data_value: int,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Process raster images to count pixels per class and export results to CSV.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    image_paths : list[str]\n",
        "        List of file paths to the raster images.\n",
        "    years : list[int]\n",
        "        List of years corresponding to the images.\n",
        "    class_labels_dict : dict\n",
        "        Dictionary mapping class IDs to metadata (name and color).\n",
        "    output_dir : str\n",
        "        Directory path where the CSV will be saved.\n",
        "    no_data_value : int\n",
        "        Pixel value to be treated as NoData.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        The pivot table containing pixel counts per year and class.\n",
        "    \"\"\"\n",
        "    # 1.1. Validate that input lengths match\n",
        "    if len(image_paths) != len(years):\n",
        "        raise ValueError(\n",
        "            f\"Input mismatch: {len(image_paths)} images vs {len(years)} years.\",\n",
        "        )\n",
        "\n",
        "    records: list[dict] = []\n",
        "\n",
        "    # 1.2. Iterate through each year and corresponding image path\n",
        "    print(\n",
        "        \"Counting pixels per class...\",\n",
        "    )\n",
        "    for year, path in zip(\n",
        "        years,\n",
        "        image_paths,\n",
        "    ):\n",
        "        # 1.3. Read the raster data\n",
        "        with rasterio.open(\n",
        "            path,\n",
        "        ) as src:\n",
        "            data = src.read(\n",
        "                1,\n",
        "            )\n",
        "\n",
        "        # 1.4. Count unique pixel values\n",
        "        values, counts = np.unique(\n",
        "            data,\n",
        "            return_counts=True,\n",
        "        )\n",
        "\n",
        "        # 1.5. Process counts and map to class names\n",
        "        for value, count in zip(\n",
        "            values,\n",
        "            counts,\n",
        "        ):\n",
        "            val_int = int(\n",
        "                value,\n",
        "            )\n",
        "\n",
        "            # Filter out NoData values\n",
        "            if val_int == no_data_value:\n",
        "                continue\n",
        "\n",
        "            # Skip classes not defined in the dictionary\n",
        "            if val_int not in class_labels_dict:\n",
        "                continue\n",
        "\n",
        "            records.append(\n",
        "                {\n",
        "                    \"Year\": year,\n",
        "                    \"ClassID\": val_int,\n",
        "                    \"ClassName\": class_labels_dict[val_int][\"name\"],\n",
        "                    \"Pixels\": int(\n",
        "                        count,\n",
        "                    ),\n",
        "                },\n",
        "            )\n",
        "\n",
        "    # 1.6. Create DataFrame and Pivot Table\n",
        "    df_pixels = pd.DataFrame(\n",
        "        records,\n",
        "    )\n",
        "\n",
        "    pivot_pixels = (\n",
        "        df_pixels.pivot_table(\n",
        "            index=\"Year\",\n",
        "            columns=\"ClassName\",\n",
        "            values=\"Pixels\",\n",
        "            aggfunc=\"sum\",\n",
        "        )\n",
        "        .fillna(\n",
        "            0,\n",
        "        )\n",
        "        .astype(\n",
        "            int,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # 1.7. Save results to CSV\n",
        "    csv_output_path = os.path.join(\n",
        "        output_dir,\n",
        "        \"pixels_per_class_per_year.csv\",\n",
        "    )\n",
        "\n",
        "    pivot_pixels.to_csv(\n",
        "        csv_output_path,\n",
        "        index_label=\"Year\",\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"CSV file exported successfully to: {csv_output_path}\",\n",
        "    )\n",
        "\n",
        "    return pivot_pixels\n",
        "\n",
        "\n",
        "# Execution of the pixel counting and exporting function with the defined parameters\n",
        "\n",
        "# 2.1. Extract years from filenames using regex\n",
        "years_from_files = []\n",
        "for p in image_paths:\n",
        "    match = re.search(\n",
        "        r\"(\\d{4})\\.tif$\",\n",
        "        os.path.basename(\n",
        "            p,\n",
        "        ),\n",
        "    )\n",
        "    years_from_files.append(\n",
        "        int(\n",
        "            match.group(\n",
        "                1,\n",
        "            ),\n",
        "        ) if match else 0\n",
        "    )\n",
        "\n",
        "# 2.2. Run the processing and export\n",
        "df_pixel_counts = process_and_export_pixel_counts(\n",
        "    image_paths=image_paths,\n",
        "    years=years_from_files,\n",
        "    class_labels_dict=class_labels_dict,\n",
        "    output_dir=output_path,\n",
        "    no_data_value=noData_value,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkQF8bCJj_IY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "# 1. Visualization Function\n",
        "def plot_pixel_counts_from_csv(\n",
        "    csv_path: str,\n",
        "    class_labels_dict: dict,\n",
        "    output_dir: str,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Read pixel counts from CSV and generate a formatted stacked bar chart.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    csv_path : str\n",
        "        Path to the CSV file containing pixel counts.\n",
        "    class_labels_dict : dict\n",
        "        Mapping of class IDs to metadata (name and color).\n",
        "    output_dir : str\n",
        "        Directory where the plot will be saved.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "    \"\"\"\n",
        "    # 1.1. Load data from CSV\n",
        "    if not os.path.exists(\n",
        "        csv_path,\n",
        "    ):\n",
        "        raise FileNotFoundError(\n",
        "            f\"CSV file not found at: {csv_path}\",\n",
        "        )\n",
        "\n",
        "    pivot_pixels = pd.read_csv(\n",
        "        csv_path,\n",
        "        index_col=\"Year\",\n",
        "    )\n",
        "    years_array = pivot_pixels.index.values\n",
        "\n",
        "    # 1.2. Determine Y-axis scaling factor and label\n",
        "    max_val = pivot_pixels.to_numpy().max()\n",
        "\n",
        "    if max_val >= 1_000_000_000_000:\n",
        "        scale_factor = 1_000_000_000_000\n",
        "        y_label = \"Change (trillion pixels)\"\n",
        "    elif max_val >= 1_000_000_000:\n",
        "        scale_factor = 1_000_000_000\n",
        "        y_label = \"Change (billion pixels)\"\n",
        "    elif max_val >= 1_000_000:\n",
        "        scale_factor = 1_000_000\n",
        "        y_label = \"Change (million pixels)\"\n",
        "    elif max_val >= 1_000:\n",
        "        scale_factor = 1_000\n",
        "        y_label = \"Change (thousand pixels)\"\n",
        "    elif max_val >= 100:\n",
        "        scale_factor = 100\n",
        "        y_label = \"Change (hundred pixels)\"\n",
        "    else:\n",
        "        scale_factor = 1\n",
        "        y_label = \"Change (pixels)\"\n",
        "\n",
        "    pivot_scaled = pivot_pixels / scale_factor\n",
        "\n",
        "    # 1.3. Prepare color map and sorting logic\n",
        "    class_ids_plot = sorted(\n",
        "        class_labels_dict.keys(),\n",
        "    )\n",
        "\n",
        "    color_map = {\n",
        "        class_labels_dict[class_id][\"name\"]: class_labels_dict[class_id][\"color\"]\n",
        "        for class_id in class_ids_plot\n",
        "    }\n",
        "\n",
        "    # 1.4. Calculate Net Change for sorting\n",
        "    first_year = years_array[0]\n",
        "    last_year = years_array[-1]\n",
        "    net_change_per_class = (\n",
        "        pivot_scaled.loc[last_year]\n",
        "        - pivot_scaled.loc[first_year]\n",
        "    )\n",
        "\n",
        "    # Map names back to IDs for tie-breaking\n",
        "    name_to_id_map = {\n",
        "        v[\"name\"]: k\n",
        "        for k, v in class_labels_dict.items()\n",
        "    }\n",
        "\n",
        "    df_sorting = net_change_per_class.to_frame(\n",
        "        name=\"net_change\",\n",
        "    )\n",
        "    df_sorting[\"class_id\"] = df_sorting.index.map(\n",
        "        name_to_id_map,\n",
        "    )\n",
        "\n",
        "    # Sort: Net Change (Desc) then Class ID (Desc)\n",
        "    classes_for_stack = list(\n",
        "        df_sorting.sort_values(\n",
        "            by=[\n",
        "                \"net_change\",\n",
        "                \"class_id\",\n",
        "            ],\n",
        "            ascending=[\n",
        "                False,\n",
        "                False,\n",
        "            ],\n",
        "        ).index,\n",
        "    )\n",
        "\n",
        "    # Legend order: Reversed stack order\n",
        "    classes_for_legend = list(\n",
        "        reversed(\n",
        "            classes_for_stack,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # 1.5. Generate the Stacked Bar Chart\n",
        "    fig, ax = plt.subplots(\n",
        "        figsize=(\n",
        "            10,\n",
        "            6,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    x = np.arange(\n",
        "        len(\n",
        "            years_array,\n",
        "        ),\n",
        "    )\n",
        "    width = 0.9\n",
        "    base = np.zeros(\n",
        "        len(\n",
        "            years_array,\n",
        "        ),\n",
        "        dtype=float,\n",
        "    )\n",
        "    patches_by_class: dict[str, plt.Artist] = {}\n",
        "\n",
        "    for cls in classes_for_stack:\n",
        "        if cls not in pivot_scaled.columns:\n",
        "            continue\n",
        "\n",
        "        values_cls = pivot_scaled[cls].reindex(\n",
        "            years_array,\n",
        "            fill_value=0.0,\n",
        "        ).values\n",
        "\n",
        "        bars = ax.bar(\n",
        "            x,\n",
        "            values_cls,\n",
        "            bottom=base,\n",
        "            width=width,\n",
        "            label=cls,\n",
        "            color=color_map.get(\n",
        "                cls,\n",
        "                \"gray\",\n",
        "            ),\n",
        "        )\n",
        "        patches_by_class[cls] = bars[0]\n",
        "        base += values_cls\n",
        "\n",
        "    # 1.6. Configure Axes and Labels\n",
        "    ax.set_xticks(\n",
        "        x,\n",
        "    )\n",
        "    ax.set_xticklabels(\n",
        "        years_array,\n",
        "    )\n",
        "\n",
        "    # Adaptive rotation for X-axis labels\n",
        "    n_labels = len(\n",
        "        years_array,\n",
        "    )\n",
        "    if n_labels <= 7:\n",
        "        rotation = 0\n",
        "        ha = \"center\"\n",
        "    elif n_labels <= 12:\n",
        "        rotation = 45\n",
        "        ha = \"right\"\n",
        "    else:\n",
        "        rotation = 90\n",
        "        ha = \"center\"\n",
        "\n",
        "    plt.setp(\n",
        "        ax.get_xticklabels(),\n",
        "        rotation=rotation,\n",
        "        ha=ha,\n",
        "    )\n",
        "\n",
        "    ax.tick_params(\n",
        "        axis=\"both\",\n",
        "        labelsize=14,\n",
        "    )\n",
        "    ax.set_ylabel(\n",
        "        y_label,\n",
        "        fontsize=18,\n",
        "    )\n",
        "    ax.set_xlabel(\n",
        "        \"Time points\",\n",
        "        fontsize=18,\n",
        "    )\n",
        "    ax.set_title(\n",
        "        \"Number of pixels per class\",\n",
        "        fontsize=20,\n",
        "    )\n",
        "\n",
        "    # 1.7. Y-axis limit and formatting\n",
        "    y_max_scaled = base.max() * 1.1 if base.max() > 0 else 1.0\n",
        "    ax.set_ylim(\n",
        "        0,\n",
        "        y_max_scaled,\n",
        "    )\n",
        "    ax.yaxis.set_major_locator(\n",
        "        ticker.MaxNLocator(\n",
        "            nbins=5,\n",
        "            integer=True,\n",
        "        ),\n",
        "    )\n",
        "    ax.yaxis.set_major_formatter(\n",
        "        ticker.FormatStrFormatter(\n",
        "            \"%d\",\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # 1.8. Add Legend\n",
        "    handles = [\n",
        "        patches_by_class[cls]\n",
        "        for cls in classes_for_legend\n",
        "        if cls in patches_by_class\n",
        "    ]\n",
        "    labels = [\n",
        "        cls\n",
        "        for cls in classes_for_legend\n",
        "        if cls in patches_by_class\n",
        "    ]\n",
        "\n",
        "    ax.legend(\n",
        "        handles,\n",
        "        labels,\n",
        "        bbox_to_anchor=(\n",
        "            1.05,\n",
        "            1.0,\n",
        "        ),\n",
        "        loc=\"upper left\",\n",
        "        frameon=False,\n",
        "        fontsize=12,\n",
        "    )\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # 1.9. Save and Show Figure\n",
        "    out_fig = os.path.join(\n",
        "        output_dir,\n",
        "        \"graph_pixel_per_class.png\",\n",
        "    )\n",
        "    plt.savefig(\n",
        "        out_fig,\n",
        "        format=\"png\",\n",
        "        bbox_inches=\"tight\",\n",
        "        dpi=300,\n",
        "    )\n",
        "    plt.show()\n",
        "\n",
        "    print(\n",
        "        f\"Plot saved successfully to: {out_fig}\",\n",
        "    )\n",
        "\n",
        "\n",
        "# Execution of the plotting function with the defined parameters\n",
        "\n",
        "# 2.1. Run the plotting function using the generated CSV\n",
        "plot_pixel_counts_from_csv(\n",
        "    csv_path=os.path.join(\n",
        "        output_path,\n",
        "        \"pixels_per_class_per_year.csv\",\n",
        "    ),\n",
        "    class_labels_dict=class_labels_dict,\n",
        "    output_dir=output_path,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQd6joqOtM2J"
      },
      "source": [
        "## **4.Generate the Transition Matrix**\n",
        "\n",
        "\n",
        "---\n",
        "This section performs the core spatial-temporal analysis by first processing multi year rasters into a consistent valid pixel mask. It executes a dual track transition analysis to compute individual interval matrices for step by step changes while simultaneously deriving aggregated components.\n",
        "\n",
        "The process generates the following aggregated transition matrices:\n",
        "\n",
        "\n",
        "1. *transition_matrix_sum*: The total sum of all transitions across all time intervals.\n",
        "\n",
        "2. *transition_matrix_extent*: The end to end transition matrix from the first to the last year.\n",
        "\n",
        "3. *transition_matrix_allocation_exchange*: Spatial swaps derived from the extent matrix.\n",
        "\n",
        "4. *transition_matrix_allocation_shift*: Spatial displacement derived from the extent matrix.\n",
        "\n",
        "5. *transition_matrix_alternation_exchange*: Temporal swaps captured at the pixel level trajectories.\n",
        "\n",
        "6. *transition_matrix_alternation_shift*: Temporal displacement captured at the pixel level trajectories."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.0 Tansition matrix tests (beta)"
      ],
      "metadata": {
        "id": "RBw59JFfHy7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Nativa"
      ],
      "metadata": {
        "id": "nvyrLCFUK3LB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import multiprocessing\n",
        "import concurrent.futures\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import rasterio\n",
        "from rasterio.windows import Window\n",
        "from tqdm import tqdm\n",
        "import numba as nb\n",
        "\n",
        "\n",
        "@nb.njit(nogil=True)\n",
        "def _build_lookup_table(\n",
        "    classes_array: np.ndarray,\n",
        "    max_val: int\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Construct a fast lookup table to map original class values to matrix indices.\n",
        "\n",
        "    This enables O(1) complexity for finding the matrix index of a pixel value,\n",
        "    avoiding slow dictionary lookups inside the processing loop.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    classes_array : np.ndarray\n",
        "        Sorted array containing all unique valid class values.\n",
        "    max_val : int\n",
        "        The maximum integer value present in the classes_array.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        An array where index `i` corresponds to the pixel value, and the value\n",
        "        at `lut[i]` is the corresponding index in the transition matrix.\n",
        "        Values not in `classes_array` are set to -1.\n",
        "    \"\"\"\n",
        "    lut = np.full(max_val + 1, -1, dtype=np.int32)\n",
        "\n",
        "    for idx, class_val in enumerate(classes_array):\n",
        "        if class_val <= max_val:\n",
        "            lut[class_val] = idx\n",
        "\n",
        "    return lut\n",
        "\n",
        "\n",
        "@nb.njit(nogil=True)\n",
        "def _accumulate_transitions_numba(\n",
        "    start_pixels: np.ndarray,\n",
        "    end_pixels: np.ndarray,\n",
        "    matrix: np.ndarray,\n",
        "    lut: np.ndarray\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Accumulate transitions between two time steps into the matrix using JIT.\n",
        "\n",
        "    This function performs the mathematically equivalent operation of:\n",
        "    `matrix[class_to_idx[start], class_to_idx[end]] += 1`\n",
        "    but does so at C-level speeds avoiding Python overhead.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    start_pixels : np.ndarray\n",
        "        1D array of pixel values at time T.\n",
        "    end_pixels : np.ndarray\n",
        "        1D array of pixel values at time T+1.\n",
        "    matrix : np.ndarray\n",
        "        The transition matrix to be updated in-place.\n",
        "    lut : np.ndarray\n",
        "        The lookup table mapping pixel values to matrix indices.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "        The operation is performed in-place on the `matrix` argument.\n",
        "    \"\"\"\n",
        "    n_pixels = start_pixels.shape[0]\n",
        "\n",
        "    for i in range(n_pixels):\n",
        "        val_t0 = start_pixels[i]\n",
        "        val_t1 = end_pixels[i]\n",
        "\n",
        "        if val_t0 >= len(lut) or val_t1 >= len(lut):\n",
        "            continue\n",
        "\n",
        "        idx_t0 = lut[val_t0]\n",
        "        idx_t1 = lut[val_t1]\n",
        "\n",
        "        if idx_t0 != -1 and idx_t1 != -1:\n",
        "            matrix[idx_t0, idx_t1] += 1\n",
        "\n",
        "\n",
        "def _process_window_worker(\n",
        "    window: Window,\n",
        "    image_paths: list,\n",
        "    no_data_value: int,\n",
        "    lut: np.ndarray,\n",
        "    n_classes: int,\n",
        "    n_years: int\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Worker function to process a single spatial window across all time steps.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    window : rasterio.windows.Window\n",
        "        The spatial bounding box to read from the rasters.\n",
        "    image_paths : list\n",
        "        List of file paths to the input raster images.\n",
        "    no_data_value : int\n",
        "        The value representing NoData in the input rasters.\n",
        "    lut : np.ndarray\n",
        "        The lookup table mapping pixel values to matrix indices.\n",
        "    n_classes : int\n",
        "        The total number of unique valid classes.\n",
        "    n_years : int\n",
        "        The total number of time steps (years) being evaluated.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        A dictionary mapping the temporal interval index (int) to its locally\n",
        "        calculated transition matrix (np.ndarray).\n",
        "    \"\"\"\n",
        "    # 1. Initialize local partial matrices for this specific window\n",
        "    local_matrices = {\n",
        "        t: np.zeros((n_classes, n_classes), dtype=np.float64)\n",
        "        for t in range(n_years - 1)\n",
        "    }\n",
        "\n",
        "    # 2. Open datasets locally within the worker process\n",
        "    srcs = [rasterio.open(p) for p in image_paths]\n",
        "\n",
        "    try:\n",
        "        stack_list = []\n",
        "        combined_nodata_mask = None\n",
        "\n",
        "        # 3. Read and mask data for the current window across all years\n",
        "        for src in srcs:\n",
        "            data = src.read(1, window=window)\n",
        "\n",
        "            mask = (data == no_data_value)\n",
        "            if src.nodata is not None:\n",
        "                mask |= (data == src.nodata)\n",
        "\n",
        "            if np.issubdtype(data.dtype, np.floating):\n",
        "                mask |= np.isnan(data)\n",
        "\n",
        "            if combined_nodata_mask is None:\n",
        "                combined_nodata_mask = mask\n",
        "            else:\n",
        "                combined_nodata_mask |= mask\n",
        "\n",
        "            stack_list.append(data)\n",
        "\n",
        "        valid_mask = ~combined_nodata_mask\n",
        "\n",
        "        # Optimization: Skip empty blocks\n",
        "        if not np.any(valid_mask):\n",
        "            return local_matrices\n",
        "\n",
        "        # 4. Flatten arrays using the valid mask\n",
        "        flat_stack = [\n",
        "            layer[valid_mask].astype(np.int32) for layer in stack_list\n",
        "        ]\n",
        "\n",
        "        # 5. Calculate transitions for consecutive intervals\n",
        "        for t in range(n_years - 1):\n",
        "            start_pixels = flat_stack[t]\n",
        "            end_pixels = flat_stack[t + 1]\n",
        "\n",
        "            _accumulate_transitions_numba(\n",
        "                start_pixels,\n",
        "                end_pixels,\n",
        "                local_matrices[t],\n",
        "                lut\n",
        "            )\n",
        "\n",
        "    finally:\n",
        "        # 6. Ensure file handlers are closed\n",
        "        for src in srcs:\n",
        "            src.close()\n",
        "\n",
        "    return local_matrices\n",
        "\n",
        "\n",
        "def calculate_transition_matrices_optimized(\n",
        "    file_list: list,\n",
        "    output_dir: str,\n",
        "    no_data_value: int,\n",
        "    known_classes: list = None,\n",
        "    block_size: int = 1024,\n",
        "    n_workers: int = None\n",
        ") -> tuple:\n",
        "    \"\"\"\n",
        "    Calculate transition matrices using multiprocessing and Numba acceleration.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    file_list : list of str\n",
        "        List of file paths to the input raster images.\n",
        "    output_dir : str\n",
        "        Directory path where the resulting CSV matrices will be saved.\n",
        "    no_data_value : int\n",
        "        The value representing NoData in the input rasters.\n",
        "    known_classes : list, optional\n",
        "        List of known class IDs to use, skipping the scanning step.\n",
        "    block_size : int, optional\n",
        "        The size of the spatial window (width and height) for processing.\n",
        "        Default is 1024.\n",
        "    n_workers : int, optional\n",
        "        Number of CPU cores to use. If None, uses (total available - 1).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple\n",
        "        A tuple containing:\n",
        "        - interval_matrices (dict): Matrices keyed by interval strings.\n",
        "        - years (list of int): Sorted list of years.\n",
        "        - sorted_classes (np.ndarray): Array of unique classes found.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If the file list is empty.\n",
        "    \"\"\"\n",
        "    if not file_list:\n",
        "        raise ValueError(\"File list is empty.\")\n",
        "\n",
        "    # Determine CPU workers automatically to prevent Colab crashes\n",
        "    if n_workers is None:\n",
        "        n_workers = max(1, multiprocessing.cpu_count() - 1)\n",
        "\n",
        "    print(f\"Initializing processing with {n_workers} concurrent workers...\")\n",
        "\n",
        "    # 1. Sort files and extract years using regex\n",
        "    image_paths = sorted(file_list)\n",
        "    years = []\n",
        "    for path in image_paths:\n",
        "        match = re.search(r\"(\\d{4})\\.tif$\", os.path.basename(path))\n",
        "        years.append(int(match.group(1)) if match else 0)\n",
        "\n",
        "    print(f\"Processing transition matrices for years: {years}\")\n",
        "\n",
        "    # 2. Open first dataset just to retrieve spatial dimensions\n",
        "    with rasterio.open(image_paths[0]) as src0:\n",
        "        height = src0.height\n",
        "        width = src0.width\n",
        "\n",
        "    # 3. Smart Class Detection\n",
        "    if known_classes is not None and len(known_classes) > 0:\n",
        "        print(\"Using predefined classes provided by user.\")\n",
        "        sorted_classes = np.array(sorted(list(known_classes)), dtype=np.int32)\n",
        "    else:\n",
        "        print(\"Scanning for unique classes...\")\n",
        "        all_classes = set()\n",
        "        scan_indices = [0, len(image_paths) // 2, -1]\n",
        "\n",
        "        for idx in scan_indices:\n",
        "            if idx < len(image_paths):\n",
        "                with rasterio.open(image_paths[idx]) as src_tmp:\n",
        "                    data = src_tmp.read(1)\n",
        "                    uniques = np.unique(data)\n",
        "                    all_classes.update(uniques)\n",
        "\n",
        "                    if src_tmp.nodata is not None:\n",
        "                        all_classes.discard(src_tmp.nodata)\n",
        "\n",
        "        all_classes.discard(no_data_value)\n",
        "        sorted_classes = np.array(sorted(list(all_classes)), dtype=np.int32)\n",
        "\n",
        "    n_classes = len(sorted_classes)\n",
        "    print(f\"Unique classes used: {sorted_classes}\")\n",
        "\n",
        "    if n_classes == 0:\n",
        "        print(\"Warning: No valid classes found. Matrices will be empty.\")\n",
        "        return {}, years, sorted_classes\n",
        "\n",
        "    # Build Numba Lookup Table\n",
        "    max_class_val = int(sorted_classes.max())\n",
        "    lut = _build_lookup_table(sorted_classes, max_val=max(255, max_class_val))\n",
        "\n",
        "    # 4. Initialize final global result matrices\n",
        "    intervals = [f\"{years[t]}-{years[t + 1]}\" for t in range(len(years) - 1)]\n",
        "    interval_matrices = {\n",
        "        interval: np.zeros((n_classes, n_classes), dtype=np.float64)\n",
        "        for interval in intervals\n",
        "    }\n",
        "\n",
        "    # 5. Define Spatial Blocks (Windows)\n",
        "    rows = range(0, height, block_size)\n",
        "    cols = range(0, width, block_size)\n",
        "\n",
        "    windows_list = []\n",
        "    for r_off in rows:\n",
        "        for c_off in cols:\n",
        "            win = Window(\n",
        "                c_off,\n",
        "                r_off,\n",
        "                min(block_size, width - c_off),\n",
        "                min(block_size, height - r_off)\n",
        "            )\n",
        "            windows_list.append(win)\n",
        "\n",
        "    total_blocks = len(windows_list)\n",
        "    n_years = len(years)\n",
        "\n",
        "    # 6. Execute Parallel Processing Pool\n",
        "    with concurrent.futures.ProcessPoolExecutor(max_workers=n_workers) as exc:\n",
        "        # Submit all tasks\n",
        "        futures = {\n",
        "            exc.submit(\n",
        "                _process_window_worker,\n",
        "                win,\n",
        "                image_paths,\n",
        "                no_data_value,\n",
        "                lut,\n",
        "                n_classes,\n",
        "                n_years\n",
        "            ): win for win in windows_list\n",
        "        }\n",
        "\n",
        "        # Retrieve and aggregate partial results as they complete\n",
        "        with tqdm(total=total_blocks, desc=\"Computing Matrices\", unit=\"block\") as p:\n",
        "            for future in concurrent.futures.as_completed(futures):\n",
        "                try:\n",
        "                    local_mats = future.result()\n",
        "\n",
        "                    # 7. Aggregate partial matrices into the global matrices\n",
        "                    for t_idx in range(n_years - 1):\n",
        "                        interval_key = intervals[t_idx]\n",
        "                        interval_matrices[interval_key] += local_mats[t_idx]\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing a window: {e}\")\n",
        "\n",
        "                p.update(1)\n",
        "\n",
        "    # 8. Save matrices to CSV automatically\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    for interval_name, mat in interval_matrices.items():\n",
        "        fname = f\"transition_matrix_{interval_name}.csv\"\n",
        "        out_csv = os.path.join(output_dir, fname)\n",
        "\n",
        "        df = pd.DataFrame(\n",
        "            mat,\n",
        "            index=sorted_classes,\n",
        "            columns=sorted_classes\n",
        "        )\n",
        "        df.to_csv(out_csv)\n",
        "\n",
        "    print(f\"All transition matrices saved to: {output_dir}\")\n",
        "\n",
        "    return interval_matrices, years, sorted_classes\n",
        "\n",
        "\n",
        "# --- Execution ---\n",
        "\n",
        "# Ensure these variables are defined in your notebook:\n",
        "# image_paths: list of file paths\n",
        "# output_path: directory to save CSVs\n",
        "# noData_value: the nodata value (e.g., 0, -9999)\n",
        "\n",
        "defined_classes = None\n",
        "if 'class_labels_dict' in locals():\n",
        "    defined_classes = list(class_labels_dict.keys())\n",
        "\n",
        "matrices, years_list, classes_found = calculate_transition_matrices_optimized(\n",
        "    file_list=image_paths,\n",
        "    output_dir=output_path,\n",
        "    no_data_value=noData_value,\n",
        "    known_classes=defined_classes,\n",
        "    block_size=1024,\n",
        "    n_workers=multiprocessing.cpu_count()  # Leave as \"None\" for automatic CPU detection\n",
        ")"
      ],
      "metadata": {
        "id": "JMzEyLnvIAAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Joblib"
      ],
      "metadata": {
        "id": "8uWEOHK6LmQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import multiprocessing\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import rasterio\n",
        "from rasterio.windows import Window\n",
        "import numba as nb\n",
        "\n",
        "\n",
        "@nb.njit(nogil=True)\n",
        "def _build_lookup_table(\n",
        "    classes_array: np.ndarray,\n",
        "    max_val: int\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Construct a fast lookup table to map original class values to indices.\n",
        "\n",
        "    This enables O(1) complexity for finding the matrix index of a pixel,\n",
        "    avoiding slow dictionary lookups inside the processing loop.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    classes_array : np.ndarray\n",
        "        Sorted array containing all unique valid class values.\n",
        "    max_val : int\n",
        "        The maximum integer value present in the classes_array.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        An array where index `i` corresponds to the pixel value, and the\n",
        "        value at `lut[i]` is the corresponding index in the matrix.\n",
        "        Values not in `classes_array` are set to -1.\n",
        "    \"\"\"\n",
        "    lut = np.full(max_val + 1, -1, dtype=np.int32)\n",
        "\n",
        "    for idx, class_val in enumerate(classes_array):\n",
        "        if class_val <= max_val:\n",
        "            lut[class_val] = idx\n",
        "\n",
        "    return lut\n",
        "\n",
        "\n",
        "@nb.njit(nogil=True)\n",
        "def _accumulate_transitions_numba(\n",
        "    start_pixels: np.ndarray,\n",
        "    end_pixels: np.ndarray,\n",
        "    matrix: np.ndarray,\n",
        "    lut: np.ndarray\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Accumulate transitions between two time steps into the matrix.\n",
        "\n",
        "    This function performs the mathematically equivalent operation of:\n",
        "    `matrix[class_to_idx[start], class_to_idx[end]] += 1`\n",
        "    but does so at C-level speeds avoiding Python overhead.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    start_pixels : np.ndarray\n",
        "        1D array of pixel values at time T.\n",
        "    end_pixels : np.ndarray\n",
        "        1D array of pixel values at time T+1.\n",
        "    matrix : np.ndarray\n",
        "        The transition matrix to be updated in-place.\n",
        "    lut : np.ndarray\n",
        "        The lookup table mapping pixel values to matrix indices.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "        The operation is performed in-place on the `matrix` argument.\n",
        "    \"\"\"\n",
        "    n_pixels = start_pixels.shape[0]\n",
        "\n",
        "    for i in range(n_pixels):\n",
        "        val_t0 = start_pixels[i]\n",
        "        val_t1 = end_pixels[i]\n",
        "\n",
        "        if val_t0 >= len(lut) or val_t1 >= len(lut):\n",
        "            continue\n",
        "\n",
        "        idx_t0 = lut[val_t0]\n",
        "        idx_t1 = lut[val_t1]\n",
        "\n",
        "        if idx_t0 != -1 and idx_t1 != -1:\n",
        "            matrix[idx_t0, idx_t1] += 1\n",
        "\n",
        "\n",
        "def _process_window_worker(\n",
        "    window: Window,\n",
        "    image_paths: list,\n",
        "    no_data_value: int,\n",
        "    lut: np.ndarray,\n",
        "    n_classes: int,\n",
        "    n_years: int\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Worker function to process a single spatial window across all steps.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    window : rasterio.windows.Window\n",
        "        The spatial bounding box to read from the rasters.\n",
        "    image_paths : list\n",
        "        List of file paths to the input raster images.\n",
        "    no_data_value : int\n",
        "        The value representing NoData in the input rasters.\n",
        "    lut : np.ndarray\n",
        "        The lookup table mapping pixel values to matrix indices.\n",
        "    n_classes : int\n",
        "        The total number of unique valid classes.\n",
        "    n_years : int\n",
        "        The total number of time steps (years) being evaluated.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        A dictionary mapping the temporal interval index (int) to its\n",
        "        locally calculated transition matrix (np.ndarray).\n",
        "    \"\"\"\n",
        "    # 1. Initialize local partial matrices for this specific window\n",
        "    local_matrices = {\n",
        "        t: np.zeros((n_classes, n_classes), dtype=np.float64)\n",
        "        for t in range(n_years - 1)\n",
        "    }\n",
        "\n",
        "    # 2. Open datasets locally within the worker process\n",
        "    srcs = [rasterio.open(p) for p in image_paths]\n",
        "\n",
        "    try:\n",
        "        stack_list = []\n",
        "        combined_nodata_mask = None\n",
        "\n",
        "        # 3. Read and mask data for the current window across all years\n",
        "        for src in srcs:\n",
        "            data = src.read(1, window=window)\n",
        "\n",
        "            mask = (data == no_data_value)\n",
        "            if src.nodata is not None:\n",
        "                mask |= (data == src.nodata)\n",
        "\n",
        "            if np.issubdtype(data.dtype, np.floating):\n",
        "                mask |= np.isnan(data)\n",
        "\n",
        "            if combined_nodata_mask is None:\n",
        "                combined_nodata_mask = mask\n",
        "            else:\n",
        "                combined_nodata_mask |= mask\n",
        "\n",
        "            stack_list.append(data)\n",
        "\n",
        "        valid_mask = ~combined_nodata_mask\n",
        "\n",
        "        # Optimization: Skip empty blocks\n",
        "        if not np.any(valid_mask):\n",
        "            return local_matrices\n",
        "\n",
        "        # 4. Flatten arrays using the valid mask\n",
        "        flat_stack = [\n",
        "            layer[valid_mask].astype(np.int32) for layer in stack_list\n",
        "        ]\n",
        "\n",
        "        # 5. Calculate transitions for consecutive intervals\n",
        "        for t in range(n_years - 1):\n",
        "            start_pixels = flat_stack[t]\n",
        "            end_pixels = flat_stack[t + 1]\n",
        "\n",
        "            _accumulate_transitions_numba(\n",
        "                start_pixels,\n",
        "                end_pixels,\n",
        "                local_matrices[t],\n",
        "                lut\n",
        "            )\n",
        "\n",
        "    finally:\n",
        "        # 6. Ensure file handlers are closed\n",
        "        for src in srcs:\n",
        "            src.close()\n",
        "\n",
        "    return local_matrices\n",
        "\n",
        "\n",
        "def calculate_transition_matrices_optimized(\n",
        "    file_list: list,\n",
        "    output_dir: str,\n",
        "    no_data_value: int,\n",
        "    known_classes: list = None,\n",
        "    block_size: int = 1024,\n",
        "    n_workers: int = None\n",
        ") -> tuple:\n",
        "    \"\"\"\n",
        "    Calculate transition matrices using Joblib and Numba acceleration.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    file_list : list of str\n",
        "        List of file paths to the input raster images.\n",
        "    output_dir : str\n",
        "        Directory path where the resulting CSV matrices will be saved.\n",
        "    no_data_value : int\n",
        "        The value representing NoData in the input rasters.\n",
        "    known_classes : list, optional\n",
        "        List of known class IDs to use, skipping the scanning step.\n",
        "    block_size : int, optional\n",
        "        The size of the spatial window (width and height). Default is 1024.\n",
        "    n_workers : int, optional\n",
        "        Number of CPU cores to use. If None, uses (total available - 1).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple\n",
        "        A tuple containing:\n",
        "        - interval_matrices (dict): Matrices keyed by interval strings.\n",
        "        - years (list of int): Sorted list of years.\n",
        "        - sorted_classes (np.ndarray): Array of unique classes found.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If the file list is empty.\n",
        "    \"\"\"\n",
        "    if not file_list:\n",
        "        raise ValueError(\"File list is empty.\")\n",
        "\n",
        "    if n_workers is None:\n",
        "        n_workers = max(1, multiprocessing.cpu_count() - 1)\n",
        "\n",
        "    print(f\"Initializing Joblib processing with {n_workers} workers...\")\n",
        "\n",
        "    # 1. Sort files and extract years using regex\n",
        "    image_paths = sorted(file_list)\n",
        "    years = []\n",
        "    for path in image_paths:\n",
        "        match = re.search(r\"(\\d{4})\\.tif$\", os.path.basename(path))\n",
        "        years.append(int(match.group(1)) if match else 0)\n",
        "\n",
        "    print(f\"Processing transition matrices for years: {years}\")\n",
        "\n",
        "    # 2. Open first dataset just to retrieve spatial dimensions\n",
        "    with rasterio.open(image_paths[0]) as src0:\n",
        "        height = src0.height\n",
        "        width = src0.width\n",
        "\n",
        "    # 3. Smart Class Detection\n",
        "    if known_classes is not None and len(known_classes) > 0:\n",
        "        print(\"Using predefined classes provided by user.\")\n",
        "        sorted_classes = np.array(sorted(list(known_classes)), dtype=np.int32)\n",
        "    else:\n",
        "        print(\"Scanning for unique classes...\")\n",
        "        all_classes = set()\n",
        "        scan_indices = [0, len(image_paths) // 2, -1]\n",
        "\n",
        "        for idx in scan_indices:\n",
        "            if idx < len(image_paths):\n",
        "                with rasterio.open(image_paths[idx]) as src_tmp:\n",
        "                    data = src_tmp.read(1)\n",
        "                    uniques = np.unique(data)\n",
        "                    all_classes.update(uniques)\n",
        "\n",
        "                    if src_tmp.nodata is not None:\n",
        "                        all_classes.discard(src_tmp.nodata)\n",
        "\n",
        "        all_classes.discard(no_data_value)\n",
        "        sorted_classes = np.array(sorted(list(all_classes)), dtype=np.int32)\n",
        "\n",
        "    n_classes = len(sorted_classes)\n",
        "    print(f\"Unique classes used: {sorted_classes}\")\n",
        "\n",
        "    if n_classes == 0:\n",
        "        print(\"Warning: No valid classes found. Matrices will be empty.\")\n",
        "        return {}, years, sorted_classes\n",
        "\n",
        "    # Build Numba Lookup Table\n",
        "    max_class_val = int(sorted_classes.max())\n",
        "    lut = _build_lookup_table(\n",
        "        sorted_classes,\n",
        "        max_val=max(255, max_class_val)\n",
        "    )\n",
        "\n",
        "    # 4. Initialize final global result matrices\n",
        "    intervals = [f\"{years[t]}-{years[t + 1]}\" for t in range(len(years) - 1)]\n",
        "    interval_matrices = {\n",
        "        interval: np.zeros((n_classes, n_classes), dtype=np.float64)\n",
        "        for interval in intervals\n",
        "    }\n",
        "\n",
        "    # 5. Define Spatial Blocks (Windows)\n",
        "    rows = range(0, height, block_size)\n",
        "    cols = range(0, width, block_size)\n",
        "    n_years = len(years)\n",
        "\n",
        "    windows_list = []\n",
        "    for r_off in rows:\n",
        "        for c_off in cols:\n",
        "            win = Window(\n",
        "                c_off,\n",
        "                r_off,\n",
        "                min(block_size, width - c_off),\n",
        "                min(block_size, height - r_off)\n",
        "            )\n",
        "            windows_list.append(win)\n",
        "\n",
        "    # 6. Execute Parallel Processing Pool with Joblib\n",
        "    # verbose=10 provides a built-in text progress bar in Joblib\n",
        "    results = joblib.Parallel(\n",
        "        n_jobs=n_workers,\n",
        "        verbose=10\n",
        "    )(\n",
        "        joblib.delayed(_process_window_worker)(\n",
        "            win,\n",
        "            image_paths,\n",
        "            no_data_value,\n",
        "            lut,\n",
        "            n_classes,\n",
        "            n_years\n",
        "        )\n",
        "        for win in windows_list\n",
        "    )\n",
        "\n",
        "    # 7. Aggregate partial matrices into the global matrices\n",
        "    for local_mats in results:\n",
        "        for t_idx in range(n_years - 1):\n",
        "            interval_key = intervals[t_idx]\n",
        "            interval_matrices[interval_key] += local_mats[t_idx]\n",
        "\n",
        "    # 8. Save matrices to CSV automatically\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    for interval_name, mat in interval_matrices.items():\n",
        "        fname = f\"transition_matrix_{interval_name}.csv\"\n",
        "        out_csv = os.path.join(output_dir, fname)\n",
        "\n",
        "        df = pd.DataFrame(\n",
        "            mat,\n",
        "            index=sorted_classes,\n",
        "            columns=sorted_classes\n",
        "        )\n",
        "        df.to_csv(out_csv)\n",
        "\n",
        "    print(f\"All transition matrices saved to: {output_dir}\")\n",
        "\n",
        "    return interval_matrices, years, sorted_classes\n",
        "\n",
        "\n",
        "# --- Execution ---\n",
        "\n",
        "# Ensure these variables are defined in your notebook:\n",
        "# image_paths: list of file paths\n",
        "# output_path: directory to save CSVs\n",
        "# noData_value: the nodata value (e.g., 0, -9999)\n",
        "\n",
        "defined_classes = None\n",
        "if 'class_labels_dict' in locals():\n",
        "    defined_classes = list(class_labels_dict.keys())\n",
        "\n",
        "matrices, years_list, classes_found = calculate_transition_matrices_optimized(\n",
        "    file_list=image_paths,\n",
        "    output_dir=output_path,\n",
        "    no_data_value=noData_value,\n",
        "    known_classes=defined_classes,\n",
        "    block_size=1024,\n",
        "    n_workers=multiprocessing.cpu_count()  # Leave as \"None\" for automatic CPU detection\n",
        ")"
      ],
      "metadata": {
        "id": "RMT8I26uLqMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dask A"
      ],
      "metadata": {
        "id": "AG_WTe_8dc9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xarray rioxarray dask dask[diagnostics]"
      ],
      "metadata": {
        "id": "cD6uh5eieTCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "import rioxarray\n",
        "import dask\n",
        "import dask.array as da\n",
        "from dask.diagnostics import ProgressBar\n",
        "import numba as nb\n",
        "import rasterio\n",
        "\n",
        "\n",
        "@nb.njit(nogil=True)\n",
        "def _map_classes_to_indices(\n",
        "    in_block: np.ndarray,\n",
        "    out_block: np.ndarray,\n",
        "    classes_array: np.ndarray,\n",
        "    invalid_idx: int\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Map raster values to matrix indices using fast binary search.\n",
        "\n",
        "    This Numba-compiled function safely handles NaNs, floats, and negative\n",
        "    NoData values without requiring massive memory lookup tables.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    in_block : np.ndarray\n",
        "        The input chunk of raster data.\n",
        "    out_block : np.ndarray\n",
        "        An empty array of the same shape to store the mapped indices.\n",
        "    classes_array : np.ndarray\n",
        "        Sorted array containing all unique valid class values.\n",
        "    invalid_idx : int\n",
        "        The index to assign to pixels that are NoData or invalid.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "        The operation modifies `out_block` in-place.\n",
        "    \"\"\"\n",
        "    in_flat = in_block.ravel()\n",
        "    out_flat = out_block.ravel()\n",
        "    n_classes = classes_array.size\n",
        "\n",
        "    for i in range(in_flat.size):\n",
        "        val = in_flat[i]\n",
        "\n",
        "        if np.isnan(val):\n",
        "            out_flat[i] = invalid_idx\n",
        "            continue\n",
        "\n",
        "        # Fast binary search for the class index\n",
        "        idx = np.searchsorted(classes_array, val)\n",
        "        if idx < n_classes and classes_array[idx] == val:\n",
        "            out_flat[i] = idx\n",
        "        else:\n",
        "            out_flat[i] = invalid_idx\n",
        "\n",
        "\n",
        "def _apply_mapping_dask(\n",
        "    block: np.ndarray,\n",
        "    classes_array: np.ndarray,\n",
        "    invalid_idx: int\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Wrapper to apply the Numba mapping function across Dask chunks.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    block : np.ndarray\n",
        "        The input chunk of raster data provided by Dask.\n",
        "    classes_array : np.ndarray\n",
        "        Sorted array containing all unique valid class values.\n",
        "    invalid_idx : int\n",
        "        The index to assign to pixels that are NoData or invalid.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        The block with original values mapped to matrix indices.\n",
        "    \"\"\"\n",
        "    out_block = np.empty(block.shape, dtype=np.int32)\n",
        "    _map_classes_to_indices(\n",
        "        in_block=block,\n",
        "        out_block=out_block,\n",
        "        classes_array=classes_array,\n",
        "        invalid_idx=invalid_idx\n",
        "    )\n",
        "    return out_block\n",
        "\n",
        "\n",
        "def calculate_transition_matrices_pipeline(\n",
        "    file_list: list,\n",
        "    output_dir: str,\n",
        "    no_data_value: int,\n",
        "    known_classes: list = None,\n",
        "    chunk_size: int = 1024\n",
        ") -> tuple:\n",
        "    \"\"\"\n",
        "    Calculate transition matrices using Xarray, Dask, and Map Algebra.\n",
        "\n",
        "    This function builds a lazy multidimensional computational graph and\n",
        "    executes transitions globally via histogram binning.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    file_list : list of str\n",
        "        List of file paths to the input raster images.\n",
        "    output_dir : str\n",
        "        Directory path where the resulting CSV matrices will be saved.\n",
        "    no_data_value : int\n",
        "        The value representing NoData in the input rasters.\n",
        "    known_classes : list, optional\n",
        "        List of known class IDs to use.\n",
        "    chunk_size : int, optional\n",
        "        The size of the spatial chunk for Dask distribution.\n",
        "        Default is 1024.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple\n",
        "        A tuple containing:\n",
        "        - interval_matrices (dict): Matrices keyed by interval strings.\n",
        "        - years (list of int): Sorted list of years.\n",
        "        - sorted_classes (np.ndarray): Array of unique classes found.\n",
        "    \"\"\"\n",
        "    if not file_list:\n",
        "        raise ValueError(\"File list is empty.\")\n",
        "\n",
        "    # 1. Sort files and extract years\n",
        "    image_paths = sorted(file_list)\n",
        "    years = []\n",
        "    for path in image_paths:\n",
        "        match = re.search(r\"(\\d{4})\\.tif$\", os.path.basename(path))\n",
        "        years.append(int(match.group(1)) if match else 0)\n",
        "\n",
        "    print(f\"Building computational graph for years: {years}\")\n",
        "\n",
        "    # 2. Smart Class Detection\n",
        "    if known_classes is not None and len(known_classes) > 0:\n",
        "        print(\"Using predefined classes provided by user.\")\n",
        "        sorted_classes = np.array(\n",
        "            sorted(list(known_classes)),\n",
        "            dtype=np.int32\n",
        "        )\n",
        "    else:\n",
        "        print(\"Scanning heuristically for unique classes...\")\n",
        "        all_classes = set()\n",
        "        scan_indices = [0, len(image_paths) // 2, -1]\n",
        "\n",
        "        for idx in scan_indices:\n",
        "            if idx < len(image_paths):\n",
        "                with rasterio.open(image_paths[idx]) as src_tmp:\n",
        "                    data = src_tmp.read(1)\n",
        "                    all_classes.update(np.unique(data))\n",
        "                    if src_tmp.nodata is not None:\n",
        "                        all_classes.discard(src_tmp.nodata)\n",
        "\n",
        "        all_classes.discard(no_data_value)\n",
        "        sorted_classes = np.array(\n",
        "            sorted(list(all_classes)),\n",
        "            dtype=np.int32\n",
        "        )\n",
        "\n",
        "    n_classes = len(sorted_classes)\n",
        "    print(f\"Unique classes used: {sorted_classes}\")\n",
        "\n",
        "    if n_classes == 0:\n",
        "        print(\"Warning: No valid classes found. Matrices will be empty.\")\n",
        "        return {}, years, sorted_classes\n",
        "\n",
        "    # 3. Lazy Load All Rasters into a 3D Dask Array\n",
        "    dsets = []\n",
        "    for p in image_paths:\n",
        "        da_layer = rioxarray.open_rasterio(\n",
        "            p,\n",
        "            chunks={\"x\": chunk_size, \"y\": chunk_size}\n",
        "        ).squeeze(drop=True)\n",
        "        dsets.append(da_layer)\n",
        "\n",
        "    time_dim = xr.Variable(\"time\", years)\n",
        "    stack = xr.concat(dsets, dim=time_dim)\n",
        "\n",
        "    # 4. Apply Mapping Lazily\n",
        "    mapped_stack = da.map_blocks(\n",
        "        _apply_mapping_dask,\n",
        "        stack.data,\n",
        "        classes_array=sorted_classes,\n",
        "        invalid_idx=n_classes,\n",
        "        dtype=np.int32,\n",
        "        meta=np.empty((0, 0, 0), dtype=np.int32)\n",
        "    )\n",
        "\n",
        "    # 5. Temporal Consistency Masking\n",
        "    is_invalid_mask = (mapped_stack == n_classes).any(axis=0)\n",
        "    final_stack = da.where(is_invalid_mask, n_classes, mapped_stack)\n",
        "\n",
        "    # 6. Map Algebra & Histogram Setup\n",
        "    intervals = [f\"{years[t]}-{years[t + 1]}\" for t in range(len(years) - 1)]\n",
        "    histograms = []\n",
        "    n_bins = (n_classes + 1) ** 2\n",
        "\n",
        "    for t in range(len(years) - 1):\n",
        "        t0 = final_stack[t]\n",
        "        t1 = final_stack[t + 1]\n",
        "\n",
        "        codes = t0 * (n_classes + 1) + t1\n",
        "\n",
        "        hist = da.histogram(\n",
        "            codes,\n",
        "            bins=np.arange(n_bins + 1) - 0.5\n",
        "        )[0]\n",
        "\n",
        "        histograms.append(hist)\n",
        "\n",
        "    # 7. Execute Dask Graph\n",
        "    print(\"Executing distributed processing...\")\n",
        "    with ProgressBar():\n",
        "        computed_hists = dask.compute(*histograms)\n",
        "\n",
        "    # 8. Decode Histograms to CSV Matrices\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    interval_matrices = {}\n",
        "    for idx, flat_hist in enumerate(computed_hists):\n",
        "        full_matrix = flat_hist.reshape((n_classes + 1, n_classes + 1))\n",
        "        valid_matrix = full_matrix[:n_classes, :n_classes]\n",
        "\n",
        "        interval_key = intervals[idx]\n",
        "        interval_matrices[interval_key] = valid_matrix\n",
        "\n",
        "        fname = f\"transition_matrix_{interval_key}.csv\"\n",
        "        out_csv = os.path.join(output_dir, fname)\n",
        "\n",
        "        df = pd.DataFrame(\n",
        "            valid_matrix,\n",
        "            index=sorted_classes,\n",
        "            columns=sorted_classes\n",
        "        )\n",
        "        df.to_csv(out_csv)\n",
        "\n",
        "    print(f\"All transition matrices saved to: {output_dir}\")\n",
        "\n",
        "    return interval_matrices, years, sorted_classes\n",
        "\n",
        "\n",
        "# --- Execution ---\n",
        "\n",
        "# Ensure these variables are defined in your notebook:\n",
        "# image_paths: list of file paths\n",
        "# output_path: directory to save CSVs\n",
        "# noData_value: the nodata value (e.g., 0, -9999)\n",
        "\n",
        "defined_classes = None\n",
        "if 'class_labels_dict' in locals():\n",
        "    defined_classes = list(class_labels_dict.keys())\n",
        "\n",
        "matrices, years_list, classes_found = calculate_transition_matrices_pipeline(\n",
        "    file_list=image_paths,\n",
        "    output_dir=output_path,\n",
        "    no_data_value=noData_value,\n",
        "    known_classes=defined_classes,\n",
        "    chunk_size=1024\n",
        ")"
      ],
      "metadata": {
        "id": "J9OgK5e1dhbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dask B"
      ],
      "metadata": {
        "id": "yT_iD9l5ik72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "import rioxarray\n",
        "import dask\n",
        "import dask.array as da\n",
        "from dask.diagnostics import ProgressBar\n",
        "import numba as nb\n",
        "import rasterio\n",
        "\n",
        "\n",
        "@nb.njit(nogil=True)\n",
        "def _map_classes_to_indices(\n",
        "    in_block: np.ndarray,\n",
        "    out_block: np.ndarray,\n",
        "    classes_array: np.ndarray,\n",
        "    invalid_idx: int\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Map raster values to matrix indices using fast binary search.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    in_block : np.ndarray\n",
        "        The input chunk of raster data.\n",
        "    out_block : np.ndarray\n",
        "        An empty array of the same shape to store the mapped indices.\n",
        "    classes_array : np.ndarray\n",
        "        Sorted array containing all unique valid class values.\n",
        "    invalid_idx : int\n",
        "        The index to assign to pixels that are NoData or invalid.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "        The operation modifies `out_block` in-place.\n",
        "    \"\"\"\n",
        "    in_flat = in_block.ravel()\n",
        "    out_flat = out_block.ravel()\n",
        "    n_classes = classes_array.size\n",
        "\n",
        "    for i in range(in_flat.size):\n",
        "        val = in_flat[i]\n",
        "\n",
        "        if np.isnan(val):\n",
        "            out_flat[i] = invalid_idx\n",
        "            continue\n",
        "\n",
        "        idx = np.searchsorted(classes_array, val)\n",
        "        if idx < n_classes and classes_array[idx] == val:\n",
        "            out_flat[i] = idx\n",
        "        else:\n",
        "            out_flat[i] = invalid_idx\n",
        "\n",
        "\n",
        "def _apply_mapping_dask(\n",
        "    block: np.ndarray,\n",
        "    classes_array: np.ndarray,\n",
        "    invalid_idx: int\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Wrapper to apply the Numba mapping function across Dask chunks.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    block : np.ndarray\n",
        "        The input chunk of raster data provided by Dask.\n",
        "    classes_array : np.ndarray\n",
        "        Sorted array containing all unique valid class values.\n",
        "    invalid_idx : int\n",
        "        The index to assign to pixels that are NoData or invalid.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        The block with original values mapped to matrix indices.\n",
        "    \"\"\"\n",
        "    out_block = np.empty(block.shape, dtype=np.int32)\n",
        "    _map_classes_to_indices(\n",
        "        in_block=block,\n",
        "        out_block=out_block,\n",
        "        classes_array=classes_array,\n",
        "        invalid_idx=invalid_idx\n",
        "    )\n",
        "    return out_block\n",
        "\n",
        "\n",
        "def calculate_transition_matrices_dask(\n",
        "    file_list: list,\n",
        "    output_dir: str,\n",
        "    no_data_value: int,\n",
        "    known_classes: list = None,\n",
        "    chunk_size: int = 1024\n",
        ") -> tuple:\n",
        "    \"\"\"\n",
        "    Calculate transition matrices using Dask array unique values counting.\n",
        "\n",
        "    This function builds a lazy computational graph and counts categorical\n",
        "    transitions precisely without histogram binning artifacts.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    file_list : list of str\n",
        "        List of file paths to the input raster images.\n",
        "    output_dir : str\n",
        "        Directory path where the resulting CSV matrices will be saved.\n",
        "    no_data_value : int\n",
        "        The value representing NoData in the input rasters.\n",
        "    known_classes : list, optional\n",
        "        List of known class IDs to use.\n",
        "    chunk_size : int, optional\n",
        "        The size of the spatial chunk for Dask distribution.\n",
        "        Default is 1024.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple\n",
        "        A tuple containing:\n",
        "        - interval_matrices (dict): Matrices keyed by interval strings.\n",
        "        - years (list of int): Sorted list of years.\n",
        "        - sorted_classes (np.ndarray): Array of unique classes found.\n",
        "    \"\"\"\n",
        "    if not file_list:\n",
        "        raise ValueError(\"File list is empty.\")\n",
        "\n",
        "    # 1. Sort files and extract years\n",
        "    image_paths = sorted(file_list)\n",
        "    years = []\n",
        "    for path in image_paths:\n",
        "        match = re.search(r\"(\\d{4})\\.tif$\", os.path.basename(path))\n",
        "        years.append(int(match.group(1)) if match else 0)\n",
        "\n",
        "    print(f\"Building computational graph for years: {years}\")\n",
        "\n",
        "    # 2. Smart Class Detection\n",
        "    if known_classes is not None and len(known_classes) > 0:\n",
        "        print(\"Using predefined classes provided by user.\")\n",
        "        sorted_classes = np.array(\n",
        "            sorted(list(known_classes)),\n",
        "            dtype=np.int32\n",
        "        )\n",
        "    else:\n",
        "        print(\"Scanning heuristically for unique classes...\")\n",
        "        all_classes = set()\n",
        "        scan_indices = [0, len(image_paths) // 2, -1]\n",
        "\n",
        "        for idx in scan_indices:\n",
        "            if idx < len(image_paths):\n",
        "                with rasterio.open(image_paths[idx]) as src_tmp:\n",
        "                    data = src_tmp.read(1)\n",
        "                    all_classes.update(np.unique(data))\n",
        "                    if src_tmp.nodata is not None:\n",
        "                        all_classes.discard(src_tmp.nodata)\n",
        "\n",
        "        all_classes.discard(no_data_value)\n",
        "        sorted_classes = np.array(\n",
        "            sorted(list(all_classes)),\n",
        "            dtype=np.int32\n",
        "        )\n",
        "\n",
        "    n_classes = len(sorted_classes)\n",
        "    print(f\"Unique classes used: {sorted_classes}\")\n",
        "\n",
        "    if n_classes == 0:\n",
        "        print(\"Warning: No valid classes found. Matrices will be empty.\")\n",
        "        return {}, years, sorted_classes\n",
        "\n",
        "    # 3. Lazy Load All Rasters into a 3D Dask Array\n",
        "    dsets = []\n",
        "    for p in image_paths:\n",
        "        da_layer = rioxarray.open_rasterio(\n",
        "            p,\n",
        "            chunks={\"x\": chunk_size, \"y\": chunk_size}\n",
        "        ).squeeze(drop=True)\n",
        "        dsets.append(da_layer)\n",
        "\n",
        "    time_dim = xr.Variable(\"time\", years)\n",
        "    stack = xr.concat(dsets, dim=time_dim)\n",
        "\n",
        "    # 4. Apply Mapping Lazily\n",
        "    mapped_stack = da.map_blocks(\n",
        "        _apply_mapping_dask,\n",
        "        stack.data,\n",
        "        classes_array=sorted_classes,\n",
        "        invalid_idx=n_classes,\n",
        "        dtype=np.int32,\n",
        "        meta=np.empty((0, 0, 0), dtype=np.int32)\n",
        "    )\n",
        "\n",
        "    # 5. Temporal Consistency Masking\n",
        "    is_invalid_mask = (mapped_stack == n_classes).any(axis=0)\n",
        "    final_stack = da.where(is_invalid_mask, n_classes, mapped_stack)\n",
        "\n",
        "    # 6. Map Algebra & Unique Values Setup\n",
        "    intervals = [f\"{years[t]}-{years[t + 1]}\" for t in range(len(years) - 1)]\n",
        "\n",
        "    unique_dask_list = []\n",
        "    counts_dask_list = []\n",
        "\n",
        "    for t in range(len(years) - 1):\n",
        "        t0 = final_stack[t]\n",
        "        t1 = final_stack[t + 1]\n",
        "\n",
        "        codes = t0 * (n_classes + 1) + t1\n",
        "\n",
        "        u_da, c_da = da.unique(\n",
        "            codes,\n",
        "            return_counts=True\n",
        "        )\n",
        "        unique_dask_list.append(u_da)\n",
        "        counts_dask_list.append(c_da)\n",
        "\n",
        "    # 7. Execute Dask Graph\n",
        "    print(\"Executing distributed processing...\")\n",
        "    with ProgressBar():\n",
        "        # Execute all unique/counts operations in parallel\n",
        "        results = dask.compute(unique_dask_list, counts_dask_list)\n",
        "\n",
        "    computed_uniques = results[0]\n",
        "    computed_counts = results[1]\n",
        "\n",
        "    # 8. Decode Unique Values back to Matrices\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    interval_matrices = {}\n",
        "    for idx in range(len(years) - 1):\n",
        "        u_vals = computed_uniques[idx]\n",
        "        c_vals = computed_counts[idx]\n",
        "        interval_key = intervals[idx]\n",
        "\n",
        "        full_matrix = np.zeros(\n",
        "            (n_classes + 1, n_classes + 1),\n",
        "            dtype=np.float64\n",
        "        )\n",
        "\n",
        "        for v, c in zip(u_vals, c_vals):\n",
        "            row = v // (n_classes + 1)\n",
        "            col = v % (n_classes + 1)\n",
        "            full_matrix[row, col] += c\n",
        "\n",
        "        # Slice to ignore the row/column corresponding to 'invalid' pixels\n",
        "        valid_matrix = full_matrix[:n_classes, :n_classes]\n",
        "        interval_matrices[interval_key] = valid_matrix\n",
        "\n",
        "        fname = f\"transition_matrix_{interval_key}.csv\"\n",
        "        out_csv = os.path.join(output_dir, fname)\n",
        "\n",
        "        df = pd.DataFrame(\n",
        "            valid_matrix,\n",
        "            index=sorted_classes,\n",
        "            columns=sorted_classes\n",
        "        )\n",
        "        df.to_csv(out_csv)\n",
        "\n",
        "    print(f\"All transition matrices saved to: {output_dir}\")\n",
        "\n",
        "    return interval_matrices, years, sorted_classes\n",
        "\n",
        "\n",
        "# --- Execution ---\n",
        "\n",
        "# Ensure these variables are defined in your notebook:\n",
        "# image_paths: list of file paths\n",
        "# output_path: directory to save CSVs\n",
        "# noData_value: the nodata value (e.g., 0, -9999)\n",
        "\n",
        "defined_classes = None\n",
        "if 'class_labels_dict' in locals():\n",
        "    defined_classes = list(class_labels_dict.keys())\n",
        "\n",
        "#matrices, years_list, classes_found = calculate_transition_matrices_dask(\n",
        "#    file_list=image_paths,\n",
        "#    output_dir=output_path,\n",
        "#    no_data_value=noData_value,\n",
        "#    known_classes=defined_classes,\n",
        "#    chunk_size=1024\n",
        "#)\n",
        "# Forca o Dask a processar os dados sequencialmente (out-of-core)\n",
        "with dask.config.set(scheduler='single-threaded'):\n",
        "    matrices, years_list, classes_found = calculate_transition_matrices_dask(\n",
        "        file_list=image_paths,\n",
        "        output_dir=output_path,\n",
        "        no_data_value=noData_value,\n",
        "        known_classes=defined_classes,\n",
        "        chunk_size=1024\n",
        "    )"
      ],
      "metadata": {
        "id": "Sjrj2-uxiorC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syFpjnQBCcL8"
      },
      "source": [
        "### 4.1 Transition matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "w7qqJnr97Cv3",
        "outputId": "3fe57290-e799-43fe-fc3b-c647be4bf311",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing transition matrices for years: [2001, 2010, 2019]\n",
            "Using predefined classes provided by user.\n",
            "Unique classes used: [1 2 3 4 5 6 7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing Matrices: 100%|██████████| 5852/5852 [01:47<00:00, 54.66block/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All transition matrices saved to: /content/drive/MyDrive/Assessments/cca/output/ecoregion567_glance/nativa_matrixa\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import rasterio\n",
        "from rasterio.windows import Window\n",
        "from tqdm import tqdm\n",
        "import numba as nb\n",
        "\n",
        "@nb.njit(nogil=True)\n",
        "def _build_lookup_table(\n",
        "    classes_array: np.ndarray,\n",
        "    max_val: int\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Construct a fast lookup table to map original class values to matrix indices.\n",
        "\n",
        "    This enables O(1) complexity for finding the matrix index of a pixel value,\n",
        "    avoiding slow dictionary lookups inside the processing loop.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    classes_array : np.ndarray\n",
        "        Sorted array containing all unique valid class values found in the rasters.\n",
        "    max_val : int\n",
        "        The maximum integer value present in the classes_array.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        An array where index `i` corresponds to the pixel value, and the value\n",
        "        at `lut[i]` is the corresponding index in the transition matrix.\n",
        "        Values not in `classes_array` are set to -1.\n",
        "    \"\"\"\n",
        "    # Initialize with -1 to represent \"invalid\" or \"not in classes list\"\n",
        "    lut = np.full(max_val + 1, -1, dtype=np.int32)\n",
        "\n",
        "    for idx, class_val in enumerate(classes_array):\n",
        "        if class_val <= max_val:\n",
        "            lut[class_val] = idx\n",
        "\n",
        "    return lut\n",
        "\n",
        "@nb.njit(nogil=True)\n",
        "def _accumulate_transitions_numba(\n",
        "    start_pixels: np.ndarray,\n",
        "    end_pixels: np.ndarray,\n",
        "    matrix: np.ndarray,\n",
        "    lut: np.ndarray\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Accumulate transitions between two time steps into the matrix using JIT.\n",
        "\n",
        "    This function performs the mathematically equivalent operation of:\n",
        "    `matrix[class_to_idx[start], class_to_idx[end]] += 1`\n",
        "    but does so at C-level speeds avoiding Python overhead.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    start_pixels : np.ndarray\n",
        "        1D array of pixel values at time T.\n",
        "    end_pixels : np.ndarray\n",
        "        1D array of pixel values at time T+1.\n",
        "    matrix : np.ndarray\n",
        "        The transition matrix to be updated in-place.\n",
        "    lut : np.ndarray\n",
        "        The lookup table mapping pixel values to matrix indices.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "        The operation is performed in-place on the `matrix` argument.\n",
        "    \"\"\"\n",
        "    n_pixels = start_pixels.shape[0]\n",
        "\n",
        "    for i in range(n_pixels):\n",
        "        val_t0 = start_pixels[i]\n",
        "        val_t1 = end_pixels[i]\n",
        "\n",
        "        # Safety check: ensure value is within LUT bounds\n",
        "        if val_t0 >= len(lut) or val_t1 >= len(lut):\n",
        "            continue\n",
        "\n",
        "        # Retrieve matrix indices from LUT\n",
        "        idx_t0 = lut[val_t0]\n",
        "        idx_t1 = lut[val_t1]\n",
        "\n",
        "        # If both pixels correspond to valid known classes, increment count\n",
        "        if idx_t0 != -1 and idx_t1 != -1:\n",
        "            matrix[idx_t0, idx_t1] += 1\n",
        "\n",
        "def calculate_transition_matrices_optimized(\n",
        "    file_list: list,\n",
        "    output_dir: str,\n",
        "    no_data_value: int,\n",
        "    known_classes: list = None,\n",
        "    block_size: int = 1024\n",
        ") -> tuple:\n",
        "    \"\"\"\n",
        "    Calculate transition matrices for a time series of rasters using optimized\n",
        "    block processing and Numba acceleration.\n",
        "\n",
        "    This function reads rasters in spatial chunks to conserve memory and uses\n",
        "    compiled code to calculate pixel transitions, ensuring both high performance\n",
        "    and strict mathematical fidelity to standard Markov chain transition counting.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    file_list : list of str\n",
        "        List of file paths to the input raster images.\n",
        "    output_dir : str\n",
        "        Directory path where the resulting CSV matrices will be saved.\n",
        "    no_data_value : int\n",
        "        The value representing NoData in the input rasters. Pixels with this\n",
        "        value in *any* year will be excluded from the transition logic for\n",
        "        that specific spatial location.\n",
        "    known_classes : list, optional\n",
        "        List of known class IDs to use, skipping the scanning step.\n",
        "    block_size : int, optional\n",
        "        The size of the spatial window (width and height) for processing.\n",
        "        Default is 1024.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple\n",
        "        A tuple containing:\n",
        "        - interval_matrices (dict): Keys are interval strings (e.g., \"2000-2001\"),\n",
        "          values are np.ndarrays of the transition counts.\n",
        "        - years (list of int): Sorted list of years extracted from filenames.\n",
        "        - sorted_classes (np.ndarray): Array of unique classes found.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If the file list is empty or if dimensions mismatch.\n",
        "    \"\"\"\n",
        "    if not file_list:\n",
        "        raise ValueError(\"File list is empty.\")\n",
        "\n",
        "    # 1. Sort files and extract years using regex\n",
        "    image_paths = sorted(file_list)\n",
        "    years = []\n",
        "    for path in image_paths:\n",
        "        # Assumes filename contains a 4-digit year (e.g., \"cover_2000.tif\")\n",
        "        match = re.search(r\"(\\d{4})\\.tif$\", os.path.basename(path))\n",
        "        years.append(int(match.group(1)) if match else 0)\n",
        "\n",
        "    print(f\"Processing transition matrices for years: {years}\")\n",
        "\n",
        "    # 2. Open all datasets ONCE to optimize I/O\n",
        "    # This creates file handles but does not load data into RAM yet.\n",
        "    srcs = [rasterio.open(p) for p in image_paths]\n",
        "    height = srcs[0].height\n",
        "    width = srcs[0].width\n",
        "\n",
        "    # 3. Smart Class Detection\n",
        "    # Use predefined classes if available to save time\n",
        "    if known_classes is not None and len(known_classes) > 0:\n",
        "        print(\"Using predefined classes provided by user.\")\n",
        "        # Convert to a sorted numpy array of integers for indexing\n",
        "        sorted_classes = np.array(sorted(list(known_classes)), dtype=np.int32)\n",
        "    else:\n",
        "        # To build the matrix dimensions, we need to know all possible classes.\n",
        "        # Scanning the first, middle, and last image is a heuristic to avoid reading everything.\n",
        "        print(\"Scanning for unique classes...\")\n",
        "        all_classes = set()\n",
        "        scan_indices = [0, len(srcs) // 2, -1]  # First, Middle, Last\n",
        "\n",
        "        for idx in scan_indices:\n",
        "            if idx < len(srcs):\n",
        "                # Read band 1 to find unique values\n",
        "                data = srcs[idx].read(1)\n",
        "                uniques = np.unique(data)\n",
        "                all_classes.update(uniques)\n",
        "\n",
        "        # Clean NoData values from the set of valid classes\n",
        "        all_classes.discard(no_data_value)\n",
        "        for src in srcs:\n",
        "            if src.nodata is not None:\n",
        "                all_classes.discard(src.nodata)\n",
        "\n",
        "        # Convert to a sorted numpy array of integers for indexing\n",
        "        sorted_classes = np.array(sorted(list(all_classes)), dtype=np.int32)\n",
        "\n",
        "    n_classes = len(sorted_classes)\n",
        "    print(f\"Unique classes used: {sorted_classes}\")\n",
        "\n",
        "    if n_classes == 0:\n",
        "        print(\"Warning: No valid classes found. Matrices will be empty.\")\n",
        "        for src in srcs:\n",
        "            src.close()\n",
        "        return {}, years, sorted_classes\n",
        "\n",
        "    # Build Numba Lookup Table\n",
        "    # Determine the size needed for the LUT (max value + 1)\n",
        "    max_class_val = int(sorted_classes.max())\n",
        "    # Create the LUT to map [Pixel Value] -> [Matrix Index]\n",
        "    lut = _build_lookup_table(sorted_classes, max_val=max(255, max_class_val))\n",
        "\n",
        "    # 4. Initialize result matrices\n",
        "    # Format: \"YearA-YearB\"\n",
        "    intervals = [f\"{years[t]}-{years[t + 1]}\" for t in range(len(years) - 1)]\n",
        "    interval_matrices = {\n",
        "        interval: np.zeros((n_classes, n_classes), dtype=np.float64)\n",
        "        for interval in intervals\n",
        "    }\n",
        "\n",
        "    # 5. Process in Spatial Blocks (Windows)\n",
        "    rows = range(0, height, block_size)\n",
        "    cols = range(0, width, block_size)\n",
        "    total_blocks = len(rows) * len(cols)\n",
        "\n",
        "    with tqdm(total=total_blocks, desc=\"Computing Matrices\", unit=\"block\") as pbar:\n",
        "        for r_off in rows:\n",
        "            for c_off in cols:\n",
        "                # Define the current window\n",
        "                window = Window(\n",
        "                    c_off,\n",
        "                    r_off,\n",
        "                    min(block_size, width - c_off),\n",
        "                    min(block_size, height - r_off)\n",
        "                )\n",
        "\n",
        "                # Load data for this window across all years\n",
        "                stack_list = []\n",
        "                combined_nodata_mask = None\n",
        "\n",
        "                for src in srcs:\n",
        "                    # Read the window\n",
        "                    data = src.read(1, window=window)\n",
        "\n",
        "                    # Create mask for this layer\n",
        "                    # Mask if value is user-defined NoData OR metadata NoData\n",
        "                    mask = (data == no_data_value)\n",
        "                    if src.nodata is not None:\n",
        "                        mask |= (data == src.nodata)\n",
        "\n",
        "                    # Handle NaNs if data is floating point\n",
        "                    if np.issubdtype(data.dtype, np.floating):\n",
        "                        mask |= np.isnan(data)\n",
        "\n",
        "                    # Accumulate mask (OR logic):\n",
        "                    # If pixel is NoData in ANY year, it is invalid for the whole series\n",
        "                    if combined_nodata_mask is None:\n",
        "                        combined_nodata_mask = mask\n",
        "                    else:\n",
        "                        combined_nodata_mask |= mask\n",
        "\n",
        "                    stack_list.append(data)\n",
        "\n",
        "                # Determine valid pixels (where NO layer has NoData)\n",
        "                valid_mask = ~combined_nodata_mask\n",
        "\n",
        "                # Optimization: Skip empty blocks\n",
        "                if not np.any(valid_mask):\n",
        "                    pbar.update(1)\n",
        "                    continue\n",
        "\n",
        "                # Flatten arrays using the valid mask\n",
        "                # This creates 1D arrays containing only valid pixel trajectories\n",
        "                # We cast to int32 for Numba compatibility with the LUT\n",
        "                flat_stack = [\n",
        "                    layer[valid_mask].astype(np.int32) for layer in stack_list\n",
        "                ]\n",
        "\n",
        "                # 6. Calculate transitions for consecutive intervals\n",
        "                for t in range(len(years) - 1):\n",
        "                    start_pixels = flat_stack[t]\n",
        "                    end_pixels = flat_stack[t + 1]\n",
        "\n",
        "                    # Numba Accumulation (In-Place update of the matrix)\n",
        "                    _accumulate_transitions_numba(\n",
        "                        start_pixels,\n",
        "                        end_pixels,\n",
        "                        interval_matrices[intervals[t]],\n",
        "                        lut\n",
        "                    )\n",
        "\n",
        "                pbar.update(1)\n",
        "\n",
        "    # 7. Close file handlers\n",
        "    for src in srcs:\n",
        "        src.close()\n",
        "\n",
        "    # 8. Save matrices to CSV automatically\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    for interval_name, mat in interval_matrices.items():\n",
        "        fname = f\"transition_matrix_{interval_name}.csv\"\n",
        "        out_csv = os.path.join(output_dir, fname)\n",
        "\n",
        "        # Create DataFrame for labeled CSV output\n",
        "        df = pd.DataFrame(\n",
        "            mat,\n",
        "            index=sorted_classes,\n",
        "            columns=sorted_classes\n",
        "        )\n",
        "        df.to_csv(out_csv)\n",
        "\n",
        "    print(f\"All transition matrices saved to: {output_dir}\")\n",
        "\n",
        "    return interval_matrices, years, sorted_classes\n",
        "\n",
        "\n",
        "# --- Execution ---\n",
        "\n",
        "# Ensure these variables are defined in your notebook:\n",
        "# image_paths: list of file paths\n",
        "# output_path: directory to save CSVs\n",
        "# noData_value: the nodata value (e.g., 0, -9999)\n",
        "\n",
        "# Check for predefined classes in the global scope (from Section 2.4)\n",
        "defined_classes = None\n",
        "if 'class_labels_dict' in locals():\n",
        "    defined_classes = list(class_labels_dict.keys())\n",
        "\n",
        "matrices, years_list, classes_found = calculate_transition_matrices_optimized(\n",
        "    file_list=image_paths,\n",
        "    output_dir=output_path,\n",
        "    no_data_value=noData_value,\n",
        "    known_classes=defined_classes\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pay2l7VpCm9u"
      },
      "source": [
        "### 4.2 Aggregate matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycppp6kRQo6P"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import rasterio\n",
        "from rasterio.windows import Window\n",
        "from tqdm import tqdm\n",
        "import numba as nb\n",
        "\n",
        "@nb.njit(\n",
        "    nogil=True,\n",
        ")\n",
        "def _build_lookup_table(\n",
        "    classes_array: np.ndarray,\n",
        "    max_val: int,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Create a fast lookup array to map pixel values to matrix indices.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    classes_array : np.ndarray\n",
        "        1D array containing the unique class identifiers.\n",
        "    max_val : int\n",
        "        The maximum pixel value present in the data to define LUT size.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        A 1D array where the index is the pixel value and the value\n",
        "        is the corresponding matrix index.\n",
        "    \"\"\"\n",
        "    # 1.1. Initialize the Lookup Table (LUT) with -1 (invalid)\n",
        "    lut = np.full(\n",
        "        max_val + 1,\n",
        "        -1,\n",
        "        dtype=np.int32,\n",
        "    )\n",
        "\n",
        "    # 1.2. Map each class value to its positional index\n",
        "    for idx, class_val in enumerate(\n",
        "        classes_array,\n",
        "    ):\n",
        "        if class_val <= max_val:\n",
        "            lut[\n",
        "                class_val\n",
        "            ] = idx\n",
        "\n",
        "    return lut\n",
        "\n",
        "@nb.njit(\n",
        "    nogil=True,\n",
        "    parallel=True,\n",
        ")\n",
        "def _process_block_accumulation(\n",
        "    stack_data: np.ndarray,\n",
        "    mat_sum: np.ndarray,\n",
        "    mat_ext: np.ndarray,\n",
        "    mat_ax: np.ndarray,\n",
        "    mat_as: np.ndarray,\n",
        "    lut: np.ndarray,\n",
        "    n_classes: int,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Process a 3D block to calculate and accumulate aggregated transition matrices.\n",
        "\n",
        "    This function implements the pixel-level decomposition logic:\n",
        "    1. Construct M_r (Individual Transition) and E_r (End-to-End).\n",
        "    2. Calculate A_r = M_r - E_r (Alternation).\n",
        "    3. Decompose A_r into Ax_r (Exchange) and As_r (Shift).\n",
        "    4. Accumulate results into global matrices.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    stack_data : np.ndarray\n",
        "        3D array of shape (time, height, width) with pixel class values.\n",
        "    mat_sum : np.ndarray\n",
        "        Global 2D array to accumulate M_r (Total transitions).\n",
        "    mat_ext : np.ndarray\n",
        "        Global 2D array to accumulate E_r (End-to-end transitions).\n",
        "    mat_ax : np.ndarray\n",
        "        Global 2D array to accumulate Ax_r (Alternation Exchange).\n",
        "    mat_as : np.ndarray\n",
        "        Global 2D array to accumulate As_r (Alternation Shift).\n",
        "    lut : np.ndarray\n",
        "        Lookup table to map class values to matrix indices.\n",
        "    n_classes : int\n",
        "        Number of unique classes (size of the transition matrices).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "        The function updates the global matrices in-place.\n",
        "    \"\"\"\n",
        "    n_pixels = stack_data.shape[0]\n",
        "    n_times = stack_data.shape[1]\n",
        "\n",
        "    # We iterate over pixels in parallel\n",
        "    for i in nb.prange(n_pixels):\n",
        "        # Extract trajectory for this pixel\n",
        "        traj = stack_data[i, :]\n",
        "\n",
        "        # Check if start and end are valid\n",
        "        val_start = traj[0]\n",
        "        val_end = traj[n_times - 1]\n",
        "\n",
        "        if val_start >= len(lut) or val_end >= len(lut):\n",
        "            continue\n",
        "\n",
        "        idx_start = lut[val_start]\n",
        "        idx_end = lut[val_end]\n",
        "\n",
        "        if idx_start == -1 or idx_end == -1:\n",
        "            continue\n",
        "\n",
        "        # --- 1. Calculate M_r (transitions count for this pixel) ---\n",
        "        # We need a temporary local matrix for this pixel.\n",
        "        # Since allocating (n,n) inside a loop is expensive, we try to use\n",
        "        # stack arrays or simple logic. For Numba, explicit loops are fast.\n",
        "\n",
        "        # To avoid allocating an NxN matrix for EVERY pixel (which kills RAM),\n",
        "        # we calculate contributions on the fly.\n",
        "\n",
        "        # A. Accumulate Sum Matrix (M_r contribution)\n",
        "        # We also need to store M_r locally to calculate Alternation later.\n",
        "        # For small N (classes < 50), a stack allocation is fine.\n",
        "        m_r = np.zeros((n_classes, n_classes), dtype=np.int32)\n",
        "\n",
        "        valid_traj = True\n",
        "        for t in range(n_times - 1):\n",
        "            v1 = traj[t]\n",
        "            v2 = traj[t+1]\n",
        "            if v1 >= len(lut) or v2 >= len(lut):\n",
        "                valid_traj = False\n",
        "                break\n",
        "            i1 = lut[v1]\n",
        "            i2 = lut[v2]\n",
        "            if i1 == -1 or i2 == -1:\n",
        "                valid_traj = False\n",
        "                break\n",
        "\n",
        "            m_r[i1, i2] += 1\n",
        "\n",
        "        if not valid_traj:\n",
        "            continue\n",
        "\n",
        "        # Add M_r to global Sum Matrix (Thread-safe atomic add needed or reduction)\n",
        "        # Since standard atomic add for 2D arrays isn't straightforward in all Numba versions,\n",
        "        # and we have many classes, we simply add to the output matrices.\n",
        "        # Note: parallel=True with array reduction is handled by Numba automatically\n",
        "        # if we do `mat += ...`. However, for complex logic, standard loops are safer.\n",
        "        # To keep it simple and correct without race conditions, we use a manual lock\n",
        "        # or simply rely on Numba's reduction capabilities for the outer loop.\n",
        "\n",
        "        # B. Calculate E_r (End-to-End)\n",
        "        # E_r has only ONE entry: [start, end] = 1\n",
        "        # (Assuming the pixel represents 1 unit of area)\n",
        "\n",
        "        # C. Calculate A_r = M_r - E_r\n",
        "        # We do this element-wise.\n",
        "\n",
        "        # D. Calculate Alternation Components\n",
        "        # Ax_r_ij = min(A_r_ij, A_r_ji)\n",
        "        # As_r_ij = A_r_ij - Ax_r_ij\n",
        "\n",
        "        for r_idx in range(n_classes):\n",
        "            for c_idx in range(n_classes):\n",
        "                # Get M_r value\n",
        "                val_m = m_r[r_idx, c_idx]\n",
        "\n",
        "                # Get E_r value (1 if it matches start/end, else 0)\n",
        "                val_e = 0\n",
        "                if r_idx == idx_start and c_idx == idx_end:\n",
        "                    val_e = 1\n",
        "\n",
        "                # Calculate A_r\n",
        "                val_a = val_m - val_e\n",
        "\n",
        "                # We need the transpose A_r_ji to calculate Exchange\n",
        "                # Reconstruct A_r_ji on the fly\n",
        "                val_m_T = m_r[c_idx, r_idx]\n",
        "                val_e_T = 0\n",
        "                if c_idx == idx_start and r_idx == idx_end:\n",
        "                    val_e_T = 1\n",
        "                val_a_T = val_m_T - val_e_T\n",
        "\n",
        "                # Ax = min(A, A.T) -> clamped to 0\n",
        "                val_ax = 0\n",
        "                if val_a < val_a_T:\n",
        "                    val_ax = val_a\n",
        "                else:\n",
        "                    val_ax = val_a_T\n",
        "\n",
        "                if val_ax < 0:\n",
        "                    val_ax = 0\n",
        "\n",
        "                # As = A - Ax\n",
        "                val_as = val_a - val_ax\n",
        "\n",
        "                # --- ACCUMULATE TO GLOBAL OUTPUTS ---\n",
        "                # Numba handles the reduction for these array slices in parallel loops\n",
        "                mat_sum[r_idx, c_idx] += val_m\n",
        "                mat_ext[r_idx, c_idx] += val_e\n",
        "                mat_ax[r_idx, c_idx]  += val_ax\n",
        "                mat_as[r_idx, c_idx]  += val_as\n",
        "\n",
        "\n",
        "def calculate_aggregated_matrices_optimized(\n",
        "    file_list,\n",
        "    output_dir,\n",
        "    no_data_value,\n",
        "    all_classes_array,\n",
        "    block_size=1024,\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculate aggregated transition matrices using block processing and Numba.\n",
        "\n",
        "    This function coordinates the accumulation of transitions (Sum),\n",
        "    end-to-end change (Extent),\n",
        "    and the decomposition of alternation into Exchange and Shift components.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    file_list : list of str\n",
        "        List of paths to the raster files.\n",
        "    output_dir : str\n",
        "        Directory to save the resulting transition matrices as CSVs.\n",
        "    no_data_value : int\n",
        "        Specific value to be treated as NoData and ignored in calculations.\n",
        "    all_classes_array : np.ndarray\n",
        "        Sorted array of all unique classes found in the study area.\n",
        "    block_size : int, optional\n",
        "        Size of the square blocks to process at once. Default is 1024.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict of np.ndarray\n",
        "        A dictionary containing the following transition matrices:\n",
        "        - 'mat_sum': Total individual transitions (M_r).\n",
        "        - 'mat_ext': End-to-end transitions (E_r).\n",
        "        - 'mat_ax': Alternation Exchange transitions (Ax_r).\n",
        "        - 'mat_as': Alternation Shift transitions (As_r).\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Setup\n",
        "    image_paths = sorted(file_list)\n",
        "    srcs = [rasterio.open(p) for p in image_paths]\n",
        "    height = srcs[0].height\n",
        "    width = srcs[0].width\n",
        "\n",
        "    n_classes = len(all_classes_array)\n",
        "    max_val = int(all_classes_array.max())\n",
        "\n",
        "    # Build Lookup Table\n",
        "    lut = _build_lookup_table(all_classes_array.astype(np.int32), max(255, max_val))\n",
        "\n",
        "    # Initialize global accumulation matrices (Float64 to prevent overflow)\n",
        "    mat_sum = np.zeros((n_classes, n_classes), dtype=np.float64)\n",
        "    mat_ext = np.zeros((n_classes, n_classes), dtype=np.float64)\n",
        "    mat_ax  = np.zeros((n_classes, n_classes), dtype=np.float64)\n",
        "    mat_as  = np.zeros((n_classes, n_classes), dtype=np.float64)\n",
        "\n",
        "    # 2. Process Blocks\n",
        "    rows = range(0, height, block_size)\n",
        "    cols = range(0, width, block_size)\n",
        "    total_blocks = len(rows) * len(cols)\n",
        "\n",
        "    with tqdm(total=total_blocks, desc=\"Aggregating Matrices\", unit=\"block\") as pbar:\n",
        "        for r_off in rows:\n",
        "            for c_off in cols:\n",
        "                window = Window(\n",
        "                    c_off, r_off,\n",
        "                    min(block_size, width - c_off),\n",
        "                    min(block_size, height - r_off)\n",
        "                )\n",
        "\n",
        "                # Load block stack\n",
        "                stack_list = []\n",
        "                combined_mask = None\n",
        "\n",
        "                for src in srcs:\n",
        "                    data = src.read(1, window=window)\n",
        "\n",
        "                    # Create mask\n",
        "                    mask = (data == no_data_value)\n",
        "                    if src.nodata is not None:\n",
        "                        mask |= (data == src.nodata)\n",
        "\n",
        "                    if combined_mask is None:\n",
        "                        combined_mask = mask\n",
        "                    else:\n",
        "                        combined_mask |= mask\n",
        "\n",
        "                    stack_list.append(data)\n",
        "\n",
        "                # Filter valid pixels\n",
        "                valid_mask = ~combined_mask\n",
        "                if not np.any(valid_mask):\n",
        "                    pbar.update(1)\n",
        "                    continue\n",
        "\n",
        "                # Flatten valid pixels: Shape becomes (n_valid_pixels, n_times)\n",
        "                # Transpose after stacking to get pixels as rows\n",
        "                flat_stack = np.array([layer[valid_mask] for layer in stack_list]).T.astype(np.int32)\n",
        "\n",
        "                # Call Numba Function\n",
        "                _process_block_accumulation(\n",
        "                    flat_stack,\n",
        "                    mat_sum,\n",
        "                    mat_ext,\n",
        "                    mat_ax,\n",
        "                    mat_as,\n",
        "                    lut,\n",
        "                    n_classes\n",
        "                )\n",
        "\n",
        "                pbar.update(1)\n",
        "\n",
        "    # Close files\n",
        "    for src in srcs:\n",
        "        src.close()\n",
        "\n",
        "    # 3. Final Decomposition for Extent (Aggregate Level)\n",
        "    # Eq 1 & 2: Decompose Extent into Exchange and Shift (Allocation)\n",
        "    # Allocation Exchange = min(E, E.T)\n",
        "    mat_alloc_ex = np.maximum(0, np.minimum(mat_ext, mat_ext.T))\n",
        "    # Allocation Shift = E - Allocation Exchange\n",
        "    mat_alloc_sh = mat_ext - mat_alloc_ex\n",
        "\n",
        "    results = {\n",
        "        \"sum\": mat_sum,\n",
        "        \"extent\": mat_ext,\n",
        "        \"allocation_exchange\": mat_alloc_ex,\n",
        "        \"allocation_shift\": mat_alloc_sh,\n",
        "        \"alternation_exchange\": mat_ax,\n",
        "        \"alternation_shift\": mat_as,\n",
        "    }\n",
        "\n",
        "    # 4. Save to CSV\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # Extract years for filename\n",
        "    years = []\n",
        "    for path in image_paths:\n",
        "        match = re.search(r\"(\\d{4})\\.tif$\", os.path.basename(path))\n",
        "        years.append(int(match.group(1)) if match else 0)\n",
        "    interval_str = f\"{years[0]}-{years[-1]}\"\n",
        "\n",
        "    for name, mat in results.items():\n",
        "        fname = f\"transition_matrix_{name}_{interval_str}.csv\"\n",
        "        out_csv = os.path.join(output_dir, fname)\n",
        "        pd.DataFrame(\n",
        "            mat,\n",
        "            index=all_classes_array,\n",
        "            columns=all_classes_array\n",
        "        ).to_csv(out_csv)\n",
        "\n",
        "    print(f\"Aggregated matrices saved to {output_dir}\")\n",
        "    return results\n",
        "\n",
        "# Execution of the optimized aggregation function with defined parameters\n",
        "\n",
        "# Check if classes_found is available; if not, recover it from the dictionary\n",
        "if 'classes_found' not in locals():\n",
        "    if 'class_labels_dict' in locals():\n",
        "        print(\"Recovering 'classes_found' from class_labels_dict keys...\")\n",
        "        classes_found = np.array(sorted(class_labels_dict.keys()), dtype=np.int32)\n",
        "    else:\n",
        "        # Simple fallback scan if dictionary is also missing (unlikely)\n",
        "        print(\"Classes not found. Performing fast scan...\")\n",
        "        # Just a placeholder for robust code, in this context we likely have the dict\n",
        "        pass\n",
        "\n",
        "aggregated_results = calculate_aggregated_matrices_optimized(\n",
        "    file_list=image_paths,\n",
        "    output_dir=output_path,\n",
        "    no_data_value=noData_value,\n",
        "    all_classes_array=classes_found # Use the array returned by the previous function\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4vyA5H8KggI"
      },
      "source": [
        "## **5.Heat Maps**\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-YBpgLqKggI"
      },
      "source": [
        "### 5.1 Input Setup and Integrity Checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYymo6z3bmN9"
      },
      "outputs": [],
      "source": [
        "# 1. Validate global inputs\n",
        "assert isinstance(\n",
        "    years,\n",
        "    (\n",
        "        list,\n",
        "        tuple,\n",
        "    ),\n",
        ") and len(years) >= 2, \"`years` missing or invalid.\"\n",
        "\n",
        "assert isinstance(\n",
        "    output_path,\n",
        "    str,\n",
        ") and output_path, \"`output_path` missing or invalid.\"\n",
        "\n",
        "assert isinstance(\n",
        "    class_labels_dict,\n",
        "    dict,\n",
        ") and class_labels_dict, \"`class_labels_dict` missing or invalid.\"\n",
        "\n",
        "\n",
        "# 2. Helper to extract year digits for filename matching\n",
        "def _extract_year_str(\n",
        "    val,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Extract the first sequence of digits from a year string or integer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    val : str or int\n",
        "        The value containing the year (e.g., \"time_2000\" or 2000).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        The extracted year digits.\n",
        "    \"\"\"\n",
        "    match = re.search(\n",
        "        r\"(\\d+)\",\n",
        "        str(val),\n",
        "    )\n",
        "    return match.group(1) if match else str(val)\n",
        "\n",
        "\n",
        "str_y0 = _extract_year_str(\n",
        "    years[0],\n",
        ")\n",
        "str_y1 = _extract_year_str(\n",
        "    years[-1],\n",
        ")\n",
        "\n",
        "# 3. Define paths for all 6 generated matrices\n",
        "CSV_SUM = os.path.join(\n",
        "    output_path,\n",
        "    f\"transition_matrix_sum_{str_y0}-{str_y1}.csv\",\n",
        ")\n",
        "CSV_EXT = os.path.join(\n",
        "    output_path,\n",
        "    f\"transition_matrix_extent_{str_y0}-{str_y1}.csv\",\n",
        ")\n",
        "CSV_EXT_EXC = os.path.join(\n",
        "    output_path,\n",
        "    f\"transition_matrix_allocation_exchange_{str_y0}-{str_y1}.csv\",\n",
        ")\n",
        "CSV_EXT_SHIFT = os.path.join(\n",
        "    output_path,\n",
        "    f\"transition_matrix_allocation_shift_{str_y0}-{str_y1}.csv\",\n",
        ")\n",
        "CSV_ALT_EXC = os.path.join(\n",
        "    output_path,\n",
        "    f\"transition_matrix_alternation_exchange_{str_y0}-{str_y1}.csv\",\n",
        ")\n",
        "CSV_ALT_SHIFT = os.path.join(\n",
        "    output_path,\n",
        "    f\"transition_matrix_alternation_shift_{str_y0}-{str_y1}.csv\",\n",
        ")\n",
        "\n",
        "# 4. Check for existence of all required files\n",
        "required_files = [\n",
        "    CSV_SUM,\n",
        "    CSV_EXT,\n",
        "    CSV_EXT_EXC,\n",
        "    CSV_EXT_SHIFT,\n",
        "    CSV_ALT_EXC,\n",
        "    CSV_ALT_SHIFT,\n",
        "]\n",
        "\n",
        "for path_csv in required_files:\n",
        "    if not os.path.exists(path_csv):\n",
        "        raise FileNotFoundError(\n",
        "            f\"Missing required CSV: {path_csv}\",\n",
        "        )\n",
        "\n",
        "print(\n",
        "    \"Found all base CSVs.\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJjIkM9rbyzL"
      },
      "source": [
        "### 5.2 Data Transformation and Matrix Alignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpAkKtRacCzS"
      },
      "outputs": [],
      "source": [
        "def load_square_matrix(\n",
        "    csv_path: str,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load a square transition matrix from CSV and align row/column labels.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    csv_path : str\n",
        "        Path to a CSV file where the first column and the header row\n",
        "        contain class IDs (or labels), and the remaining cells contain\n",
        "        transition counts.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Square DataFrame with string labels on both rows and columns.\n",
        "        When row and column labels differ, their union is used and\n",
        "        missing cells are filled with 0.0.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(\n",
        "        csv_path,\n",
        "        index_col=0,\n",
        "    )\n",
        "    df.index = df.index.map(str)\n",
        "    df.columns = df.columns.map(str)\n",
        "\n",
        "    if list(df.index) != list(df.columns):\n",
        "        labels = sorted(\n",
        "            set(df.index).union(df.columns),\n",
        "            key=lambda x: int(x),\n",
        "        )\n",
        "        df = df.reindex(\n",
        "            index=labels,\n",
        "            columns=labels,\n",
        "        ).fillna(0.0)\n",
        "\n",
        "    if df.shape[0] != df.shape[1]:\n",
        "        raise ValueError(\n",
        "            f\"Matrix not square after alignment: {csv_path}\",\n",
        "        )\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def label_id_to_name(\n",
        "    labels: Iterable[str],\n",
        ") -> list[str]:\n",
        "    \"\"\"\n",
        "    Map class ID strings to human-readable names using ``class_labels_dict``.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    labels : Iterable[str]\n",
        "        Class IDs as strings (e.g. ``[\"0\", \"1\", \"2\"]``).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list[str]\n",
        "        List of class names mapped from IDs.\n",
        "    \"\"\"\n",
        "    id_to_name = {\n",
        "        int(k): v.get(\n",
        "            \"rename\",\n",
        "            v.get(\n",
        "                \"name\",\n",
        "                str(k),\n",
        "            ),\n",
        "        )\n",
        "        for k, v in class_labels_dict.items()\n",
        "    }\n",
        "\n",
        "    names: list[str] = []\n",
        "    for lab in labels:\n",
        "        try:\n",
        "            cid = int(str(lab))\n",
        "            names.append(\n",
        "                id_to_name.get(\n",
        "                    cid,\n",
        "                    str(lab),\n",
        "                ),\n",
        "            )\n",
        "        except Exception:\n",
        "            names.append(str(lab))\n",
        "\n",
        "    return names\n",
        "\n",
        "\n",
        "def compute_net_change_from_sum(\n",
        "    df_sum: pd.DataFrame,\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Compute net change per class from a SUM transition matrix.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_sum : pd.DataFrame\n",
        "        Square transition matrix representing total transitions over\n",
        "        the full time span.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.Series\n",
        "        Net change for each class (gains - losses).\n",
        "    \"\"\"\n",
        "    M = df_sum.values.astype(float).copy()\n",
        "\n",
        "    # Remove persistence on the diagonal for the calculation\n",
        "    np.fill_diagonal(\n",
        "        M,\n",
        "        0.0,\n",
        "    )\n",
        "\n",
        "    gains = M.sum(\n",
        "        axis=0,\n",
        "    )\n",
        "    losses = M.sum(\n",
        "        axis=1,\n",
        "    )\n",
        "\n",
        "    net_change = gains - losses\n",
        "\n",
        "    return pd.Series(\n",
        "        net_change,\n",
        "        index=df_sum.index,\n",
        "    )\n",
        "\n",
        "\n",
        "def reorder_matrices_by_net_change(\n",
        "    df_sum: pd.DataFrame,\n",
        "    df_ext: pd.DataFrame,\n",
        "    df_ext_exc: pd.DataFrame,\n",
        "    df_ext_shift: pd.DataFrame,\n",
        "    df_alt_exc: pd.DataFrame,\n",
        "    df_alt_shift: pd.DataFrame,\n",
        ") -> tuple[\n",
        "    pd.DataFrame,\n",
        "    pd.DataFrame,\n",
        "    pd.DataFrame,\n",
        "    pd.DataFrame,\n",
        "    pd.DataFrame,\n",
        "    pd.DataFrame,\n",
        "]:\n",
        "    \"\"\"\n",
        "    Reorder rows and columns of all matrices using net change from ``df_sum``.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_sum : pd.DataFrame\n",
        "        SUM transition matrix (used for sorting logic).\n",
        "    df_ext : pd.DataFrame\n",
        "        Extent matrix.\n",
        "    df_ext_exc : pd.DataFrame\n",
        "        Extent Exchange matrix.\n",
        "    df_ext_shift : pd.DataFrame\n",
        "        Extent Shift matrix.\n",
        "    df_alt_exc : pd.DataFrame\n",
        "        Alternation Exchange matrix.\n",
        "    df_alt_shift : pd.DataFrame\n",
        "        Alternation Shift matrix.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple[pd.DataFrame, ...]\n",
        "        All input dataframes reindexed so that:\n",
        "        - largest losers (most negative net change) are at the top/left.\n",
        "        - largest gainers (most positive net change) are at the bottom/right.\n",
        "    \"\"\"\n",
        "    net_change = compute_net_change_from_sum(\n",
        "        df_sum=df_sum,\n",
        "    )\n",
        "    order_labels = (\n",
        "        net_change.sort_values(\n",
        "            ascending=True,\n",
        "        )\n",
        "        .index.tolist()\n",
        "    )\n",
        "\n",
        "    def _reorder(\n",
        "        df: pd.DataFrame,\n",
        "    ) -> pd.DataFrame:\n",
        "        return df.reindex(\n",
        "            index=order_labels,\n",
        "            columns=order_labels,\n",
        "        )\n",
        "\n",
        "    return (\n",
        "        _reorder(df_sum),\n",
        "        _reorder(df_ext),\n",
        "        _reorder(df_ext_exc),\n",
        "        _reorder(df_ext_shift),\n",
        "        _reorder(df_alt_exc),\n",
        "        _reorder(df_alt_shift),\n",
        "    )\n",
        "\n",
        "# 1. Load the base Sum and Extent matrices\n",
        "df_sum = load_square_matrix(\n",
        "    CSV_SUM,\n",
        ")\n",
        "df_ext = load_square_matrix(\n",
        "    CSV_EXT,\n",
        ")\n",
        "\n",
        "# 2. Load the 4 Decomposed Component Matrices\n",
        "df_ext_exc = load_square_matrix(\n",
        "    CSV_EXT_EXC,\n",
        ")\n",
        "df_ext_shift = load_square_matrix(\n",
        "    CSV_EXT_SHIFT,\n",
        ")\n",
        "df_alt_exc = load_square_matrix(\n",
        "    CSV_ALT_EXC,\n",
        ")\n",
        "df_alt_shift = load_square_matrix(\n",
        "    CSV_ALT_SHIFT,\n",
        ")\n",
        "\n",
        "# 3. Reorder all matrices based on Net Change (Losers to Gainers)\n",
        "# This function (defined in the previous cell) ensures all 6 matrices\n",
        "# share the exact same row/column order for consistent visualization.\n",
        "(\n",
        "    df_sum,\n",
        "    df_ext,\n",
        "    df_ext_exc,\n",
        "    df_ext_shift,\n",
        "    df_alt_exc,\n",
        "    df_alt_shift,\n",
        ") = reorder_matrices_by_net_change(\n",
        "    df_sum=df_sum,\n",
        "    df_ext=df_ext,\n",
        "    df_ext_exc=df_ext_exc,\n",
        "    df_ext_shift=df_ext_shift,\n",
        "    df_alt_exc=df_alt_exc,\n",
        "    df_alt_shift=df_alt_shift,\n",
        ")\n",
        "\n",
        "print(\"Successfully loaded and reordered all 6 matrices:\",)\n",
        "print(\"1. Sum\",)\n",
        "print(\"2. Extent\",)\n",
        "print(\"3. Allocation Exchange\",)\n",
        "print(\"4. Quantity & Allocation Shift\",)\n",
        "print(\"5. Alternation Exchange\",)\n",
        "print(\"6. Alternation Shift\",)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-swlwc-cI-y"
      },
      "source": [
        "### 5.3 Visualization Engine and Heatmap Rendering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r72eGQUrcMZB"
      },
      "outputs": [],
      "source": [
        "def annotate_heatmap(\n",
        "    ax: plt.Axes,\n",
        "    M: np.ndarray,\n",
        "    fontsize: int = 8,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Annotate a heatmap with integer cell values and adaptive text color.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    ax : matplotlib.axes.Axes\n",
        "        The axes object where the heatmap is plotted.\n",
        "    M : np.ndarray\n",
        "        The matrix containing the values to display.\n",
        "    fontsize : int, optional\n",
        "        The font size of the annotations (default is 8).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "    \"\"\"\n",
        "    if M.size == 0:\n",
        "        return\n",
        "\n",
        "    M_off = M.copy()\n",
        "    np.fill_diagonal(\n",
        "        M_off,\n",
        "        np.nan,\n",
        "    )\n",
        "    data_off = M_off[np.isfinite(M_off)]\n",
        "\n",
        "    has_pos = np.any(data_off > 0)\n",
        "    has_neg = np.any(data_off < 0)\n",
        "\n",
        "    max_pos = float(np.nanmax(data_off[data_off > 0])) if has_pos else 0.0\n",
        "    min_neg = float(np.nanmin(data_off[data_off < 0])) if has_neg else 0.0\n",
        "\n",
        "    thresh_pos = 0.5 * max_pos if has_pos else np.inf\n",
        "    thresh_neg = 0.5 * min_neg if has_neg else -np.inf\n",
        "\n",
        "    for i in range(M.shape[0]):\n",
        "        for j in range(M.shape[1]):\n",
        "            # Skip diagonal annotation to keep it clean black\n",
        "            if i == j:\n",
        "                continue\n",
        "\n",
        "            v = float(M[i, j])\n",
        "            txt = f\"{int(round(v))}\"\n",
        "\n",
        "            if i == j:\n",
        "                color = \"white\"\n",
        "            else:\n",
        "                if (\n",
        "                    has_pos\n",
        "                    and v >= thresh_pos\n",
        "                ) or (\n",
        "                    has_neg\n",
        "                    and v <= thresh_neg\n",
        "                ):\n",
        "                    color = \"white\"\n",
        "                else:\n",
        "                    color = \"black\"\n",
        "\n",
        "            ax.text(\n",
        "                j,\n",
        "                i,\n",
        "                txt,\n",
        "                ha=\"center\",\n",
        "                va=\"center\",\n",
        "                fontsize=fontsize,\n",
        "                color=color,\n",
        "                clip_on=True,\n",
        "            )\n",
        "\n",
        "\n",
        "def _unit_label(\n",
        "    suffix: str,\n",
        "    base_label: str = \"number of pixels\",\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Build a descriptive label for the colorbar based on a unit suffix.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    suffix : str\n",
        "        The suffix for the unit (e.g., 'k', 'M').\n",
        "    base_label : str, optional\n",
        "        The base label text (default is \"Number of pixels\").\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        The formatted unit label.\n",
        "    \"\"\"\n",
        "    mapping = {\n",
        "        \"\": base_label,\n",
        "        \"hundreds\": \"hundred pixels\",\n",
        "        \"k\": \"thousand pixels\",\n",
        "        \"M\": \"million pixels\",\n",
        "        \"B\": \"billion pixels\",\n",
        "        \"T\": \"trillion pixels\",\n",
        "    }\n",
        "\n",
        "    return mapping.get(\n",
        "        suffix,\n",
        "        f\"{base_label} ({suffix})\",\n",
        "    )\n",
        "\n",
        "\n",
        "def _unit_formatter(\n",
        "    factor: float,\n",
        "    suffix: str,\n",
        "    decimals: int = 1,\n",
        ") -> mticker.FuncFormatter:\n",
        "    \"\"\"\n",
        "    Build a tick formatter that scales values and appends a suffix.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    factor : float\n",
        "        The factor to divide the values by (e.g., 1000 for 'k').\n",
        "    suffix : str\n",
        "        The string suffix to append to the formatted value.\n",
        "    decimals : int, optional\n",
        "        The number of decimal places to include (default is 1).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    matplotlib.ticker.FuncFormatter\n",
        "        A formatter function for the plot ticks.\n",
        "    \"\"\"\n",
        "    fmt = f\"{{:.{decimals}f}}{suffix}\"\n",
        "\n",
        "    def _fmt(\n",
        "        x: float,\n",
        "        pos: int,\n",
        "    ) -> str:\n",
        "        return fmt.format(x / factor)\n",
        "\n",
        "    return mticker.FuncFormatter(_fmt)\n",
        "\n",
        "\n",
        "def plot_heatmap(\n",
        "    df: pd.DataFrame,\n",
        "    title: str,\n",
        "    save_path: Optional[str] = None,\n",
        "    figsize: Optional[Tuple[float, float]] = None,\n",
        "    cmap: str = \"YlOrRd\",\n",
        "    vmin: float = 0.0,\n",
        "    vmax: Optional[float] = None,\n",
        "    rotate_xticks_deg: int = 90,\n",
        "    cbar_label: str = \"Number of pixels\",\n",
        "    annotate: bool = True,\n",
        "    cell_size_inch: float = 0.8,\n",
        "    tick_fontsize: Optional[int] = None,\n",
        "    ann_fontsize: int = 8,\n",
        "    cbar_fraction: float = 0.025,\n",
        "    cbar_pad: float = 0.02,\n",
        "    tick_fontsize_x: Optional[int] = None,\n",
        "    tick_fontsize_y: Optional[int] = None,\n",
        "    axis_label_fontsize: Optional[int] = None,\n",
        "    title_fontsize: Optional[int] = None,\n",
        "    cbar_tick_labelsize: Optional[int] = None, # New parameter\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Plot a square matrix as a heatmap with adaptive integer colorbar and dual scaling.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        The square dataframe to plot.\n",
        "    title : str\n",
        "        The title of the plot.\n",
        "    save_path : str, optional\n",
        "        Path to save the figure image.\n",
        "    figsize : tuple, optional\n",
        "        Figure size in inches (width, height).\n",
        "    cmap : str, optional\n",
        "        Colormap name (default is \"YlOrRd\").\n",
        "    vmin : float, optional\n",
        "        Minimum value for colormap scaling.\n",
        "    vmax : float, optional\n",
        "        Maximum value for colormap scaling.\n",
        "    rotate_xticks_deg : int, optional\n",
        "        Rotation angle for x-axis ticks.\n",
        "    cbar_label : str, optional\n",
        "        Label for the colorbar.\n",
        "    annotate : bool, optional\n",
        "        Whether to annotate cells with values.\n",
        "    cell_size_inch : float, optional\n",
        "        Size of each cell in inches used for auto-figsize.\n",
        "    tick_fontsize : int, optional\n",
        "        Deprecated; use tick_fontsize_x/y.\n",
        "    ann_fontsize : int, optional\n",
        "        Font size for annotations.\n",
        "    cbar_fraction : float, optional\n",
        "        Fraction of original axes to use for colorbar.\n",
        "    cbar_pad : float, optional\n",
        "        Padding between axes and colorbar.\n",
        "    tick_fontsize_x : int, optional\n",
        "        Font size for x-axis ticks.\n",
        "    tick_fontsize_y : int, optional\n",
        "        Font size for y-axis ticks.\n",
        "    axis_label_fontsize : int, optional\n",
        "        Font size for axis labels.\n",
        "    title_fontsize : int, optional\n",
        "        Font size for the title.\n",
        "    cbar_tick_labelsize : int, optional\n",
        "        Font size for colorbar tick labels.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "    \"\"\"\n",
        "    if tick_fontsize_x is None or tick_fontsize_y is None:\n",
        "        raise ValueError(\n",
        "            \"Set `tick_fontsize_x` and `tick_fontsize_y` explicitly.\",\n",
        "        )\n",
        "\n",
        "    if axis_label_fontsize is None:\n",
        "        axis_label_fontsize = 12\n",
        "\n",
        "    if title_fontsize is None:\n",
        "        title_fontsize = 14\n",
        "\n",
        "    labels = list(\n",
        "        df.index,\n",
        "    )\n",
        "    matrix_values = df.values.astype(\n",
        "        float,\n",
        "    )\n",
        "\n",
        "    # Prepare scale ignoring diagonal to highlight transitions\n",
        "    matrix_scale = matrix_values.copy()\n",
        "    np.fill_diagonal(\n",
        "        matrix_scale,\n",
        "        0.0,\n",
        "    )\n",
        "    finite_vals = matrix_scale[\n",
        "        np.isfinite(\n",
        "            matrix_scale,\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Define effective limits for the color scale\n",
        "    if finite_vals.size == 0:\n",
        "        has_negative = False\n",
        "        vmin_eff, vmax_eff = 0.0, 1.0\n",
        "    else:\n",
        "        has_negative = float(np.nanmin(finite_vals)) < 0.0\n",
        "        min_val = float(np.nanmin(finite_vals))\n",
        "        max_val = float(np.nanmax(finite_vals))\n",
        "\n",
        "        if has_negative:\n",
        "            # Use data bounds for divergent scaling\n",
        "            vmin_eff, vmax_eff = min_val, max_val\n",
        "        else:\n",
        "            vmin_eff = vmin\n",
        "            vmax_eff = float(max_val) if vmax is None else float(vmax)\n",
        "\n",
        "        if vmin_eff == vmax_eff:\n",
        "            vmax_eff += 1.0\n",
        "\n",
        "    nrows, ncols = df.shape\n",
        "    if figsize is None:\n",
        "        figsize = (\n",
        "            cell_size_inch * ncols,\n",
        "            cell_size_inch * nrows,\n",
        "        )\n",
        "\n",
        "    fig, ax = plt.subplots(\n",
        "        figsize=figsize,\n",
        "        constrained_layout=True,\n",
        "    )\n",
        "\n",
        "    # Plot layers (Negative in Blues, Positive in YlOrRd)\n",
        "    cmap_neg = mcolors.LinearSegmentedColormap.from_list(\"CustomBlues\", [\"#08306b\", \"#b3e0ff\"])\n",
        "    if has_negative:\n",
        "        matrix_pos = np.ma.masked_less_equal(matrix_values, 0.0)\n",
        "        norm_pos = mcolors.Normalize(vmin=0.0, vmax=vmax_eff)\n",
        "        ax.imshow(\n",
        "            matrix_pos,\n",
        "            aspect=\"equal\",\n",
        "            cmap=plt.cm.YlOrRd,\n",
        "            norm=norm_pos,\n",
        "        )\n",
        "        matrix_neg = np.ma.masked_where(matrix_values >= 0.0, matrix_values)\n",
        "        norm_neg = mcolors.Normalize(vmin=vmin_eff, vmax=0.0)\n",
        "        ax.imshow(\n",
        "            matrix_neg,\n",
        "            aspect=\"equal\",\n",
        "            cmap=cmap_neg,\n",
        "            norm=norm_neg,\n",
        "        )\n",
        "    else:\n",
        "        matrix_pos = np.ma.masked_equal(matrix_values, 0.0)\n",
        "        norm_pos = mcolors.Normalize(vmin=vmin_eff, vmax=vmax_eff)\n",
        "        ax.imshow(\n",
        "            matrix_pos,\n",
        "            aspect=\"equal\",\n",
        "            cmap=plt.cm.YlOrRd,\n",
        "            norm=norm_pos,\n",
        "        )\n",
        "\n",
        "    # Overlay black diagonal\n",
        "    diag_mask = np.eye(nrows, dtype=bool)\n",
        "    matrix_diag = np.ma.masked_where(~diag_mask, np.ones_like(matrix_values))\n",
        "    ax.imshow(\n",
        "        matrix_diag,\n",
        "        aspect=\"equal\",\n",
        "        cmap=mcolors.ListedColormap([\"black\"]),\n",
        "        vmin=0,\n",
        "        vmax=1,\n",
        "    )\n",
        "\n",
        "    # Axis formatting\n",
        "    ax.set_xticks(range(len(labels)))\n",
        "    ax.set_yticks(range(len(labels)))\n",
        "    tick_names = label_id_to_name(labels)\n",
        "\n",
        "    ax.set_xticklabels(\n",
        "        tick_names,\n",
        "        rotation=rotate_xticks_deg,\n",
        "        fontsize=tick_fontsize_x,\n",
        "    )\n",
        "    ax.set_yticklabels(\n",
        "        tick_names,\n",
        "        fontsize=tick_fontsize_y,\n",
        "    )\n",
        "    ax.set_xlabel(\"To class\", fontsize=axis_label_fontsize)\n",
        "    ax.set_ylabel(\"From class\", fontsize=axis_label_fontsize)\n",
        "    ax.set_title(title, fontsize=title_fontsize)\n",
        "\n",
        "    # Build continuous colorbar\n",
        "    n_bar = 256\n",
        "    vals = np.linspace(vmin_eff, vmax_eff, n_bar)\n",
        "    colors_bar = []\n",
        "    for v in vals:\n",
        "        if has_negative and v < 0.0:\n",
        "            # Scale negative values to Blues map\n",
        "            t = (v - vmin_eff) / (0.0 - vmin_eff) if vmin_eff < 0 else 0\n",
        "            colors_bar.append(cmap_neg(t))\n",
        "        elif v == 0:\n",
        "          colors_bar.append((1.0, 1.0, 1.0, 1.0))\n",
        "        else:\n",
        "            # Scale positive values to YlOrRd map\n",
        "            t = max(0.0, v) / vmax_eff if vmax_eff > 0.0 else 0.0\n",
        "            colors_bar.append(plt.cm.YlOrRd(t))\n",
        "\n",
        "    cmap_bar = mcolors.ListedColormap(colors_bar)\n",
        "    norm_bar = mcolors.Normalize(vmin=vmin_eff, vmax=vmax_eff)\n",
        "    scalar_mappable = plt.cm.ScalarMappable(cmap=cmap_bar, norm=norm_bar)\n",
        "    scalar_mappable.set_array([])\n",
        "\n",
        "    # Create Colorbar\n",
        "    cbar = fig.colorbar(\n",
        "        scalar_mappable,\n",
        "        ax=ax,\n",
        "        fraction=cbar_fraction,\n",
        "        pad=cbar_pad,\n",
        "    )\n",
        "\n",
        "    # Adaptive unit scaling\n",
        "    max_abs = float(np.nanmax(np.abs(finite_vals))) if finite_vals.size > 0 else 0.0\n",
        "    if max_abs >= 1_000_000_000_000:\n",
        "        factor, suffix = 1_000_000_000_000.0, \"T\"\n",
        "    elif max_abs >= 1_000_000_000:\n",
        "        factor, suffix = 1_000_000_000.0, \"B\"\n",
        "    elif max_abs >= 1_000_000:\n",
        "        factor, suffix = 1_000_000.0, \"M\"\n",
        "    elif max_abs >= 1_000:\n",
        "        factor, suffix = 1_000.0, \"k\"\n",
        "    elif max_abs >= 100:\n",
        "        factor, suffix = 100.0, \"hundreds\"\n",
        "    else:\n",
        "        factor, suffix = 1.0, \"\"\n",
        "\n",
        "    # Use Integer Locator correctly for Toy Data and small ranges\n",
        "    cbar.locator = mticker.MaxNLocator(\n",
        "        nbins=6,\n",
        "        integer=True,\n",
        "        steps=[1, 2, 5, 10]\n",
        "    )\n",
        "    cbar.formatter = _unit_formatter(\n",
        "        factor=factor,\n",
        "        suffix=\"\",\n",
        "        decimals=0,\n",
        "    )\n",
        "    cbar.set_label(\n",
        "        _unit_label(suffix, base_label=cbar_label),\n",
        "        rotation=270,\n",
        "        labelpad=15,\n",
        "        fontsize=8,\n",
        "    )\n",
        "    # Apply cbar_tick_labelsize if provided\n",
        "    if cbar_tick_labelsize is not None:\n",
        "        cbar.ax.tick_params(labelsize=cbar_tick_labelsize)\n",
        "\n",
        "    cbar.update_ticks()\n",
        "\n",
        "    if annotate:\n",
        "        annotate_heatmap(\n",
        "            ax=ax,\n",
        "            M=matrix_values,\n",
        "            fontsize=ann_fontsize,\n",
        "        )\n",
        "\n",
        "    if save_path:\n",
        "        fig.savefig(\n",
        "            save_path,\n",
        "            dpi=300,\n",
        "            bbox_inches=\"tight\",\n",
        "        )\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# 1. Define start and end years for file naming\n",
        "Y0 = years[0]\n",
        "Y1 = years[-1]\n",
        "\n",
        "# 2. Define Output Paths using os.path.join for consistency\n",
        "# Sum Matrix\n",
        "FIG_SUM = os.path.join(\n",
        "    output_path,\n",
        "    f\"heatmap_sum_{Y0}-{Y1}.png\",\n",
        ")\n",
        "\n",
        "# Allocation components\n",
        "FIG_EXT_EXC = os.path.join(\n",
        "    output_path,\n",
        "    f\"heatmap_allocation_exchange_{Y0}-{Y1}.png\",\n",
        ")\n",
        "FIG_EXT_SHIFT = os.path.join(\n",
        "    output_path,\n",
        "    f\"heatmap_quantiy_allocation_shift_{Y0}-{Y1}.png\",\n",
        ")\n",
        "\n",
        "# Alternation components\n",
        "FIG_ALT_EXC = os.path.join(\n",
        "    output_path,\n",
        "    f\"heatmap_alternation_exchange_{Y0}...{Y1}.png\",\n",
        ")\n",
        "FIG_ALT_SHIFT = os.path.join(\n",
        "    output_path,\n",
        "    f\"heatmap_alternation_shift_{Y0}...{Y1}.png\",\n",
        ")\n",
        "\n",
        "# 3. List of matrices to plot\n",
        "plots_config = [\n",
        "    (\n",
        "        df_sum,\n",
        "        f\"Sum {Y0}...{Y1}\",\n",
        "        FIG_SUM,\n",
        "    ),\n",
        "    (\n",
        "        df_ext_exc,\n",
        "        f\"Allocation Exchange {Y0}-{Y1}\",\n",
        "        FIG_EXT_EXC,\n",
        "    ),\n",
        "    (\n",
        "        df_ext_shift,\n",
        "        f\"Quantity & Allocation Shift {Y0}-{Y1}\",\n",
        "        FIG_EXT_SHIFT,\n",
        "    ),\n",
        "    (\n",
        "        df_alt_exc,\n",
        "        f\"Alternation Exchange {Y0}...{Y1}\",\n",
        "        FIG_ALT_EXC,\n",
        "    ),\n",
        "    (\n",
        "        df_alt_shift,\n",
        "        f\"Alternation Shift {Y0}...{Y1}\",\n",
        "        FIG_ALT_SHIFT,\n",
        "    ),\n",
        "]\n",
        "\n",
        "# 4. Iterate and plot each heatmap\n",
        "for df, title, path in plots_config:\n",
        "    plot_heatmap(\n",
        "        df=df,\n",
        "        title=title,\n",
        "        save_path=path,\n",
        "        tick_fontsize_x=9,\n",
        "        tick_fontsize_y=9,\n",
        "        axis_label_fontsize=8,\n",
        "        title_fontsize=10,\n",
        "        ann_fontsize=7,\n",
        "        cbar_fraction=0.025,\n",
        "        cbar_pad=0.02,\n",
        "        cbar_tick_labelsize=7, # Added new parameter\n",
        "    )\n",
        "\n",
        "print(\"Saved all heat maps:\",)\n",
        "print(f\"1. {FIG_SUM}\",)\n",
        "print(f\"2. {FIG_EXT_EXC}\",)\n",
        "print(f\"3. {FIG_EXT_SHIFT}\",)\n",
        "print(f\"4. {FIG_ALT_EXC}\",)\n",
        "print(f\"5. {FIG_ALT_SHIFT}\",)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFz2g-KWIot8"
      },
      "source": [
        "## **6.Components of Change**\n",
        "\n",
        "\n",
        "---\n",
        "The code calculates components of change from transition matrices generated in the previous step. It features a ComponentCalculator class that processes matrices to determine the gain and loss of Quantity, Exchange, and Shift. The process_matrix function handles matrices for defined time intervals and the main function systematically processes these matrices for each time interval, aggregates the results, and exports the outcomes to a CSV file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCNyrys2w-S_"
      },
      "source": [
        "### 6.1 Compute Components of Change"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cba86-aOKggJ"
      },
      "outputs": [],
      "source": [
        "class ComponentCalculator:\n",
        "    \"\"\"\n",
        "    Compute change components for a matrix.\n",
        "\n",
        "    Supports pre-decomposed matrices via force_component parameter.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        transition_matrix: np.ndarray,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the calculator with a transition matrix.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        transition_matrix : np.ndarray\n",
        "            Square matrix representing transitions.\n",
        "        \"\"\"\n",
        "        self.matrix = transition_matrix.astype(\n",
        "            float,\n",
        "        )\n",
        "        self.num_classes = transition_matrix.shape[0]\n",
        "        self.class_components: list[dict] = []\n",
        "\n",
        "    def calculate_components(\n",
        "        self,\n",
        "        force_component: str = None,\n",
        "    ) -> \"ComponentCalculator\":\n",
        "        \"\"\"\n",
        "        Calculate gain, loss, exchange, and shift for all classes.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        force_component : str, optional\n",
        "            If set to \"Exchange\" or \"Shift\", forces the interpretation of the\n",
        "            matrix content to that specific component.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ComponentCalculator\n",
        "            Returns self for chaining.\n",
        "        \"\"\"\n",
        "        for class_idx in range(\n",
        "            self.num_classes,\n",
        "        ):\n",
        "            gain_sum = np.sum(\n",
        "                self.matrix[:, class_idx],\n",
        "            )\n",
        "            loss_sum = np.sum(\n",
        "                self.matrix[class_idx, :],\n",
        "            )\n",
        "\n",
        "            # Standard net change calculation\n",
        "            q_gain = max(\n",
        "                0.0,\n",
        "                gain_sum - loss_sum,\n",
        "            )\n",
        "            q_loss = max(\n",
        "                0.0,\n",
        "                loss_sum - gain_sum,\n",
        "            )\n",
        "\n",
        "            if force_component == \"Exchange\":\n",
        "                # Matrix content is treated purely as exchange\n",
        "                exchange = loss_sum - self.matrix[class_idx, class_idx]\n",
        "                shift = 0.0\n",
        "                q_gain, q_loss = (\n",
        "                    gain_sum - loss_sum\n",
        "                ), (\n",
        "                    loss_sum - gain_sum\n",
        "                )\n",
        "            elif force_component == \"Shift\":\n",
        "                # Matrix content is treated purely as shift\n",
        "                exchange = 0.0\n",
        "                shift = loss_sum - self.matrix[class_idx, class_idx]\n",
        "                q_gain, q_loss = 0.0, 0.0\n",
        "            else:\n",
        "                # Standard Pontius decomposition\n",
        "                mutual = np.sum(\n",
        "                    np.minimum(\n",
        "                        self.matrix[class_idx, :],\n",
        "                        self.matrix[:, class_idx],\n",
        "                    ),\n",
        "                )\n",
        "                exchange = mutual - self.matrix[class_idx, class_idx]\n",
        "                total_trans = loss_sum - self.matrix[class_idx, class_idx]\n",
        "                shift = total_trans - q_loss - exchange\n",
        "\n",
        "            self.class_components.append(\n",
        "                {\n",
        "                    \"Quantity_Gain\": q_gain,\n",
        "                    \"Quantity_Loss\": q_loss,\n",
        "                    \"Exchange_Gain\": exchange,\n",
        "                    \"Exchange_Loss\": exchange,\n",
        "                    \"Shift_Gain\": shift,\n",
        "                    \"Shift_Loss\": shift,\n",
        "                },\n",
        "            )\n",
        "        return self\n",
        "\n",
        "\n",
        "def process_matrix(\n",
        "    matrix_type,\n",
        "    output_path,\n",
        "    years_list,\n",
        "    start_year=None,\n",
        "    end_year=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Search for a transition matrix file and calculate its change components.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    matrix_type : str\n",
        "        Type of matrix (\"interval\", \"extent\", \"sum\", etc.).\n",
        "    output_path : str\n",
        "        Directory where CSV files are stored.\n",
        "    years_list : list\n",
        "        List of all years in the timeline.\n",
        "    start_year : str or int, optional\n",
        "        Start year for interval matrices.\n",
        "    end_year : str or int, optional\n",
        "        End year for interval matrices.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list[dict]\n",
        "        List of dictionaries containing component values per class.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    # 1. Determine naming patterns to search for (Short vs Long names)\n",
        "    patterns = []\n",
        "\n",
        "    if matrix_type == \"interval\":\n",
        "        # Try \"time_0-time_1\" AND \"0-1\"\n",
        "        s_str, e_str = str(start_year), str(end_year)\n",
        "        patterns.append(\n",
        "            f\"transition_matrix_{s_str}-{e_str}.csv\",\n",
        "        )\n",
        "        patterns.append(\n",
        "            f\"transition_matrix_{s_str.replace('time_', '')}-{e_str.replace('time_', '')}.csv\",\n",
        "        )\n",
        "        label_time = f\"{s_str}-{e_str}\"\n",
        "    else:\n",
        "        # Try \"sum_time_0-time_2\" AND \"sum_0-2\"\n",
        "        y0_str, yN_str = str(years_list[0]), str(years_list[-1])\n",
        "        patterns.append(\n",
        "            f\"transition_matrix_{matrix_type}_{y0_str}-{yN_str}.csv\",\n",
        "        )\n",
        "        patterns.append(\n",
        "            f\"transition_matrix_{matrix_type}_{y0_str.replace('time_', '')}-{yN_str.replace('time_', '')}.csv\",\n",
        "        )\n",
        "        label_time = matrix_type\n",
        "\n",
        "    # 2. Find the existing file\n",
        "    full_path = None\n",
        "    for p in patterns:\n",
        "        path = os.path.join(\n",
        "            output_path,\n",
        "            p,\n",
        "        )\n",
        "        if os.path.exists(path):\n",
        "            full_path = path\n",
        "            break\n",
        "\n",
        "    if not full_path:\n",
        "        return []\n",
        "\n",
        "    # 3. Process components\n",
        "    force_comp = (\n",
        "        \"Exchange\"\n",
        "        if \"exchange\" in matrix_type\n",
        "        else (\"Shift\" if \"shift\" in matrix_type else None)\n",
        "    )\n",
        "\n",
        "    df_mat = pd.read_csv(\n",
        "        full_path,\n",
        "        index_col=0,\n",
        "    )\n",
        "    calc = ComponentCalculator(\n",
        "        df_mat.values,\n",
        "    ).calculate_components(\n",
        "        force_component=force_comp,\n",
        "    )\n",
        "\n",
        "    for idx, class_id in enumerate(\n",
        "        [int(c) for c in df_mat.index],\n",
        "    ):\n",
        "        # Use simple \"Class X\" if class_labels_dict is missing or generic\n",
        "        cls_name = class_labels_dict.get(\n",
        "            class_id,\n",
        "            {},\n",
        "        ).get(\n",
        "            \"name\",\n",
        "            f\"Class {class_id}\",\n",
        "        )\n",
        "        comp_vals = calc.class_components[idx]\n",
        "\n",
        "        for comp_name in [\"Quantity\", \"Exchange\", \"Shift\"]:\n",
        "            # Standardize component labels for the plot\n",
        "            label_comp = comp_name\n",
        "            if matrix_type in [\"extent\", \"sum\"]:\n",
        "                label_comp = f\"Allocation_{comp_name}\"\n",
        "            if \"alternation\" in matrix_type:\n",
        "                label_comp = f\"Alternation_{comp_name}\"\n",
        "\n",
        "            results.append(\n",
        "                {\n",
        "                    \"Time_Interval\": label_time,\n",
        "                    \"Class\": cls_name,\n",
        "                    \"Component\": label_comp,\n",
        "                    \"Gain\": comp_vals[f\"{comp_name}_Gain\"],\n",
        "                    \"Loss\": comp_vals[f\"{comp_name}_Loss\"],\n",
        "                },\n",
        "            )\n",
        "    return results\n",
        "\n",
        "\n",
        "def main(\n",
        "    output_path,\n",
        "):\n",
        "    \"\"\"\n",
        "    Main execution function to process all interval and aggregate matrices.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    output_path : str\n",
        "        Path to the output directory.\n",
        "    \"\"\"\n",
        "    all_results = []\n",
        "    # 1) Annual Intervals\n",
        "    for i in range(\n",
        "        len(years) - 1,\n",
        "    ):\n",
        "        # Apply Item 6: Clean year extraction for intervals\n",
        "        # Assuming _extract_year_str is defined in previous cells\n",
        "        y_start = _extract_year_str(\n",
        "            years[i],\n",
        "        )\n",
        "        y_end = _extract_year_str(\n",
        "            years[i + 1],\n",
        "        )\n",
        "\n",
        "        all_results.extend(\n",
        "            process_matrix(\n",
        "                \"interval\",\n",
        "                output_path,\n",
        "                years,\n",
        "                y_start,\n",
        "                y_end,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    # 2) Aggregated Matrices\n",
        "    for mtype in [\n",
        "        \"extent\",\n",
        "        \"sum\",\n",
        "        \"alternation_exchange\",\n",
        "        \"alternation_shift\",\n",
        "    ]:\n",
        "        all_results.extend(\n",
        "            process_matrix(\n",
        "                mtype,\n",
        "                output_path,\n",
        "                years,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    df_out = pd.DataFrame(\n",
        "        all_results,\n",
        "    )\n",
        "    output_file = os.path.join(\n",
        "        output_path,\n",
        "        \"change_components.csv\",\n",
        "    )\n",
        "    df_out.to_csv(\n",
        "        output_file,\n",
        "        index=False,\n",
        "    )\n",
        "    print(\n",
        "        f\"Success! Final components saved to {output_file}\",\n",
        "    )\n",
        "\n",
        "\n",
        "# Execute main function\n",
        "main(\n",
        "    output_path=output_path,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVn7p6Q7gOkZ"
      },
      "source": [
        "### 6.2 Plot graphics\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lzJWlWIw-S_"
      },
      "outputs": [],
      "source": [
        "# Read change components CSV\n",
        "csv_path = os.path.join(\n",
        "    output_path,\n",
        "    \"change_components.csv\",\n",
        ")\n",
        "\n",
        "# Validate path\n",
        "if not os.path.exists(csv_path):\n",
        "    raise FileNotFoundError(\n",
        "        f\"CSV not found: {csv_path}\",\n",
        "    )\n",
        "\n",
        "print(f\"Loaded change components from: {csv_path}\")\n",
        "\n",
        "# Load into DataFrame\n",
        "df = pd.read_csv(\n",
        "    csv_path,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kfuldlq814uj"
      },
      "source": [
        "#### 6.2.1 Change Components by Time Interval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1X3_t7utKggJ"
      },
      "outputs": [],
      "source": [
        "# Filter only time intervals\n",
        "time_df = df[\n",
        "    df[\"Time_Interval\"].str.contains(\"-\")\n",
        "]\n",
        "\n",
        "# Aggregate totals per interval and component (Gain)\n",
        "totals = (\n",
        "    time_df.groupby(\n",
        "        [\"Time_Interval\", \"Component\"],\n",
        "    )[\"Gain\"]\n",
        "    .sum()\n",
        "    .unstack()\n",
        ")\n",
        "\n",
        "# Choose scale automatically based on max value\n",
        "max_val = totals[[\"Quantity\", \"Shift\", \"Exchange\"]].to_numpy().max()\n",
        "\n",
        "if max_val >= 1_000_000_000_000:\n",
        "    scale_factor = 1_000_000_000_000\n",
        "    y_label = \"Change (trillion pixels)\"\n",
        "elif max_val >= 1_000_000_000:\n",
        "    scale_factor = 1_000_000_000\n",
        "    y_label = \"Change (billion pixels)\"\n",
        "elif max_val >= 1_000_000:\n",
        "    scale_factor = 1_000_000\n",
        "    y_label = \"Change (million pixels)\"\n",
        "elif max_val >= 1_000:\n",
        "    scale_factor = 1_000\n",
        "    y_label = \"Change (thousand pixels)\"\n",
        "elif max_val >= 100:\n",
        "    scale_factor = 100\n",
        "    y_label = \"Change (hundred pixels)\"\n",
        "else:\n",
        "    scale_factor = 1\n",
        "    y_label = \"Change (pixels)\"\n",
        "\n",
        "# Scaled totals per component for plotting\n",
        "scaled_totals = totals[\n",
        "    [\n",
        "        \"Quantity\",\n",
        "        \"Shift\",\n",
        "        \"Exchange\",\n",
        "    ]\n",
        "] / scale_factor\n",
        "\n",
        "# Maximum stacked height (Quantity + Exchange + Shift)\n",
        "stacked_max = scaled_totals.sum(\n",
        "    axis=1,\n",
        ").max()\n",
        "\n",
        "# Create figure and axis\n",
        "fig, ax = plt.subplots(\n",
        "    figsize=(\n",
        "        14,\n",
        "        6,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Colors\n",
        "colors = [\n",
        "    \"#1f77b4\",  # Quantity\n",
        "    \"#2ca02c\",  # Shift\n",
        "    \"#ffd700\",  # Exchange\n",
        "\n",
        "]\n",
        "components_color = {\n",
        "    \"Quantity\": \"#1f77b4\",\n",
        "    \"Shift\": \"#2ca02c\",\n",
        "    \"Exchange\": \"#ffd700\"\n",
        "\n",
        "}\n",
        "\n",
        "# Stacked bars using scaled values\n",
        "for idx, comp in enumerate(\n",
        "    [\n",
        "        \"Quantity\",\n",
        "        \"Shift\",\n",
        "        \"Exchange\",\n",
        "    ],\n",
        "):\n",
        "    bottom_values = (\n",
        "        scaled_totals.iloc[:, :idx].sum(\n",
        "            axis=1,\n",
        "        )\n",
        "        if idx > 0\n",
        "        else 0\n",
        "    )\n",
        "    ax.bar(\n",
        "        totals.index,\n",
        "        scaled_totals[comp],\n",
        "        label=comp,\n",
        "        color=colors[idx],\n",
        "        edgecolor=\"none\",\n",
        "        bottom=bottom_values,\n",
        "    )\n",
        "\n",
        "# Axes formatting\n",
        "ax.set_ylabel(\n",
        "    y_label,\n",
        "    fontsize=18,\n",
        ")\n",
        "ax.set_title(\n",
        "    \"Change Components by Time Interval\",\n",
        "    fontsize=20,\n",
        ")\n",
        "ax.tick_params(\n",
        "    axis=\"both\",\n",
        "    which=\"major\",\n",
        "    labelsize=18,\n",
        ")\n",
        "\n",
        "# Adaptive rotation for x-axis tick labels\n",
        "labels = ax.get_xticklabels()\n",
        "n_labels = len(labels)\n",
        "\n",
        "if n_labels <= 6:\n",
        "    rotation = 0\n",
        "    ha = \"center\"\n",
        "elif n_labels <= 12:\n",
        "    rotation = 45\n",
        "    ha = \"right\"\n",
        "else:\n",
        "    rotation = 90\n",
        "    ha = \"center\"\n",
        "\n",
        "plt.setp(\n",
        "    labels,\n",
        "    rotation=rotation,\n",
        "    ha=ha,\n",
        ")\n",
        "\n",
        "# Y-axis limits and ticks based on stacked maximum\n",
        "y_max_scaled = stacked_max * 1.1\n",
        "ax.set_ylim(\n",
        "    0,\n",
        "    y_max_scaled,\n",
        ")\n",
        "ax.yaxis.set_major_locator(\n",
        "    ticker.MaxNLocator(\n",
        "        nbins=5,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Legend (custom labels)\n",
        "legend_elements = [\n",
        "    plt.Rectangle(\n",
        "        (0, 0),\n",
        "        1,\n",
        "        1,\n",
        "        color=components_color[\"Exchange\"],\n",
        "        label=\"Allocation Exchange\",\n",
        "    ),\n",
        "    plt.Rectangle(\n",
        "        (0, 0),\n",
        "        1,\n",
        "        1,\n",
        "        color=components_color[\"Shift\"],\n",
        "        label=\"Allocation Shift\",\n",
        "    ),\n",
        "    plt.Rectangle(\n",
        "        (0, 0),\n",
        "        1,\n",
        "        1,\n",
        "        color=components_color[\"Quantity\"],\n",
        "        label=\"Quantity\",\n",
        "    ),\n",
        "]\n",
        "ax.legend(\n",
        "    handles=legend_elements,\n",
        "    loc=\"center left\",\n",
        "    bbox_to_anchor=(\n",
        "        1.01,\n",
        "        0.5,\n",
        "    ),\n",
        "    fontsize=16,\n",
        "    frameon=False,\n",
        ")\n",
        "\n",
        "# Save and show\n",
        "plt.tight_layout()\n",
        "plt.savefig(\n",
        "    os.path.join(\n",
        "        output_path,\n",
        "        \"graphic_change_components_time_interval.png\",\n",
        "    ),\n",
        "    bbox_inches=\"tight\",\n",
        "    format=\"png\",\n",
        "    dpi=300,\n",
        ")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNEyoJeHXKRS"
      },
      "source": [
        "#### 6.2.2 Change Components Overall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNsdWULSKggJ"
      },
      "outputs": [],
      "source": [
        "def plot_components_with_alternation(\n",
        "    csv_path: str,\n",
        "    output_path: str,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Plot overall change components as a single stacked bar with auto-scaled y-axis for Real Data.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    csv_path : str\n",
        "        Path to the CSV file containing change components.\n",
        "    output_path : str\n",
        "        Directory where the output figure will be saved.\n",
        "    \"\"\"\n",
        "    # 1. Load data\n",
        "    df = pd.read_csv(\n",
        "        csv_path,\n",
        "    )\n",
        "\n",
        "    # 2. Define Colors and component order\n",
        "    components_color = {\n",
        "        \"Quantity\": \"#1f77b4\",\n",
        "        \"Allocation_Exchange\": \"#ffd700\",\n",
        "        \"Alternation_Exchange\": \"#ff8080\",\n",
        "        \"Allocation_Shift\": \"#2ca02c\",\n",
        "        \"Alternation_Shift\": \"#990099\",\n",
        "    }\n",
        "\n",
        "    component_order = [\n",
        "        \"Quantity\",\n",
        "        \"Allocation_Shift\",\n",
        "        \"Allocation_Exchange\",\n",
        "        \"Alternation_Shift\",\n",
        "        \"Alternation_Exchange\",\n",
        "    ]\n",
        "\n",
        "    # 3. Aggregate totals per component\n",
        "    component_totals = {\n",
        "        \"Quantity\": df[\n",
        "            (df[\"Component\"] == \"Allocation_Quantity\")\n",
        "            & (df[\"Time_Interval\"] == \"extent\")\n",
        "        ][\"Gain\"].sum(),\n",
        "        \"Allocation_Exchange\": df[\n",
        "            (df[\"Component\"] == \"Allocation_Exchange\")\n",
        "            & (df[\"Time_Interval\"] == \"extent\")\n",
        "        ][\"Gain\"].sum(),\n",
        "        \"Allocation_Shift\": df[\n",
        "            (df[\"Component\"] == \"Allocation_Shift\")\n",
        "            & (df[\"Time_Interval\"] == \"extent\")\n",
        "        ][\"Gain\"].sum(),\n",
        "        \"Alternation_Exchange\": df[\n",
        "            (df[\"Time_Interval\"] == \"alternation_exchange\")\n",
        "        ][\"Gain\"].sum(),\n",
        "        \"Alternation_Shift\": df[\n",
        "            (df[\"Time_Interval\"] == \"alternation_shift\")\n",
        "        ][\"Gain\"].sum(),\n",
        "    }\n",
        "\n",
        "    # 4. Automatic scale based on the sum of all stacked components\n",
        "    total_change = sum(\n",
        "        component_totals.values(),\n",
        "    )\n",
        "\n",
        "    if max_val >= 1_000_000_000_000:\n",
        "        scale_factor = 1_000_000_000_000\n",
        "        y_label = \"Change (trillion pixels)\"\n",
        "    elif max_val >= 1_000_000_000:\n",
        "        scale_factor = 1_000_000_000\n",
        "        y_label = \"Change (billion pixels)\"\n",
        "    elif max_val >= 1_000_000:\n",
        "        scale_factor = 1_000_000\n",
        "        y_label = \"Change (million pixels)\"\n",
        "    elif max_val >= 1_000:\n",
        "        scale_factor = 1_000\n",
        "        y_label = \"Change (thousand pixels)\"\n",
        "    elif max_val >= 100:\n",
        "        scale_factor = 100\n",
        "        y_label = \"Change (hundred pixels)\"\n",
        "    else:\n",
        "        scale_factor = 1\n",
        "        y_label = \"Change (pixels)\"\n",
        "\n",
        "    # 5. Initialize figure and axis\n",
        "    fig, ax = plt.subplots(\n",
        "        figsize=(\n",
        "            10,\n",
        "            6,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # 6. Plot each component in a stacked bar at a single x-position\n",
        "    bottom = 0.0\n",
        "    for component in component_order:\n",
        "        value = (\n",
        "            component_totals.get(\n",
        "                component,\n",
        "                0.0,\n",
        "            )\n",
        "            / scale_factor\n",
        "        )\n",
        "        ax.bar(\n",
        "            x=0,\n",
        "            height=value,\n",
        "            bottom=bottom,\n",
        "            color=components_color[component],\n",
        "            edgecolor=\"none\",\n",
        "            width=0.8,\n",
        "        )\n",
        "        bottom += value\n",
        "\n",
        "    # 7. Axes formatting and labels\n",
        "    ax.set_ylabel(\n",
        "        y_label,\n",
        "        fontsize=16,\n",
        "    )\n",
        "    ax.set_title(\n",
        "        \"Change Components Overall\",\n",
        "        fontsize=18,\n",
        "    )\n",
        "    ax.xaxis.set_visible(\n",
        "        False,\n",
        "    )\n",
        "    ax.tick_params(\n",
        "        axis=\"both\",\n",
        "        which=\"major\",\n",
        "        labelsize=16,\n",
        "    )\n",
        "\n",
        "    # 8. Set y-axis limits and major tick locators\n",
        "    y_max_scaled = bottom * 1.1 if bottom > 0 else 1.0\n",
        "    ax.set_ylim(\n",
        "        0,\n",
        "        y_max_scaled,\n",
        "    )\n",
        "\n",
        "    ax.yaxis.set_major_locator(\n",
        "        mticker.MaxNLocator(\n",
        "            nbins=5,\n",
        "            integer=True,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    ax.yaxis.set_major_formatter(\n",
        "        mticker.FormatStrFormatter(\n",
        "            \"%d\",\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # 9. Configure visible spines for the plot frame\n",
        "    for spine in [\n",
        "        \"top\",\n",
        "        \"right\",\n",
        "        \"left\",\n",
        "        \"bottom\",\n",
        "    ]:\n",
        "        ax.spines[spine].set_visible(\n",
        "            True,\n",
        "        )\n",
        "\n",
        "    # 10. Define custom legend elements\n",
        "    legend_elements = [\n",
        "        plt.Rectangle(\n",
        "            (0, 0),\n",
        "            1,\n",
        "            1,\n",
        "            color=components_color[\"Alternation_Exchange\"],\n",
        "            label=\"Alternation Exchange\",\n",
        "        ),\n",
        "        plt.Rectangle(\n",
        "            (0, 0),\n",
        "            1,\n",
        "            1,\n",
        "            color=components_color[\"Alternation_Shift\"],\n",
        "            label=\"Alternation Shift\",\n",
        "        ),\n",
        "        plt.Rectangle(\n",
        "            (0, 0),\n",
        "            1,\n",
        "            1,\n",
        "            color=components_color[\"Allocation_Exchange\"],\n",
        "            label=\"Allocation Exchange\",\n",
        "        ),\n",
        "        plt.Rectangle(\n",
        "            (0, 0),\n",
        "            1,\n",
        "            1,\n",
        "            color=components_color[\"Allocation_Shift\"],\n",
        "            label=\"Allocation Shift\",\n",
        "        ),\n",
        "        plt.Rectangle(\n",
        "            (0, 0),\n",
        "            1,\n",
        "            1,\n",
        "            color=components_color[\"Quantity\"],\n",
        "            label=\"Quantity\",\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "    ax.legend(\n",
        "        handles=legend_elements,\n",
        "        loc=\"center left\",\n",
        "        bbox_to_anchor=(\n",
        "            1.05,\n",
        "            0.5,\n",
        "        ),\n",
        "        fontsize=14,\n",
        "        frameon=False,\n",
        "    )\n",
        "\n",
        "    # 11. Final layout adjustment and export\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\n",
        "        os.path.join(\n",
        "            output_path,\n",
        "            \"graphic_change_components_overall.png\",\n",
        "        ),\n",
        "        bbox_inches=\"tight\",\n",
        "        format=\"png\",\n",
        "        dpi=300,\n",
        "    )\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# 12. Specify paths and execute the plotting function\n",
        "csv_components_path = os.path.join(\n",
        "    output_path,\n",
        "    \"change_components.csv\",\n",
        ")\n",
        "\n",
        "plot_components_with_alternation(\n",
        "    csv_path=csv_components_path,\n",
        "    output_path=output_path,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "La030pE327Uu"
      },
      "source": [
        "#### 6.2.3 Change Componentes by Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rH_XrWoKggK"
      },
      "outputs": [],
      "source": [
        "class ComponentVisualizer:\n",
        "    \"\"\"\n",
        "    Visualize change components by class.\n",
        "\n",
        "    Handles negative correction terms in alternation components correctly.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_gain_loss_stacked(\n",
        "        df: pd.DataFrame,\n",
        "        class_labels_dict: dict,\n",
        "        title: str,\n",
        "        output_path: str,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Plot per-class gains and losses as stacked bars with auto-scaled y-axis.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        df : pd.DataFrame\n",
        "            DataFrame containing the change components per class.\n",
        "        class_labels_dict : dict\n",
        "            Dictionary mapping class IDs to names/metadata.\n",
        "        title : str\n",
        "            Title of the plot.\n",
        "        output_path : str\n",
        "            Directory to save the output figure.\n",
        "        \"\"\"\n",
        "        # 1. Filter and Prepare Data\n",
        "        target_intervals = [\n",
        "            \"extent\",\n",
        "            \"alternation_shift\",\n",
        "            \"alternation_exchange\",\n",
        "        ]\n",
        "\n",
        "        # Select rows\n",
        "        df_sub = df[\n",
        "            df[\"Time_Interval\"].isin(\n",
        "                target_intervals,\n",
        "            )\n",
        "        ].copy()\n",
        "\n",
        "        # Rename 'Allocation_Quantity' to 'Quantity' for consistency\n",
        "        df_sub[\"Component\"] = df_sub[\"Component\"].str.replace(\n",
        "            \"Allocation_Quantity\",\n",
        "            \"Quantity\",\n",
        "        )\n",
        "\n",
        "        # 2. Colors and Order\n",
        "        components_map = {\n",
        "            \"Quantity\": \"#1f77b4\",\n",
        "            \"Allocation_Exchange\": \"#ffd700\",\n",
        "            \"Alternation_Exchange\": \"#ff8080\",\n",
        "            \"Allocation_Shift\": \"#2ca02c\",\n",
        "            \"Alternation_Shift\": \"#990099\",\n",
        "        }\n",
        "\n",
        "        # We process these groups separately to handle the math\n",
        "        comp_groups = [\n",
        "            \"Quantity\",\n",
        "            \"Allocation_Shift\",\n",
        "            \"Allocation_Exchange\",\n",
        "        ]\n",
        "\n",
        "        # 3. Identify Classes and Sort by Net Quantity Change\n",
        "        classes = sorted(\n",
        "            df_sub[\"Class\"].unique(),\n",
        "        )\n",
        "\n",
        "        # Calculate Net Quantity for Sorting\n",
        "        class_stats = []\n",
        "        for cls in classes:\n",
        "            c_data = df_sub[\n",
        "                df_sub[\"Class\"] == cls\n",
        "            ]\n",
        "            qty_gain = c_data[\n",
        "                c_data[\"Component\"] == \"Quantity\"\n",
        "            ][\"Gain\"].sum()\n",
        "\n",
        "            qty_loss = c_data[\n",
        "                c_data[\"Component\"] == \"Quantity\"\n",
        "            ][\"Loss\"].sum()\n",
        "\n",
        "            class_stats.append(\n",
        "                (\n",
        "                    cls,\n",
        "                    qty_gain - qty_loss,\n",
        "                ),\n",
        "            )\n",
        "\n",
        "        # Sort classes\n",
        "        ordered_classes = [\n",
        "            x[0]\n",
        "            for x in sorted(\n",
        "                class_stats,\n",
        "                key=lambda x: x[1],\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        # 4. Prepare Plot Data (Handling the Net Logic)\n",
        "        plot_data = []\n",
        "        max_val = 0.0\n",
        "\n",
        "        for cls in ordered_classes:\n",
        "            c_data = df_sub[\n",
        "                df_sub[\"Class\"] == cls\n",
        "            ]\n",
        "\n",
        "            # --- GAINS ---\n",
        "            # Standard Components\n",
        "            gains = {}\n",
        "            for comp in comp_groups:\n",
        "                gains[comp] = c_data[\n",
        "                    c_data[\"Component\"] == comp\n",
        "                ][\"Gain\"].sum()\n",
        "\n",
        "            # Alternation Logic (Consolidate Exchange + Shift)\n",
        "            raw_alt_exc = c_data[\n",
        "                c_data[\"Component\"] == \"Alternation_Exchange\"\n",
        "            ][\"Gain\"].sum()\n",
        "\n",
        "            raw_alt_shift = c_data[\n",
        "                c_data[\"Component\"] == \"Alternation_Shift\"\n",
        "            ][\"Gain\"].sum()\n",
        "\n",
        "            net_alt = raw_alt_exc + raw_alt_shift\n",
        "\n",
        "            # If Net Alternation is effectively 0, plot neither.\n",
        "            # If positive, attribute to Exchange first if valid, else Shift.\n",
        "            adj_alt_exc = 0.0\n",
        "            adj_alt_shift = 0.0\n",
        "\n",
        "            if net_alt > 0.0001:\n",
        "                # If raw exchange is positive, keep it\n",
        "                if raw_alt_exc > 0:\n",
        "                    adj_alt_exc = raw_alt_exc\n",
        "                    adj_alt_shift = max(\n",
        "                        0,\n",
        "                        net_alt - raw_alt_exc,\n",
        "                    )\n",
        "                else:\n",
        "                    # If raw exchange is negative, hide it inside Shift\n",
        "                    adj_alt_exc = 0\n",
        "                    adj_alt_shift = net_alt\n",
        "\n",
        "            gains[\"Alternation_Exchange\"] = adj_alt_exc\n",
        "            gains[\"Alternation_Shift\"] = adj_alt_shift\n",
        "\n",
        "            # --- LOSSES (Logic is identical but using Loss column) ---\n",
        "            losses = {}\n",
        "            for comp in comp_groups:\n",
        "                losses[comp] = c_data[\n",
        "                    c_data[\"Component\"] == comp\n",
        "                ][\"Loss\"].sum()\n",
        "\n",
        "            raw_alt_exc_l = c_data[\n",
        "                c_data[\"Component\"] == \"Alternation_Exchange\"\n",
        "            ][\"Loss\"].sum()\n",
        "\n",
        "            raw_alt_shift_l = c_data[\n",
        "                c_data[\"Component\"] == \"Alternation_Shift\"\n",
        "            ][\"Loss\"].sum()\n",
        "\n",
        "            net_alt_l = raw_alt_exc_l + raw_alt_shift_l\n",
        "\n",
        "            adj_alt_exc_l = 0.0\n",
        "            adj_alt_shift_l = 0.0\n",
        "\n",
        "            if net_alt_l > 0.0001:\n",
        "                if raw_alt_exc_l > 0:\n",
        "                    adj_alt_exc_l = raw_alt_exc_l\n",
        "                    adj_alt_shift_l = max(\n",
        "                        0,\n",
        "                        net_alt_l - raw_alt_exc_l,\n",
        "                    )\n",
        "                else:\n",
        "                    adj_alt_exc_l = 0\n",
        "                    adj_alt_shift_l = net_alt_l\n",
        "\n",
        "            losses[\"Alternation_Exchange\"] = adj_alt_exc_l\n",
        "            losses[\"Alternation_Shift\"] = adj_alt_shift_l\n",
        "\n",
        "            # Track max height for scaling\n",
        "            total_g = sum(\n",
        "                gains.values(),\n",
        "            )\n",
        "            total_l = sum(\n",
        "                losses.values(),\n",
        "            )\n",
        "            max_val = max(\n",
        "                max_val,\n",
        "                total_g,\n",
        "                total_l,\n",
        "            )\n",
        "\n",
        "            plot_data.append(\n",
        "                {\n",
        "                    \"class\": cls,\n",
        "                    \"gains\": gains,\n",
        "                    \"losses\": losses,\n",
        "                },\n",
        "            )\n",
        "\n",
        "        # 5. Determine Scale Factor\n",
        "        if max_val >= 1_000_000_000_000:\n",
        "            scale_factor = 1_000_000_000_000\n",
        "            y_label = \"Change (trillion pixels)\"\n",
        "        elif max_val >= 1_000_000_000:\n",
        "            scale_factor = 1_000_000_000\n",
        "            y_label = \"Change (billion pixels)\"\n",
        "        elif max_val >= 1_000_000:\n",
        "            scale_factor = 1_000_000\n",
        "            y_label = \"Change (million pixels)\"\n",
        "        elif max_val >= 1_000:\n",
        "            scale_factor = 1_000\n",
        "            y_label = \"Change (thousand pixels)\"\n",
        "        elif max_val >= 100:\n",
        "            scale_factor = 100\n",
        "            y_label = \"Change (hundred pixels)\"\n",
        "        else:\n",
        "            scale_factor = 1\n",
        "            y_label = \"Change (pixels)\"\n",
        "\n",
        "        # 6. Plotting\n",
        "        fig, ax = plt.subplots(\n",
        "            figsize=(\n",
        "                14,\n",
        "                8,\n",
        "            ),\n",
        "        )\n",
        "        fig.subplots_adjust(\n",
        "            left=0.1,\n",
        "            right=0.75,\n",
        "        )\n",
        "\n",
        "        x_pos = np.arange(\n",
        "            len(ordered_classes),\n",
        "        )\n",
        "        width = 0.6\n",
        "\n",
        "        # Order for stacking: Quantity at bottom, then Alloc, then Alt\n",
        "        stack_order = [\n",
        "            \"Quantity\",\n",
        "            \"Allocation_Shift\",\n",
        "            \"Allocation_Exchange\",\n",
        "            \"Alternation_Shift\",\n",
        "            \"Alternation_Exchange\",\n",
        "        ]\n",
        "\n",
        "        for idx, item in enumerate(\n",
        "            plot_data,\n",
        "        ):\n",
        "            # Gains (Upwards)\n",
        "            bottom_g = 0.0\n",
        "            for comp in stack_order:\n",
        "                val = item[\"gains\"][comp] / scale_factor\n",
        "                if val > 0:\n",
        "                    ax.bar(\n",
        "                        x_pos[idx],\n",
        "                        val,\n",
        "                        width,\n",
        "                        bottom=bottom_g,\n",
        "                        color=components_map[\n",
        "                            comp\n",
        "                        ],\n",
        "                        edgecolor=\"none\",\n",
        "                    )\n",
        "                    bottom_g += val\n",
        "\n",
        "            # Losses (Downwards)\n",
        "            bottom_l = 0.0\n",
        "            for comp in stack_order:\n",
        "                val = item[\"losses\"][comp] / scale_factor\n",
        "                if val > 0:\n",
        "                    ax.bar(\n",
        "                        x_pos[idx],\n",
        "                        -val,\n",
        "                        width,\n",
        "                        bottom=bottom_l,\n",
        "                        color=components_map[\n",
        "                            comp\n",
        "                        ],\n",
        "                        edgecolor=\"none\",\n",
        "                    )\n",
        "                    bottom_l -= val\n",
        "\n",
        "        # 7. Formatting\n",
        "        class_names = [\n",
        "            class_labels_dict.get(\n",
        "                int(c) if str(c).isdigit() else c,\n",
        "                {},\n",
        "            ).get(\n",
        "                \"name\",\n",
        "                str(c),\n",
        "            )\n",
        "            for c in ordered_classes\n",
        "        ]\n",
        "\n",
        "        ax.set_xticks(\n",
        "            x_pos,\n",
        "        )\n",
        "        ax.set_xticklabels(\n",
        "            class_names,\n",
        "            rotation=0,\n",
        "            ha=\"center\",\n",
        "            fontsize=14,\n",
        "        )\n",
        "        ax.axhline(\n",
        "            0,\n",
        "            color=\"black\",\n",
        "            linewidth=0.8,\n",
        "        )\n",
        "\n",
        "        ax.set_ylabel(\n",
        "            y_label,\n",
        "            fontsize=16,\n",
        "        )\n",
        "        ax.set_title(\n",
        "            title,\n",
        "            fontsize=18,\n",
        "        )\n",
        "        ax.tick_params(\n",
        "            axis=\"both\",\n",
        "            labelsize=14,\n",
        "        )\n",
        "\n",
        "        # Y-Axis Limits and Integer Ticks\n",
        "        limit = max_val / scale_factor * 1.1 if max_val > 0 else 1.0\n",
        "        ax.set_ylim(\n",
        "            -limit,\n",
        "            limit,\n",
        "        )\n",
        "        ax.yaxis.set_major_locator(\n",
        "            ticker.MaxNLocator(\n",
        "                integer=True,\n",
        "                nbins=6,\n",
        "            ),\n",
        "        )\n",
        "        ax.yaxis.set_major_formatter(\n",
        "            ticker.FormatStrFormatter(\n",
        "                \"%d\",\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        # Legend\n",
        "        handles = [\n",
        "            plt.Rectangle(\n",
        "                (0, 0),\n",
        "                1,\n",
        "                1,\n",
        "                color=components_map[\n",
        "                    c\n",
        "                ],\n",
        "                label=c.replace(\n",
        "                    \"_\",\n",
        "                    \" \",\n",
        "                ),\n",
        "            )\n",
        "            for c in reversed(\n",
        "                stack_order,\n",
        "            )  # Top to bottom order in legend\n",
        "        ]\n",
        "\n",
        "        ax.legend(\n",
        "            handles=handles,\n",
        "            loc=\"center left\",\n",
        "            bbox_to_anchor=(\n",
        "                1.02,\n",
        "                0.5,\n",
        "            ),\n",
        "            fontsize=14,\n",
        "            frameon=False,\n",
        "        )\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(\n",
        "            os.path.join(\n",
        "                output_path,\n",
        "                \"graphic_change_component_change_class.png\",\n",
        "            ),\n",
        "            bbox_inches=\"tight\",\n",
        "            format=\"png\",\n",
        "            dpi=300\n",
        "        )\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# Execution\n",
        "ComponentVisualizer.plot_gain_loss_stacked(\n",
        "    df=df,\n",
        "    class_labels_dict=class_labels_dict,\n",
        "    title=\"Change Components by Class\",\n",
        "    output_path=output_path,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrRpt9v9CuzG"
      },
      "source": [
        "## **7. Number of Changes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEewbaTnDKvl"
      },
      "source": [
        "### 7.1 Compute Number of Changes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CbqTb6LKggK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import rasterio\n",
        "from rasterio.windows import Window\n",
        "from tqdm import tqdm\n",
        "\n",
        "def number_of_changes_raster_optimized(\n",
        "    file_list: list,\n",
        "    output_dir: str,\n",
        "    nodata_val: int,\n",
        "    block_size: int = 1024,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Compute and save a raster representing the total number of class changes\n",
        "    per pixel, using optimized block processing (files kept open).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    file_list : list of str\n",
        "        List of paths to the input raster images (sorted temporally).\n",
        "    output_dir : str\n",
        "        Directory to save the output raster.\n",
        "    nodata_val : int\n",
        "        Value representing NoData pixels to be ignored in comparisons.\n",
        "    block_size : int, optional\n",
        "        Size of the square blocks to process at once (default is 1024).\n",
        "    \"\"\"\n",
        "    image_paths = sorted(\n",
        "        file_list,\n",
        "    )\n",
        "\n",
        "    out_raster_path = os.path.join(\n",
        "        output_dir,\n",
        "        \"number_of_changes.tif\",\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"Computing number of changes across {len(image_paths)} raster maps...\",\n",
        "    )\n",
        "\n",
        "    # 1. Open all datasets ONCE to avoid I/O overhead during the loop\n",
        "    # This is much faster than opening/closing inside the block loop\n",
        "    srcs = [\n",
        "        rasterio.open(\n",
        "            path,\n",
        "        ) for path in image_paths\n",
        "    ]\n",
        "\n",
        "    # 2. Extract metadata from the first source\n",
        "    height = srcs[0].height\n",
        "    width = srcs[0].width\n",
        "    meta = srcs[0].profile.copy()\n",
        "\n",
        "    # 3. Update profile for the output raster (optimized for chunks)\n",
        "    meta.update(\n",
        "        {\n",
        "            \"driver\": \"GTiff\",\n",
        "            \"count\": 1,\n",
        "            \"nodata\": nodata_val,\n",
        "            \"compress\": \"lzw\",\n",
        "            \"tiled\": True,\n",
        "            \"blockxsize\": 256,\n",
        "            \"blockysize\": 256,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # 4. Open output file and iterate by spatial blocks\n",
        "    with rasterio.open(\n",
        "        out_raster_path,\n",
        "        \"w\",\n",
        "        **meta,\n",
        "    ) as dst:\n",
        "\n",
        "        rows = range(\n",
        "            0,\n",
        "            height,\n",
        "            block_size,\n",
        "        )\n",
        "        cols = range(\n",
        "            0,\n",
        "            width,\n",
        "            block_size,\n",
        "        )\n",
        "        total_blocks = len(rows) * len(cols)\n",
        "\n",
        "        with tqdm(\n",
        "            total=total_blocks,\n",
        "            desc=\"Processing blocks\",\n",
        "            unit=\"block\",\n",
        "        ) as pbar:\n",
        "\n",
        "            for r_off in rows:\n",
        "                for c_off in cols:\n",
        "                    window = Window(\n",
        "                        c_off,\n",
        "                        r_off,\n",
        "                        min(\n",
        "                            block_size,\n",
        "                            width - c_off,\n",
        "                        ),\n",
        "                        min(\n",
        "                            block_size,\n",
        "                            height - r_off,\n",
        "                        ),\n",
        "                    )\n",
        "\n",
        "                    # 5. Read the block from the first year\n",
        "                    data_prev = srcs[0].read(\n",
        "                        1,\n",
        "                        window=window,\n",
        "                    )\n",
        "\n",
        "                    cumulative_nodata_mask = (data_prev == nodata_val)\n",
        "\n",
        "                    total_changes = np.zeros(\n",
        "                        data_prev.shape,\n",
        "                        dtype=data_prev.dtype,\n",
        "                    )\n",
        "\n",
        "                    # 6. Iterate through the already opened datasets\n",
        "                    for i in range(\n",
        "                        1,\n",
        "                        len(srcs),\n",
        "                    ):\n",
        "                        data_curr = srcs[i].read(\n",
        "                            1,\n",
        "                            window=window,\n",
        "                        )\n",
        "\n",
        "                        cumulative_nodata_mask |= (data_curr == nodata_val)\n",
        "\n",
        "                        change_mask = (\n",
        "                            (data_prev != data_curr)\n",
        "                            & (data_prev != nodata_val)\n",
        "                            & (data_curr != nodata_val)\n",
        "                        )\n",
        "\n",
        "                        total_changes += change_mask.astype(\n",
        "                            data_prev.dtype,\n",
        "                        )\n",
        "\n",
        "                        data_prev = data_curr\n",
        "\n",
        "                    total_changes[cumulative_nodata_mask] = nodata_val\n",
        "\n",
        "                    # 7. Write the computed block to disk\n",
        "                    dst.write(\n",
        "                        total_changes,\n",
        "                        1,\n",
        "                        window=window,\n",
        "                    )\n",
        "\n",
        "                    pbar.update(1)\n",
        "\n",
        "    # 8. Close all datasets to free up file descriptors\n",
        "    for src in srcs:\n",
        "        src.close()\n",
        "\n",
        "    print(\n",
        "        f\"Number of changes raster saved to: {out_raster_path}\",\n",
        "    )\n",
        "\n",
        "# --- Execution ---\n",
        "\n",
        "number_of_changes_raster_optimized(\n",
        "    file_list=image_paths,\n",
        "    output_dir=output_path,\n",
        "    nodata_val=noData_value,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-q3XCT-DTzw"
      },
      "source": [
        "### 7.2 Plot Number of Changes Map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cK81VQpCmvb6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mticker\n",
        "import rasterio\n",
        "from rasterio.enums import Resampling\n",
        "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
        "from matplotlib.patches import Patch\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "from matplotlib_scalebar.scalebar import ScaleBar\n",
        "from pyproj import Transformer\n",
        "\n",
        "def plot_number_of_changes_map(\n",
        "    output_dir: str,\n",
        "    nodata_val: int,\n",
        "    raster_filename: str = \"number_of_changes.tif\",\n",
        "    max_display_size: int = 1500, # Added dynamic downsampling parameter\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Plot the Number of Changes raster map with cartographic elements.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    output_dir : str\n",
        "        Directory containing the raster file and where the map will be saved.\n",
        "    nodata_val : int\n",
        "        Value representing NoData in the raster to be masked out.\n",
        "    raster_filename : str, optional\n",
        "        Name of the raster file to plot (default is \"number_of_changes.tif\").\n",
        "    max_display_size : int, optional\n",
        "        Maximum width/height in pixels for the generated plot to prevent\n",
        "        RAM overflow. Default is 1500.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "        This function does not return a value; it saves a PNG file to disk\n",
        "        and displays the plot.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    FileNotFoundError\n",
        "        If the specified raster file does not exist in the output directory.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1) Input raster path\n",
        "    raster_to_plot_path = os.path.join(\n",
        "        output_dir,\n",
        "        raster_filename,\n",
        "    )\n",
        "\n",
        "    if not os.path.exists(raster_to_plot_path):\n",
        "        raise FileNotFoundError(\n",
        "            f\"Raster not found: {raster_to_plot_path}\",\n",
        "        )\n",
        "\n",
        "    # Calculate dynamic scale factor based on max display size\n",
        "    with rasterio.open(raster_to_plot_path) as src:\n",
        "        max_dim = max(src.width, src.height)\n",
        "        scale_factor = max(1.0, max_dim / max_display_size)\n",
        "\n",
        "        left, bottom, right, top = src.bounds\n",
        "        src_crs = src.crs\n",
        "        transform = src.transform\n",
        "\n",
        "    # Get the exact pixel size in km for the scalebar\n",
        "    pixel_size_km = compute_display_pixel_size_km(\n",
        "        raster_path=raster_to_plot_path,\n",
        "        downsample_divisor=scale_factor,\n",
        "    )\n",
        "\n",
        "    # 2) Read raster and basic metadata\n",
        "    with rasterio.open(\n",
        "        raster_to_plot_path,\n",
        "    ) as src:\n",
        "\n",
        "        # Calculate new shape safely\n",
        "        new_height = max(1, int(src.height / scale_factor))\n",
        "        new_width = max(1, int(src.width / scale_factor))\n",
        "\n",
        "        data = src.read(\n",
        "            1,\n",
        "            out_shape=(\n",
        "                new_height,\n",
        "                new_width,\n",
        "            ),\n",
        "            resampling=Resampling.nearest,\n",
        "        )\n",
        "\n",
        "        # Force masking using the provided variable\n",
        "        data = np.ma.masked_equal(\n",
        "            data,\n",
        "            nodata_val,\n",
        "        )\n",
        "\n",
        "    # 3) Figure\n",
        "    fig, ax = plt.subplots(\n",
        "        figsize=(\n",
        "            6,\n",
        "            8,\n",
        "        ),\n",
        "        dpi=300,\n",
        "    )\n",
        "\n",
        "    # Data range\n",
        "    min_val = int(\n",
        "        np.ma.min(\n",
        "            data,\n",
        "        ),\n",
        "    )\n",
        "    max_val = int(\n",
        "        np.ma.max(\n",
        "            data,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # 4) Colormap (gray for 0 + jet for 1..max)\n",
        "    original_cmap = plt.get_cmap(\n",
        "        \"viridis\",\n",
        "    )\n",
        "    color_list = [\n",
        "        \"#c0c0c0\",\n",
        "    ] + [\n",
        "        original_cmap(i)\n",
        "        for i in np.linspace(\n",
        "            0,\n",
        "            1,\n",
        "            max_val,\n",
        "        )\n",
        "    ]\n",
        "    cmap = ListedColormap(\n",
        "        color_list,\n",
        "    )\n",
        "\n",
        "    # Discrete normalization\n",
        "    bounds = np.arange(\n",
        "        min_val,\n",
        "        max_val + 2,\n",
        "    ) - 0.5\n",
        "    norm = BoundaryNorm(\n",
        "        bounds,\n",
        "        cmap.N,\n",
        "    )\n",
        "\n",
        "    # 5) Plot raster in downsampled pixel coordinates\n",
        "    ax.imshow(\n",
        "        data,\n",
        "        cmap=cmap,\n",
        "        interpolation=\"nearest\",\n",
        "        norm=norm,\n",
        "    )\n",
        "\n",
        "    # 6) Discrete box legend\n",
        "    legend_elements = []\n",
        "    for i in range(\n",
        "        min_val,\n",
        "        max_val + 1,\n",
        "    ):\n",
        "        legend_elements.append(\n",
        "            Patch(\n",
        "                facecolor=cmap(\n",
        "                    norm(i),\n",
        "                ),\n",
        "                edgecolor=\"black\",\n",
        "                linewidth=0.5,\n",
        "                label=str(i),\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    ax.legend(\n",
        "        handles=legend_elements,\n",
        "        title=\"Number of Changes\",\n",
        "        loc=\"center left\",\n",
        "        bbox_to_anchor=(\n",
        "            1.02,\n",
        "            0.5,\n",
        "        ),\n",
        "        frameon=False,\n",
        "        fontsize=8,\n",
        "        title_fontsize=10,\n",
        "        alignment=\"left\",\n",
        "    )\n",
        "\n",
        "    # 7) Cartographic elements\n",
        "    scalebar = ScaleBar(\n",
        "        dx=pixel_size_km,\n",
        "        units=\"km\",\n",
        "        length_fraction=0.35,\n",
        "        location=\"lower right\",\n",
        "        box_alpha=0.0,\n",
        "        scale_formatter=lambda value, _: f\"{int(value)} km\",\n",
        "    )\n",
        "    ax.add_artist(\n",
        "        scalebar,\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        north_arrow(\n",
        "            ax,\n",
        "            location=\"upper left\",\n",
        "            shadow=False,\n",
        "            rotation={\n",
        "                \"degrees\": 0,\n",
        "            },\n",
        "        )\n",
        "    except NameError:\n",
        "        print(\"Note: north_arrow function not found, skipping.\")\n",
        "\n",
        "    # 8) Axes styling (with Lat/Lon labels)\n",
        "    ax.set_title(\n",
        "        \"Number of Changes\",\n",
        "        fontsize=18,\n",
        "        pad=5,\n",
        "    )\n",
        "    ax.set_aspect(\n",
        "        \"equal\",\n",
        "    )\n",
        "\n",
        "    # Initialize Transformer (Native CRS -> Lat/Lon)\n",
        "    to_latlon = Transformer.from_crs(\n",
        "        src_crs,\n",
        "        \"EPSG:4326\",\n",
        "        always_xy=True,\n",
        "    )\n",
        "\n",
        "    # Get dimensions from the loaded DOWNSAMPLED data\n",
        "    height, width = data.shape\n",
        "\n",
        "    def format_lon(\n",
        "        x,\n",
        "        pos,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Convert a pixel column index to a formatted Longitude string.\n",
        "        \"\"\"\n",
        "        x = np.clip(\n",
        "            x,\n",
        "            0,\n",
        "            width - 1,\n",
        "        )\n",
        "        # Multiply by scale_factor to get the original unscaled pixel position\n",
        "        original_x = x * scale_factor\n",
        "        original_y = (height // 2) * scale_factor\n",
        "\n",
        "        x_proj, y_proj = rasterio.transform.xy(\n",
        "            transform,\n",
        "            original_y,\n",
        "            original_x,\n",
        "        )\n",
        "        lon, lat = to_latlon.transform(\n",
        "            x_proj,\n",
        "            y_proj,\n",
        "        )\n",
        "        return f\"{lon:.1f}°\"\n",
        "\n",
        "    def format_lat(\n",
        "        y,\n",
        "        pos,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Convert a pixel row index to a formatted Latitude string.\n",
        "        \"\"\"\n",
        "        y = np.clip(\n",
        "            y,\n",
        "            0,\n",
        "            height - 1,\n",
        "        )\n",
        "        # Multiply by scale_factor to get the original unscaled pixel position\n",
        "        original_y = y * scale_factor\n",
        "        original_x = (width // 2) * scale_factor\n",
        "\n",
        "        x_proj, y_proj = rasterio.transform.xy(\n",
        "            transform,\n",
        "            original_y,\n",
        "            original_x,\n",
        "        )\n",
        "        lon, lat = to_latlon.transform(\n",
        "            x_proj,\n",
        "            y_proj,\n",
        "        )\n",
        "        return f\"{lat:.1f}°\"\n",
        "\n",
        "    # Apply the custom formatters\n",
        "    ax.xaxis.set_major_formatter(\n",
        "        FuncFormatter(\n",
        "            format_lon,\n",
        "        ),\n",
        "    )\n",
        "    ax.yaxis.set_major_formatter(\n",
        "        FuncFormatter(\n",
        "            format_lat,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Limit ticks to avoid overcrowding\n",
        "    ax.xaxis.set_major_locator(\n",
        "        mticker.MaxNLocator(\n",
        "            nbins=4,\n",
        "        ),\n",
        "    )\n",
        "    ax.yaxis.set_major_locator(\n",
        "        mticker.MaxNLocator(\n",
        "            nbins=6,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Final styling for ticks\n",
        "    ax.tick_params(\n",
        "        axis=\"both\",\n",
        "        which=\"major\",\n",
        "        labelsize=7,\n",
        "        pad=4,\n",
        "    )\n",
        "    plt.setp(\n",
        "        ax.get_yticklabels(),\n",
        "        rotation=90,\n",
        "        va=\"center\",\n",
        "    )\n",
        "\n",
        "    # 9) Save\n",
        "    output_figure_path = os.path.join(\n",
        "        output_dir,\n",
        "        \"map_number_of_changes.png\",\n",
        "    )\n",
        "    plt.savefig(\n",
        "        output_figure_path,\n",
        "        dpi=300,\n",
        "        bbox_inches=\"tight\",\n",
        "        format=\"png\",\n",
        "        pad_inches=0.5,\n",
        "    )\n",
        "    plt.show()\n",
        "\n",
        "    print(\n",
        "        f\"Map figure saved successfully to: {output_figure_path}\",\n",
        "    )\n",
        "\n",
        "\n",
        "# Execute the function\n",
        "plot_number_of_changes_map(\n",
        "    output_dir=output_path,\n",
        "    nodata_val=noData_value\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4Zkd4JjWM2_"
      },
      "source": [
        "## **8. Trajectory Classification**\n",
        "\n",
        "\n",
        "---\n",
        "This section provides a framework for processing and classifying pixel trajectories in raster datasets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-GHrkW4cB6G"
      },
      "source": [
        "### 8.1 Compute Trajectory Analysis\n",
        "This section processes and visualizes raster data by scaling, applying a color map, and adding graphical elements like legends, scale bars, and north arrows. The output is a high-resolution image of the classified raster data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wug4TKS8KggL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import rasterio\n",
        "from rasterio.windows import Window\n",
        "from tqdm import tqdm\n",
        "import numba as nb\n",
        "from numba import prange\n",
        "\n",
        "# --- Numba Optimized Functions ---\n",
        "\n",
        "@nb.njit(\n",
        "    nogil=True,\n",
        ")\n",
        "def classify_pixel(\n",
        "    pixel_series: np.ndarray,\n",
        "    nodata_val: int,\n",
        "):\n",
        "    \"\"\"\n",
        "    Classify a single pixel trajectory into four categories based on temporal changes.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    pixel_series : np.ndarray\n",
        "        A 1D numpy array representing the land cover class values\n",
        "        of a single pixel over time.\n",
        "    nodata_val : int\n",
        "        Value representing NoData pixels.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    int\n",
        "        An integer representing the trajectory class:\n",
        "        nodata_val: NoData (Background)\n",
        "        1: Stable (No change)\n",
        "        2: Alternation (Change occurs, but Start Class == End Class)\n",
        "        3: Step (Start != End, and a direct transition exists)\n",
        "        4: Complex (Start != End, and no direct transition exists)\n",
        "    \"\"\"\n",
        "    if pixel_series[0] == nodata_val:\n",
        "        return nodata_val\n",
        "\n",
        "    start = pixel_series[\n",
        "        0\n",
        "    ]\n",
        "    end = pixel_series[\n",
        "        -1\n",
        "    ]\n",
        "\n",
        "    has_variation = False\n",
        "    direct_transition = False\n",
        "\n",
        "    for i in range(\n",
        "        len(\n",
        "            pixel_series,\n",
        "        )\n",
        "        - 1,\n",
        "    ):\n",
        "        current = pixel_series[\n",
        "            i\n",
        "        ]\n",
        "        next_val = pixel_series[\n",
        "            i + 1\n",
        "        ]\n",
        "\n",
        "        if current == nodata_val or next_val == nodata_val:\n",
        "            return nodata_val\n",
        "\n",
        "        if current != next_val:\n",
        "            has_variation = True\n",
        "            if (current == start) and (next_val == end):\n",
        "                direct_transition = True\n",
        "\n",
        "    if not has_variation:\n",
        "        return 1\n",
        "\n",
        "    if start == end:\n",
        "        return 2\n",
        "\n",
        "    if direct_transition:\n",
        "        return 3\n",
        "\n",
        "    return 4\n",
        "\n",
        "\n",
        "@nb.njit(\n",
        "    nogil=True,\n",
        "    parallel=True,\n",
        ")\n",
        "def process_stack_block_parallel(\n",
        "    stack: np.ndarray,\n",
        "    nodata_val: int,\n",
        "):\n",
        "    \"\"\"\n",
        "    Apply pixel classification to a 3D raster block using parallel processing.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    stack : np.ndarray\n",
        "        A 3D numpy array of shape (height, width, time) containing\n",
        "        the stacked raster data.\n",
        "    nodata_val : int\n",
        "        Value representing NoData pixels.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        A 2D numpy array of shape (height, width) containing the\n",
        "        classified trajectory codes, inheriting the input's data type.\n",
        "    \"\"\"\n",
        "    height = stack.shape[0]\n",
        "    width = stack.shape[1]\n",
        "\n",
        "    # Initialize output array with the exact input dtype to preserve custom nodata\n",
        "    result = np.full(\n",
        "        (\n",
        "            height,\n",
        "            width,\n",
        "        ),\n",
        "        nodata_val,\n",
        "        dtype=stack.dtype,\n",
        "    )\n",
        "\n",
        "    for y in prange(\n",
        "        height,\n",
        "    ):\n",
        "        for x in range(\n",
        "            width,\n",
        "        ):\n",
        "            # The pixel series is extracted across the time axis (last dimension)\n",
        "            result[\n",
        "                y,\n",
        "                x,\n",
        "            ] = classify_pixel(\n",
        "                stack[\n",
        "                    y,\n",
        "                    x,\n",
        "                    :,\n",
        "                ],\n",
        "                nodata_val,\n",
        "            )\n",
        "\n",
        "    return result\n",
        "\n",
        "# --- Main Processing Function ---\n",
        "\n",
        "def generate_trajectory_raster_optimized(\n",
        "    file_list: list,\n",
        "    output_dir: str,\n",
        "    nodata_val: int,\n",
        "    block_size: int = 1024,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Load time-series rasters from directory, classify trajectories, and save the result\n",
        "    using memory-efficient chunking and parallel Numba processing.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    file_list : list of str\n",
        "        The list containing paths to input rasters.\n",
        "    output_dir : str\n",
        "        The directory path where the output file will be saved.\n",
        "    nodata_val : int\n",
        "        Value representing NoData pixels.\n",
        "    block_size : int, optional\n",
        "        Size of the square blocks to process at once (default is 1024).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        The full file path to the generated trajectory raster.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError\n",
        "        If no images are found in the file list.\n",
        "    \"\"\"\n",
        "    if not file_list:\n",
        "        raise ValueError(\n",
        "            \"The provided file list is empty.\",\n",
        "        )\n",
        "\n",
        "    image_paths = sorted(\n",
        "        file_list,\n",
        "    )\n",
        "\n",
        "    out_file = os.path.join(\n",
        "        output_dir,\n",
        "        \"trajectory.tif\",\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"Classifying trajectories across {len(image_paths)} raster maps...\",\n",
        "    )\n",
        "\n",
        "    # 1. Open all datasets ONCE to avoid I/O overhead during the loop\n",
        "    srcs = [\n",
        "        rasterio.open(\n",
        "            path,\n",
        "        ) for path in image_paths\n",
        "    ]\n",
        "\n",
        "    # 2. Extract metadata from the first source\n",
        "    height = srcs[0].height\n",
        "    width = srcs[0].width\n",
        "    meta = srcs[0].profile.copy()\n",
        "\n",
        "    # 3. Update profile for the output raster\n",
        "    # Notice we keep the original dtype so nodata_val fits perfectly\n",
        "    meta.update(\n",
        "        {\n",
        "            \"driver\": \"GTiff\",\n",
        "            \"count\": 1,\n",
        "            \"nodata\": nodata_val,\n",
        "            \"compress\": \"lzw\",\n",
        "            \"tiled\": True,\n",
        "            \"blockxsize\": 256,\n",
        "            \"blockysize\": 256,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # 4. Open output file and iterate by spatial blocks\n",
        "    with rasterio.open(\n",
        "        out_file,\n",
        "        \"w\",\n",
        "        **meta,\n",
        "    ) as dst:\n",
        "\n",
        "        rows = range(\n",
        "            0,\n",
        "            height,\n",
        "            block_size,\n",
        "        )\n",
        "        cols = range(\n",
        "            0,\n",
        "            width,\n",
        "            block_size,\n",
        "        )\n",
        "        total_blocks = len(rows) * len(cols)\n",
        "\n",
        "        with tqdm(\n",
        "            total=total_blocks,\n",
        "            desc=\"Processing trajectory blocks\",\n",
        "            unit=\"block\",\n",
        "        ) as pbar:\n",
        "\n",
        "            for r_off in rows:\n",
        "                for c_off in cols:\n",
        "                    window = Window(\n",
        "                        c_off,\n",
        "                        r_off,\n",
        "                        min(\n",
        "                            block_size,\n",
        "                            width - c_off,\n",
        "                        ),\n",
        "                        min(\n",
        "                            block_size,\n",
        "                            height - r_off,\n",
        "                        ),\n",
        "                    )\n",
        "\n",
        "                    # 5. Load stack for this block from all opened datasets\n",
        "                    stack_list = []\n",
        "                    for src in srcs:\n",
        "                        data = src.read(\n",
        "                            1,\n",
        "                            window=window,\n",
        "                        )\n",
        "                        stack_list.append(\n",
        "                            data,\n",
        "                        )\n",
        "\n",
        "                    # 6. Convert to 3D array and restructure for speed\n",
        "                    # New shape: (Height, Width, Time) -> best for pixel-level loops\n",
        "                    stack_data = np.array(\n",
        "                        stack_list,\n",
        "                    )\n",
        "                    stack_data = np.moveaxis(\n",
        "                        stack_data,\n",
        "                        0,\n",
        "                        -1,\n",
        "                    )\n",
        "\n",
        "                    # 7. Apply Numba-accelerated classification on the block\n",
        "                    trajectory_block = process_stack_block_parallel(\n",
        "                        stack_data,\n",
        "                        nodata_val,\n",
        "                    )\n",
        "\n",
        "                    # 8. Write the processed block to disk\n",
        "                    dst.write(\n",
        "                        trajectory_block,\n",
        "                        1,\n",
        "                        window=window,\n",
        "                    )\n",
        "\n",
        "                    pbar.update(1)\n",
        "\n",
        "    # 9. Close all open datasets\n",
        "    for src in srcs:\n",
        "        src.close()\n",
        "\n",
        "    print(\n",
        "        f\"Trajectory map saved successfully to: {out_file}\",\n",
        "    )\n",
        "\n",
        "    return out_file\n",
        "\n",
        "# --- Execution ---\n",
        "\n",
        "trajectory_file = generate_trajectory_raster_optimized(\n",
        "    file_list=image_paths,\n",
        "    output_dir=output_path,\n",
        "    nodata_val=noData_value,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LR4eo6CBcZSQ"
      },
      "source": [
        "### 8.2 Plot Trajectory Analysis Graphic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmeBa_DdKggL"
      },
      "outputs": [],
      "source": [
        "def plot_trajectory_distribution(\n",
        "    output_path: str,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Generate and save a stacked bar chart of trajectory class distributions.\n",
        "\n",
        "    This function reads a 'trajectory.tif' raster from the specified directory,\n",
        "    calculates the percentage of the area covered by trajectory classes 2, 3,\n",
        "    and 4, and plots these as a stacked bar chart. The resulting figure is\n",
        "    saved as a JPEG file.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    output_path : str\n",
        "        Directory path containing the input 'trajectory.tif' raster.\n",
        "        The output image 'graphic_trajectory_distribution.jpeg' will also\n",
        "        be saved here.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "        The function saves the plot to disk and displays it.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    FileNotFoundError\n",
        "        If 'trajectory.tif' is not found in the provided ``output_path``.\n",
        "    \"\"\"\n",
        "\n",
        "    raster_path = os.path.join(\n",
        "        output_path,\n",
        "        \"trajectory.tif\",\n",
        "    )\n",
        "\n",
        "    with rasterio.open(\n",
        "        raster_path,\n",
        "    ) as src:\n",
        "        traj_data = src.read(\n",
        "            1,\n",
        "        )\n",
        "        nodata = src.nodata\n",
        "\n",
        "    masked_traj = np.ma.masked_where(\n",
        "        traj_data == nodata,\n",
        "        traj_data,\n",
        "    )\n",
        "\n",
        "    unique_vals, counts = np.unique(\n",
        "        masked_traj.compressed(),\n",
        "        return_counts=True,\n",
        "    )\n",
        "\n",
        "    data_counts = dict(\n",
        "        zip(\n",
        "            unique_vals,\n",
        "            counts,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    total_pixels = sum(\n",
        "        data_counts.values(),\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"Unique values in raster: {unique_vals}\",\n",
        "    )\n",
        "\n",
        "    # Convert to native python float to avoid 'np.float64' in print\n",
        "    percentages = {\n",
        "        i: float(\n",
        "            (\n",
        "                data_counts.get(\n",
        "                    i,\n",
        "                    0,\n",
        "                )\n",
        "                / total_pixels\n",
        "            )\n",
        "            * 100.0\n",
        "        )\n",
        "        for i in [\n",
        "            2,\n",
        "            3,\n",
        "            4,\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    formatted_msg = \", \".join(f\"{k}: {v:.0f}%\" for k, v in percentages.items())\n",
        "\n",
        "    print(\n",
        "        f\"Computed Percentages: {formatted_msg}\",\n",
        "    )\n",
        "\n",
        "    ordered_trajs = [\n",
        "        4,\n",
        "        3,\n",
        "        2,\n",
        "    ]\n",
        "    colors = {\n",
        "        4: \"#000066\",  # Dark Blue\n",
        "        3: \"#cccc00\",  # Yellow\n",
        "        2: \"#990033\",  # Dark Red\n",
        "    }\n",
        "\n",
        "    fig, ax = plt.subplots(\n",
        "        figsize=(\n",
        "            8,\n",
        "            6,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    bottom = 0.0\n",
        "    for traj in ordered_trajs:\n",
        "        val = percentages[\n",
        "            traj\n",
        "        ]\n",
        "        ax.bar(\n",
        "            0,\n",
        "            val,\n",
        "            bottom=bottom,\n",
        "            color=colors[\n",
        "                traj\n",
        "            ],\n",
        "            width=0.4,\n",
        "            edgecolor=\"none\",\n",
        "        )\n",
        "        bottom += val\n",
        "\n",
        "    ax.set_ylabel(\n",
        "        \"Trajectory Area (% of the region)\",\n",
        "        fontsize=16,\n",
        "    )\n",
        "\n",
        "    for spine in [\n",
        "        \"top\",\n",
        "        \"right\",\n",
        "        \"bottom\",\n",
        "        \"left\",\n",
        "    ]:\n",
        "        ax.spines[\n",
        "            spine\n",
        "        ].set_visible(\n",
        "            True,\n",
        "        )\n",
        "        ax.spines[\n",
        "            spine\n",
        "        ].set_color(\n",
        "            \"black\",\n",
        "        )\n",
        "        ax.spines[\n",
        "            spine\n",
        "        ].set_linewidth(\n",
        "            0.5,\n",
        "        )\n",
        "\n",
        "    ax.tick_params(\n",
        "        axis=\"y\",\n",
        "        which=\"major\",\n",
        "        labelsize=18,\n",
        "    )\n",
        "\n",
        "    # Remove x-axis ticks to leave it blank\n",
        "    ax.set_xticks(\n",
        "        [],\n",
        "    )\n",
        "\n",
        "    ax.set_ylim(\n",
        "        0,\n",
        "        bottom * 1.05,\n",
        "    )\n",
        "\n",
        "    # Use mticker for consistency\n",
        "    ax.yaxis.set_major_locator(\n",
        "        mticker.MaxNLocator(\n",
        "            nbins=5,\n",
        "        ),\n",
        "    )\n",
        "    ax.yaxis.set_major_formatter(\n",
        "        mticker.FormatStrFormatter(\n",
        "            \"%d\",\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Legend with Original Names\n",
        "    legend_elements = [\n",
        "        Patch(\n",
        "            facecolor=colors[\n",
        "                2\n",
        "            ],\n",
        "            label=\"Trajectory 2\",\n",
        "        ),\n",
        "        Patch(\n",
        "            facecolor=colors[\n",
        "                3\n",
        "            ],\n",
        "            label=\"Trajectory 3\",\n",
        "        ),\n",
        "        Patch(\n",
        "            facecolor=colors[\n",
        "                4\n",
        "            ],\n",
        "            label=\"Trajectory 4\",\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "    ax.legend(\n",
        "        handles=legend_elements,\n",
        "        loc=\"center left\",\n",
        "        bbox_to_anchor=(\n",
        "            1.05,\n",
        "            0.5,\n",
        "        ),\n",
        "        fontsize=14,\n",
        "        frameon=False,\n",
        "    )\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the figure\n",
        "    out_fig_path = os.path.join(\n",
        "        output_path,\n",
        "        \"graphic_trajectory_distribution.png\",\n",
        "    )\n",
        "    plt.savefig(\n",
        "        out_fig_path,\n",
        "        dpi=300,\n",
        "        bbox_inches=\"tight\",\n",
        "        format=\"png\",\n",
        "    )\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Execution\n",
        "plot_trajectory_distribution(\n",
        "    output_path=output_path,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4dFyLOeclkG"
      },
      "source": [
        "### 8.3 Plot Trajectory Analysis Map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxtPHs4PKggL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mticker\n",
        "import rasterio\n",
        "from rasterio.enums import Resampling\n",
        "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
        "from matplotlib.patches import Patch\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "from matplotlib_scalebar.scalebar import ScaleBar\n",
        "from pyproj import Transformer\n",
        "\n",
        "def plot_trajectory_map(\n",
        "    output_dir: str,\n",
        "    raster_filename: str = \"trajectory.tif\",\n",
        "    max_display_size: int = 1500, # Adicionado o parâmetro de segurança de RAM\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Plot the Trajectory raster map with specific categorical coloring and full descriptions.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    output_dir : str\n",
        "        Directory containing the raster file and where the map will be saved.\n",
        "    raster_filename : str, optional\n",
        "        Name of the raster file to plot (default is \"trajectory.tif\").\n",
        "    max_display_size : int, optional\n",
        "        Maximum width/height in pixels for the generated plot to prevent\n",
        "        RAM overflow. Default is 1500.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "        This function does not return a value; it saves a PNG file to disk\n",
        "        and displays the plot.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    FileNotFoundError\n",
        "        If the specified raster file does not exist in the output directory.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1) Input raster path\n",
        "    raster_path = os.path.join(\n",
        "        output_dir,\n",
        "        raster_filename,\n",
        "    )\n",
        "\n",
        "    if not os.path.exists(raster_path):\n",
        "        raise FileNotFoundError(\n",
        "            f\"Raster not found: {raster_path}\",\n",
        "        )\n",
        "\n",
        "    # 2) Calculate dynamic scale factor\n",
        "    with rasterio.open(raster_path) as src:\n",
        "        max_dim = max(src.width, src.height)\n",
        "        scale_factor = max(1.0, max_dim / max_display_size)\n",
        "\n",
        "        left, bottom, right, top = src.bounds\n",
        "        src_crs = src.crs\n",
        "        transform = src.transform\n",
        "\n",
        "    # Get the exact pixel size in km for the scalebar using the scale_factor\n",
        "    pixel_size_km = compute_display_pixel_size_km(\n",
        "        raster_path=raster_path,\n",
        "        downsample_divisor=scale_factor,\n",
        "    )\n",
        "\n",
        "    # 3) Read raster and apply downsampling\n",
        "    with rasterio.open(\n",
        "        raster_path,\n",
        "    ) as src:\n",
        "\n",
        "        new_height = max(1, int(src.height / scale_factor))\n",
        "        new_width = max(1, int(src.width / scale_factor))\n",
        "\n",
        "        data = src.read(\n",
        "            1,\n",
        "            out_shape=(\n",
        "                new_height,\n",
        "                new_width,\n",
        "            ),\n",
        "            resampling=Resampling.nearest,\n",
        "        )\n",
        "\n",
        "        nodata_val = src.nodata\n",
        "        if nodata_val is not None:\n",
        "             data = np.ma.masked_equal(data, nodata_val)\n",
        "\n",
        "    # 4) Figure\n",
        "    fig, ax = plt.subplots(\n",
        "        figsize=(\n",
        "            6,\n",
        "            8,\n",
        "        ),\n",
        "        dpi=300,\n",
        "    )\n",
        "\n",
        "    # 5) Colormap (categorical: 1–4)\n",
        "    cmap = ListedColormap(\n",
        "        [\n",
        "            \"#d9d9d9\",  # 1\n",
        "            \"#990033\",  # 2\n",
        "            \"#cccc00\",  # 3\n",
        "            \"#000066\",  # 4\n",
        "        ],\n",
        "    )\n",
        "    bounds = [0.5, 1.5, 2.5, 3.5, 4.5]\n",
        "    norm = BoundaryNorm(bounds, cmap.N)\n",
        "\n",
        "    # 6) Plot raster in downsampled pixel coordinates (No 'extent' used here)\n",
        "    ax.imshow(\n",
        "        data,\n",
        "        cmap=cmap,\n",
        "        norm=norm,\n",
        "        interpolation=\"nearest\"\n",
        "    )\n",
        "\n",
        "    # 7) Legend\n",
        "    legend_elements = [\n",
        "        Patch(\n",
        "            facecolor=\"#d9d9d9\",\n",
        "            label=(\n",
        "                \"1:Start class matches end class while\\n\"\n",
        "                \"alternation = 0\"\n",
        "            ),\n",
        "            edgecolor=\"black\",\n",
        "            linewidth=0.5,\n",
        "        ),\n",
        "        Patch(\n",
        "            facecolor=\"#990033\",\n",
        "            label=(\n",
        "                \"2:Start class matches end class while\\n\"\n",
        "                \"alternation > 0\"\n",
        "            ),\n",
        "            edgecolor=\"black\",\n",
        "            linewidth=0.5,\n",
        "        ),\n",
        "        Patch(\n",
        "            facecolor=\"#cccc00\",\n",
        "            label=(\n",
        "                \"3:Start class differs from end class while\\n\"\n",
        "                \"at least one time interval transitions from\\n\"\n",
        "                \"start class to end class\"\n",
        "            ),\n",
        "            edgecolor=\"black\",\n",
        "            linewidth=0.5,\n",
        "        ),\n",
        "        Patch(\n",
        "            facecolor=\"#000066\",\n",
        "            label=(\n",
        "                \"4:Start class differs from end class while\\n\"\n",
        "                \"no time interval transitions from start\\n\"\n",
        "                \"class to end class\"\n",
        "            ),\n",
        "            edgecolor=\"black\",\n",
        "            linewidth=0.5,\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "    ax.legend(\n",
        "        handles=legend_elements,\n",
        "        loc=\"center left\",\n",
        "        bbox_to_anchor=(\n",
        "            1.02,\n",
        "            0.5,\n",
        "        ),\n",
        "        frameon=False,\n",
        "        fontsize=8,\n",
        "        borderpad=1.2,\n",
        "        handletextpad=0.8,\n",
        "        columnspacing=2,\n",
        "    )\n",
        "\n",
        "    # 8) Cartographic elements\n",
        "    scalebar = ScaleBar(\n",
        "        dx=pixel_size_km,\n",
        "        units=\"km\",\n",
        "        length_fraction=0.35,\n",
        "        location=\"lower right\",\n",
        "        box_alpha=0.0,\n",
        "        scale_formatter=lambda value, _: f\"{int(value)} km\",\n",
        "    )\n",
        "    ax.add_artist(\n",
        "        scalebar,\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        north_arrow(\n",
        "            ax,\n",
        "            location=\"upper left\",\n",
        "            shadow=False,\n",
        "            rotation={\n",
        "                \"degrees\": 0,\n",
        "            },\n",
        "        )\n",
        "    except NameError:\n",
        "        print(\"Note: north_arrow function not found, skipping north arrow.\")\n",
        "\n",
        "    # 9) Axes styling (with Lat/Lon labels)\n",
        "    ax.set_title(\n",
        "        \"Trajectory\",\n",
        "        fontsize=18,\n",
        "        pad=5,\n",
        "    )\n",
        "    ax.set_aspect(\n",
        "        \"equal\",\n",
        "    )\n",
        "\n",
        "    # Initialize Transformer (Native CRS -> Lat/Lon)\n",
        "    to_latlon = Transformer.from_crs(\n",
        "        src_crs,\n",
        "        \"EPSG:4326\",\n",
        "        always_xy=True,\n",
        "    )\n",
        "\n",
        "    # Get dimensions from the loaded DOWNSAMPLED data\n",
        "    height, width = data.shape\n",
        "\n",
        "    def format_lon(\n",
        "        x,\n",
        "        pos,\n",
        "    ):\n",
        "        \"\"\"Convert a pixel column index to a formatted Longitude string.\"\"\"\n",
        "        x = np.clip(\n",
        "            x,\n",
        "            0,\n",
        "            width - 1,\n",
        "        )\n",
        "        # Multiply by scale_factor to map back to original pixels\n",
        "        original_x = x * scale_factor\n",
        "        original_y = (height // 2) * scale_factor\n",
        "\n",
        "        x_proj, y_proj = rasterio.transform.xy(\n",
        "            transform,\n",
        "            original_y,\n",
        "            original_x,\n",
        "        )\n",
        "        lon, lat = to_latlon.transform(\n",
        "            x_proj,\n",
        "            y_proj,\n",
        "        )\n",
        "        return f\"{lon:.1f}°\"\n",
        "\n",
        "    def format_lat(\n",
        "        y,\n",
        "        pos,\n",
        "    ):\n",
        "        \"\"\"Convert a pixel row index to a formatted Latitude string.\"\"\"\n",
        "        y = np.clip(\n",
        "            y,\n",
        "            0,\n",
        "            height - 1,\n",
        "        )\n",
        "        # Multiply by scale_factor to map back to original pixels\n",
        "        original_y = y * scale_factor\n",
        "        original_x = (width // 2) * scale_factor\n",
        "\n",
        "        x_proj, y_proj = rasterio.transform.xy(\n",
        "            transform,\n",
        "            original_y,\n",
        "            original_x,\n",
        "        )\n",
        "        lon, lat = to_latlon.transform(\n",
        "            x_proj,\n",
        "            y_proj,\n",
        "        )\n",
        "        return f\"{lat:.1f}°\"\n",
        "\n",
        "    # Apply the custom formatters\n",
        "    ax.xaxis.set_major_formatter(\n",
        "        FuncFormatter(\n",
        "            format_lon,\n",
        "        ),\n",
        "    )\n",
        "    ax.yaxis.set_major_formatter(\n",
        "        FuncFormatter(\n",
        "            format_lat,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Limit ticks to avoid overcrowding\n",
        "    ax.xaxis.set_major_locator(\n",
        "        mticker.MaxNLocator(\n",
        "            nbins=4,\n",
        "        ),\n",
        "    )\n",
        "    ax.yaxis.set_major_locator(\n",
        "        mticker.MaxNLocator(\n",
        "            nbins=6,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Final styling for ticks\n",
        "    ax.tick_params(\n",
        "        axis=\"both\",\n",
        "        which=\"major\",\n",
        "        labelsize=7,\n",
        "        pad=4,\n",
        "    )\n",
        "    plt.setp(\n",
        "        ax.get_yticklabels(),\n",
        "        rotation=90,\n",
        "        va=\"center\",\n",
        "    )\n",
        "\n",
        "    # 10) Save\n",
        "    output_figure_path = os.path.join(\n",
        "        output_dir,\n",
        "        \"map_trajectories.png\",\n",
        "    )\n",
        "    plt.savefig(\n",
        "        output_figure_path,\n",
        "        format=\"png\",\n",
        "        dpi=300,\n",
        "        bbox_inches=\"tight\",\n",
        "        pad_inches=0.5,\n",
        "    )\n",
        "    plt.show()\n",
        "\n",
        "    print(\n",
        "        f\"Trajectory map saved successfully to: {output_figure_path}\",\n",
        "    )\n",
        "\n",
        "\n",
        "# Execute the function\n",
        "plot_trajectory_map(\n",
        "    output_dir=output_path,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsO3cMQBKggL"
      },
      "source": [
        "### **8.4 Trajectory contributions per Time Interval**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85PZyHu6j_Ib"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import rasterio\n",
        "from rasterio.windows import Window\n",
        "import numba as nb\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# 1. Numba Optimized Function\n",
        "@nb.njit(\n",
        "    nogil=True,\n",
        "    fastmath=True,\n",
        ")\n",
        "def compute_trajectory_changes_numba(\n",
        "    trajectory_window: np.ndarray,\n",
        "    stack_window: np.ndarray,\n",
        "    nodata: int,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Count pixels per trajectory type changing in each time interval using Numba.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    trajectory_window : np.ndarray\n",
        "        2D array (block_height, block_width) with trajectory IDs.\n",
        "    stack_window : np.ndarray\n",
        "        3D array (time, block_height, block_width) with pixel values.\n",
        "    nodata : int\n",
        "        Value to ignore.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        2D array of shape (n_intervals, 5) containing change counts.\n",
        "    \"\"\"\n",
        "    n_times = stack_window.shape[0]\n",
        "    height = stack_window.shape[1]\n",
        "    width = stack_window.shape[2]\n",
        "    n_intervals = n_times - 1\n",
        "\n",
        "    # 1.1. Initialize counts array: rows=intervals, cols=trajectory_ids(0-4)\n",
        "    counts = np.zeros(\n",
        "        (\n",
        "            n_intervals,\n",
        "            5,\n",
        "        ),\n",
        "        dtype=np.int64,\n",
        "    )\n",
        "\n",
        "    # 1.2. Iterate over pixels in the current window\n",
        "    for y in range(\n",
        "        height,\n",
        "    ):\n",
        "        for x in range(\n",
        "            width,\n",
        "        ):\n",
        "            traj_id = trajectory_window[\n",
        "                y,\n",
        "                x,\n",
        "            ]\n",
        "\n",
        "            # 1.3. Filter: We only care about Alternation(2), Step(3), Complex(4)\n",
        "            if traj_id < 2 or traj_id > 4:\n",
        "                continue\n",
        "\n",
        "            # 1.4. Check changes across all time intervals\n",
        "            for t in range(\n",
        "                n_intervals,\n",
        "            ):\n",
        "                val_from = stack_window[\n",
        "                    t,\n",
        "                    y,\n",
        "                    x,\n",
        "                ]\n",
        "                val_to = stack_window[\n",
        "                    t + 1,\n",
        "                    y,\n",
        "                    x,\n",
        "                ]\n",
        "\n",
        "                # 1.5. Check validity and detect change\n",
        "                if (\n",
        "                    val_from != nodata\n",
        "                    and val_to != nodata\n",
        "                    and val_from != val_to\n",
        "                ):\n",
        "                    counts[\n",
        "                        t,\n",
        "                        traj_id,\n",
        "                    ] += 1\n",
        "\n",
        "    return counts\n",
        "\n",
        "\n",
        "# 2. Main Processing Function with Windowed Reading\n",
        "def analyze_trajectory_intervals_windowed(\n",
        "    image_paths: list[str],\n",
        "    output_path: str,\n",
        "    years: list[int],\n",
        "    nodata_val: int,\n",
        "    block_size: int = 2048,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load data in windows, compute trajectory contributions, and save CSV.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    image_paths : list[str]\n",
        "        List of paths to input rasters.\n",
        "    output_path : str\n",
        "        Base directory containing the trajectory raster.\n",
        "    years : list[int]\n",
        "        List of years corresponding to the images.\n",
        "    nodata_val : int\n",
        "        NoData value to ignore.\n",
        "    block_size : int, optional\n",
        "        Size of the window to process at a time. Default is 2048.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        DataFrame containing the counts per interval and trajectory type.\n",
        "    \"\"\"\n",
        "    # 2.1. Define path to the trajectory map\n",
        "    traj_path = os.path.join(\n",
        "        output_path,\n",
        "        \"trajectory.tif\",\n",
        "    )\n",
        "\n",
        "    if not os.path.exists(\n",
        "        traj_path,\n",
        "    ):\n",
        "        raise FileNotFoundError(\n",
        "            f\"Trajectory raster not found at: {traj_path}\",\n",
        "        )\n",
        "\n",
        "    print(\n",
        "        \"Initializing windowed processing...\",\n",
        "    )\n",
        "\n",
        "    # 2.2. Setup Windows and Accumulators\n",
        "    with rasterio.open(\n",
        "        traj_path,\n",
        "    ) as src_traj:\n",
        "        height = src_traj.height\n",
        "        width = src_traj.width\n",
        "\n",
        "        windows = []\n",
        "        for row_off in range(\n",
        "            0,\n",
        "            height,\n",
        "            block_size,\n",
        "        ):\n",
        "            h = min(\n",
        "                block_size,\n",
        "                height - row_off,\n",
        "            )\n",
        "            for col_off in range(\n",
        "                0,\n",
        "                width,\n",
        "                block_size,\n",
        "            ):\n",
        "                w = min(\n",
        "                    block_size,\n",
        "                    width - col_off,\n",
        "                )\n",
        "                windows.append(\n",
        "                    Window(\n",
        "                        col_off,\n",
        "                        row_off,\n",
        "                        w,\n",
        "                        h,\n",
        "                    ),\n",
        "                )\n",
        "\n",
        "    # 2.3. Initialize global counter array\n",
        "    n_intervals = len(image_paths) - 1\n",
        "    total_counts = np.zeros(\n",
        "        (\n",
        "            n_intervals,\n",
        "            5,\n",
        "        ),\n",
        "        dtype=np.int64,\n",
        "    )\n",
        "\n",
        "    # 2.4. Determine dtype from the first image\n",
        "    with rasterio.open(\n",
        "        image_paths[0],\n",
        "    ) as tmp:\n",
        "        dtype_img = tmp.profile[\n",
        "            \"dtype\"\n",
        "        ]\n",
        "\n",
        "    # 2.5. Iterate over Windows\n",
        "    print(\n",
        "        f\"Processing {len(windows)} blocks...\",\n",
        "    )\n",
        "    for window in tqdm(\n",
        "        windows,\n",
        "        desc=\"Processing Blocks\",\n",
        "    ):\n",
        "        # 2.6. Read Trajectory Map for current window\n",
        "        with rasterio.open(\n",
        "            traj_path,\n",
        "        ) as src_traj:\n",
        "            traj_window = src_traj.read(\n",
        "                1,\n",
        "                window=window,\n",
        "            )\n",
        "\n",
        "        # 2.7. Skip window if no relevant trajectories are found\n",
        "        if not np.any(\n",
        "            (traj_window >= 2) & (traj_window <= 4)\n",
        "        ):\n",
        "            continue\n",
        "\n",
        "        # 2.8. Pre-allocate stack chunk (Time, H, W)\n",
        "        stack_chunk = np.zeros(\n",
        "            (\n",
        "                len(image_paths),\n",
        "                int(window.height),\n",
        "                int(window.width),\n",
        "            ),\n",
        "            dtype=dtype_img,\n",
        "        )\n",
        "\n",
        "        # 2.9. Fill the stack chunk by reading each image\n",
        "        for t, img_p in enumerate(\n",
        "            image_paths,\n",
        "        ):\n",
        "            with rasterio.open(\n",
        "                img_p,\n",
        "            ) as src_img:\n",
        "                stack_chunk[\n",
        "                    t,\n",
        "                    :,\n",
        "                    :,\n",
        "                ] = src_img.read(\n",
        "                    1,\n",
        "                    window=window,\n",
        "                )\n",
        "\n",
        "        # 2.10. Compute Counts for this chunk using Numba\n",
        "        chunk_counts = compute_trajectory_changes_numba(\n",
        "            trajectory_window=traj_window,\n",
        "            stack_window=stack_chunk,\n",
        "            nodata=nodata_val,\n",
        "        )\n",
        "\n",
        "        # 2.11. Accumulate results\n",
        "        total_counts += chunk_counts\n",
        "\n",
        "    # 2.12. Format Results into DataFrame\n",
        "    print(\n",
        "        \"Formatting results...\",\n",
        "    )\n",
        "\n",
        "    # 2.13. Generate labels based on the actual number of intervals (n_intervals)\n",
        "    # This solves the dimension mismatch if 'years' length differs from 'image_paths'\n",
        "    interval_labels = [\n",
        "        f\"{years[i]}-{years[i+1]}\"\n",
        "        for i in range(\n",
        "            n_intervals,\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    df_results = pd.DataFrame(\n",
        "        total_counts[\n",
        "            :,\n",
        "            2:5,\n",
        "        ],\n",
        "        index=interval_labels,\n",
        "        columns=[\n",
        "            2,\n",
        "            3,\n",
        "            4,\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    # 2.14. Rename axes for clarity\n",
        "    df_results.index.name = \"Interval\"\n",
        "    df_results.columns.name = \"Trajectory_Class\"\n",
        "\n",
        "    # 2.15. Save to CSV\n",
        "    output_csv = os.path.join(\n",
        "        output_path,\n",
        "        \"trajectory_contributions_per_interval.csv\",\n",
        "    )\n",
        "    df_results.to_csv(\n",
        "        output_csv,\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"Results saved to: {output_csv}\",\n",
        "    )\n",
        "\n",
        "    return df_results\n",
        "\n",
        "\n",
        "# 3. Execution using the optimized function\n",
        "df_trajectory_intervals = analyze_trajectory_intervals_windowed(\n",
        "    image_paths=image_paths,\n",
        "    output_path=output_path,\n",
        "    years=years,\n",
        "    nodata_val=noData_value,\n",
        "    block_size=2048,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71KOkhF4KggL"
      },
      "outputs": [],
      "source": [
        "def plot_trajectory_contributions(\n",
        "    df: pd.DataFrame,\n",
        "    output_path: str,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Create stacked bar chart for trajectory contributions per interval.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        DataFrame with intervals as index and trajectory IDs (2, 3, 4) as columns.\n",
        "    output_path : str\n",
        "        Path to output directory for saving figure.\n",
        "    \"\"\"\n",
        "    # 0. Ensure columns are integers to match logic\n",
        "    df = df.copy()\n",
        "    df.columns = df.columns.astype(\n",
        "        int,\n",
        "    )\n",
        "\n",
        "    # 1. Calculate the maximum value to determine scale factor\n",
        "    max_val = df.sum(\n",
        "        axis=1,\n",
        "    ).max()\n",
        "\n",
        "    if max_val >= 1_000_000_000_000:\n",
        "        scale_factor = 1_000_000_000_000\n",
        "        y_label = \"Change (trillion pixels)\"\n",
        "    elif max_val >= 1_000_000_000:\n",
        "        scale_factor = 1_000_000_000\n",
        "        y_label = \"Change (billion pixels)\"\n",
        "    elif max_val >= 1_000_000:\n",
        "        scale_factor = 1_000_000\n",
        "        y_label = \"Change (million pixels)\"\n",
        "    elif max_val >= 1_000:\n",
        "        scale_factor = 1_000\n",
        "        y_label = \"Change (thousand pixels)\"\n",
        "    elif max_val >= 100:\n",
        "        scale_factor = 100\n",
        "        y_label = \"Change (hundred pixels)\"\n",
        "    else:\n",
        "        scale_factor = 1\n",
        "        y_label = \"Change (pixels)\"\n",
        "\n",
        "    # Apply scaling\n",
        "    df_scaled = df / scale_factor\n",
        "\n",
        "    # 2. Define colors and stacking order\n",
        "    colors = {\n",
        "        2: \"#990033\",\n",
        "        3: \"#cccc00\",\n",
        "        4: \"#000066\",\n",
        "    }\n",
        "\n",
        "    # Stacking order: 4 (bottom), 3 (middle), 2 (top)\n",
        "    stack_order = [\n",
        "        4,\n",
        "        3,\n",
        "        2,\n",
        "    ]\n",
        "\n",
        "    # 3. Create figure and axis\n",
        "    fig, ax = plt.subplots(\n",
        "        figsize=(\n",
        "            12,\n",
        "            6,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # 4. Plot stacked bars\n",
        "    bottom = pd.Series(\n",
        "        0.0,\n",
        "        index=df_scaled.index,\n",
        "    )\n",
        "\n",
        "    for traj_id in stack_order:\n",
        "        if traj_id in df_scaled.columns:\n",
        "            values = df_scaled[\n",
        "                traj_id\n",
        "            ]\n",
        "            ax.bar(\n",
        "                df_scaled.index,\n",
        "                values,\n",
        "                label=f\"Trajectory {traj_id}\",\n",
        "                bottom=bottom,\n",
        "                color=colors[\n",
        "                    traj_id\n",
        "                ],\n",
        "                edgecolor=\"none\",\n",
        "                width=0.8,\n",
        "            )\n",
        "            bottom += values\n",
        "\n",
        "    # 5. Customize axes and labels\n",
        "    ax.set_ylabel(\n",
        "        y_label,\n",
        "        fontsize=16,\n",
        "    )\n",
        "    ax.set_title(\n",
        "        \"Trajectory Contributions per Time Interval\",\n",
        "        fontsize=18,\n",
        "        pad=20,\n",
        "    )\n",
        "\n",
        "    # X-Axis formatting: Horizontal labels\n",
        "    ax.tick_params(\n",
        "        axis=\"x\",\n",
        "        labelsize=12,\n",
        "        rotation=0,\n",
        "    )\n",
        "\n",
        "    # Y-Axis formatting (mticker)\n",
        "    ax.tick_params(\n",
        "        axis=\"y\",\n",
        "        labelsize=14,\n",
        "    )\n",
        "    ax.yaxis.set_major_locator(\n",
        "        mticker.MaxNLocator(\n",
        "            integer=True,\n",
        "            nbins=5,\n",
        "        ),\n",
        "    )\n",
        "    ax.yaxis.set_major_formatter(\n",
        "        mticker.FormatStrFormatter(\n",
        "            \"%d\",\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Spines visible, NO GRID\n",
        "    for spine in [\n",
        "        \"top\",\n",
        "        \"right\",\n",
        "        \"left\",\n",
        "        \"bottom\",\n",
        "    ]:\n",
        "        ax.spines[\n",
        "            spine\n",
        "        ].set_visible(\n",
        "            True,\n",
        "        )\n",
        "\n",
        "    # 6. Legend\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "\n",
        "    if handles:\n",
        "        # Reorder handles to match 2, 3, 4\n",
        "        legend_order_map = {\n",
        "            \"Trajectory 2\": 0,\n",
        "            \"Trajectory 3\": 1,\n",
        "            \"Trajectory 4\": 2,\n",
        "        }\n",
        "\n",
        "        # Sort handles based on labels\n",
        "        sorted_pairs = sorted(\n",
        "            zip(\n",
        "                handles,\n",
        "                labels,\n",
        "            ),\n",
        "            key=lambda x: legend_order_map.get(\n",
        "                x[1],\n",
        "                99,\n",
        "            ),\n",
        "        )\n",
        "        sorted_handles, sorted_labels = zip(\n",
        "            *sorted_pairs,\n",
        "        )\n",
        "\n",
        "        ax.legend(\n",
        "            sorted_handles,\n",
        "            sorted_labels,\n",
        "            loc=\"center left\",\n",
        "            bbox_to_anchor=(\n",
        "                1.01,\n",
        "                0.5,\n",
        "            ),\n",
        "            fontsize=14,\n",
        "            frameon=False,\n",
        "        )\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # 7. Save figure\n",
        "    output_fig = os.path.join(\n",
        "        output_path,\n",
        "        \"graphic_trajectory_contributions_stacked_bar.png\",\n",
        "    )\n",
        "    plt.savefig(\n",
        "        output_fig,\n",
        "        dpi=300,\n",
        "        bbox_inches=\"tight\",\n",
        "        format=\"png\",\n",
        "    )\n",
        "    plt.show()\n",
        "\n",
        "    print(\n",
        "        f\"Figure saved to: {output_fig}\",\n",
        "    )\n",
        "\n",
        "\n",
        "# Executtion u\n",
        "plot_trajectory_contributions(\n",
        "    df=df_trajectory_intervals,\n",
        "    output_path=output_path,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRuZkO4EKggM"
      },
      "source": [
        "## **9. Change Component Maps**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.1 Quantity"
      ],
      "metadata": {
        "id": "MpJdlwu7v2YD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9.1.1 Compute Quantity Map"
      ],
      "metadata": {
        "id": "Tq1WrhJTv_Ba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import rasterio\n",
        "from rasterio.windows import Window\n",
        "from tqdm import tqdm\n",
        "from numba import jit\n",
        "\n",
        "@jit(nopython=True)\n",
        "def _compute_quantity_block_numba(\n",
        "    stack_data,\n",
        "    input_nodata,\n",
        "    output_nodata,\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculate the Quantity Component metric pixel-by-pixel using JIT compilation.\n",
        "\n",
        "    This function runs at C-speed. According to Pontius's logic, a pixel's\n",
        "    quantity component is 1 if the begin class differs from the finish class;\n",
        "    otherwise, it is 0.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    stack_data : np.ndarray\n",
        "        3D array of shape (Height, Width, Time) containing the pixel trajectories.\n",
        "    input_nodata : int or float\n",
        "        The NoData value present in the input rasters.\n",
        "    output_nodata : int or float\n",
        "        The value to assign to invalid pixels in the output.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        2D array (Height, Width) containing the quantity values (0 or 1).\n",
        "    \"\"\"\n",
        "    rows = stack_data.shape[0]\n",
        "    cols = stack_data.shape[1]\n",
        "    time_steps = stack_data.shape[2]\n",
        "\n",
        "    # Initialize output array. Using uint8 since values will strictly be 0, 1, or NoData.\n",
        "    output_array = np.full(\n",
        "        (\n",
        "            rows,\n",
        "            cols,\n",
        "        ),\n",
        "        output_nodata,\n",
        "        dtype=np.uint8,\n",
        "    )\n",
        "\n",
        "    for r in range(rows):\n",
        "        for c in range(cols):\n",
        "            # Extract trajectory for the current pixel\n",
        "            traj = stack_data[\n",
        "                r,\n",
        "                c,\n",
        "                :,\n",
        "            ]\n",
        "\n",
        "            # 1. Check for Input NoData (Strict Masking)\n",
        "            is_valid = True\n",
        "            for t in range(time_steps):\n",
        "                if traj[t] == input_nodata:\n",
        "                    is_valid = False\n",
        "                    break\n",
        "\n",
        "            if not is_valid:\n",
        "                continue\n",
        "\n",
        "            # 2. Calculate Quantity Component\n",
        "            # Compare only the first (start) and last (end) steps of the trajectory\n",
        "            start_val = traj[0]\n",
        "            end_val = traj[time_steps - 1]\n",
        "\n",
        "            if start_val != end_val:\n",
        "                output_array[\n",
        "                    r,\n",
        "                    c,\n",
        "                ] = 1\n",
        "            else:\n",
        "                output_array[\n",
        "                    r,\n",
        "                    c,\n",
        "                ] = 0\n",
        "\n",
        "    return output_array\n",
        "\n",
        "# --- Main Processing Function ---\n",
        "\n",
        "def generate_quantity_component_map_optimized(\n",
        "    file_list,\n",
        "    output_dir,\n",
        "    input_nodata_val,\n",
        "    output_filename=\"map_quantity_component.tif\",\n",
        "    block_size=1024,\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate the Quantity Component map using Numba acceleration and\n",
        "    optimized I/O block processing.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    file_list : list of str\n",
        "        List of paths to the raster files (sorted by time).\n",
        "    output_dir : str\n",
        "        Directory to save the output raster.\n",
        "    input_nodata_val : int or float\n",
        "        The NoData value used in the input rasters.\n",
        "    output_filename : str, optional\n",
        "        Name of the output file.\n",
        "    block_size : int, optional\n",
        "        Size of the processing window (default is 1024).\n",
        "    \"\"\"\n",
        "    output_path = os.path.join(\n",
        "        output_dir,\n",
        "        output_filename,\n",
        "    )\n",
        "\n",
        "    output_nodata_val = input_nodata_val\n",
        "\n",
        "    print(\n",
        "        f\"Starting optimized generation of Quantity Component map: {output_path}\",\n",
        "    )\n",
        "\n",
        "    # 1. Open all datasets ONCE to avoid I/O overhead\n",
        "    srcs = [\n",
        "        rasterio.open(\n",
        "            path,\n",
        "        ) for path in file_list\n",
        "    ]\n",
        "\n",
        "    # 2. Extract metadata from the first source\n",
        "    height = srcs[0].height\n",
        "    width = srcs[0].width\n",
        "    profile = srcs[0].profile.copy()\n",
        "\n",
        "    # 3. Update profile for the output raster\n",
        "    profile.update(\n",
        "        {\n",
        "            \"driver\": \"GTiff\",\n",
        "            \"dtype\": rasterio.uint8,\n",
        "            \"count\": 1,\n",
        "            \"nodata\": output_nodata_val,\n",
        "            \"compress\": \"lzw\",\n",
        "            \"tiled\": True,\n",
        "            \"blockxsize\": 256,\n",
        "            \"blockysize\": 256,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # 4. Create output file and iterate blocks\n",
        "    with rasterio.open(\n",
        "        output_path,\n",
        "        \"w+\",\n",
        "        **profile,\n",
        "    ) as dst:\n",
        "\n",
        "        rows = range(\n",
        "            0,\n",
        "            height,\n",
        "            block_size,\n",
        "        )\n",
        "        cols = range(\n",
        "            0,\n",
        "            width,\n",
        "            block_size,\n",
        "        )\n",
        "        total_blocks = len(rows) * len(cols)\n",
        "\n",
        "        with tqdm(\n",
        "            total=total_blocks,\n",
        "            desc=\"Processing Blocks\",\n",
        "            unit=\"block\",\n",
        "        ) as pbar:\n",
        "\n",
        "            for r_off in rows:\n",
        "                for c_off in cols:\n",
        "                    window = Window(\n",
        "                        c_off,\n",
        "                        r_off,\n",
        "                        min(\n",
        "                            block_size,\n",
        "                            width - c_off,\n",
        "                        ),\n",
        "                        min(\n",
        "                            block_size,\n",
        "                            height - r_off,\n",
        "                        ),\n",
        "                    )\n",
        "\n",
        "                    # 5. Read the block from datasets\n",
        "                    stack_list = []\n",
        "\n",
        "                    for src in srcs:\n",
        "                        data = src.read(\n",
        "                            1,\n",
        "                            window=window,\n",
        "                        )\n",
        "                        stack_list.append(\n",
        "                            data,\n",
        "                        )\n",
        "\n",
        "                    # 6. Prepare data for Numba (Height, Width, Time)\n",
        "                    stack_data = np.array(\n",
        "                        stack_list,\n",
        "                    )\n",
        "                    stack_data = np.moveaxis(\n",
        "                        stack_data,\n",
        "                        0,\n",
        "                        -1,\n",
        "                    )\n",
        "\n",
        "                    # 7. Execute Numba Optimized Computation\n",
        "                    output_block = _compute_quantity_block_numba(\n",
        "                        stack_data,\n",
        "                        input_nodata_val,\n",
        "                        output_nodata_val,\n",
        "                    )\n",
        "\n",
        "                    # 8. Write result to disk\n",
        "                    dst.write(\n",
        "                        output_block,\n",
        "                        1,\n",
        "                        window=window,\n",
        "                    )\n",
        "\n",
        "                    pbar.update(1)\n",
        "\n",
        "    # 9. Close all datasets\n",
        "    for src in srcs:\n",
        "        src.close()\n",
        "\n",
        "    print(\n",
        "        f\"Process completed successfully. Quantity Component map saved to: {output_path}\",\n",
        "    )\n",
        "\n",
        "# Execution using the optimized function\n",
        "generate_quantity_component_map_optimized(\n",
        "    file_list=image_paths,\n",
        "    output_dir=output_path,\n",
        "    input_nodata_val=noData_value,\n",
        ")"
      ],
      "metadata": {
        "id": "kDuJ2Qoev6QP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9.1.2 Plot Quantity Map"
      ],
      "metadata": {
        "id": "t2b0GXALxIKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import rasterio\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
        "from matplotlib.patches import Patch\n",
        "import matplotlib.ticker as mticker\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "from pyproj import Transformer\n",
        "from matplotlib_scalebar.scalebar import ScaleBar\n",
        "\n",
        "def plot_quantity_component_map(\n",
        "    output_dir: str,\n",
        "    nodata_val: int,\n",
        "    raster_filename: str = \"map_quantity_component.tif\",\n",
        "    max_display_size: int = 1500,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Plot the Quantity Component map using a discrete integer scale,\n",
        "    with dynamic downsampling to prevent out-of-memory errors.\n",
        "    \"\"\"\n",
        "    # 1. Input Validation and Path Setup\n",
        "    raster_path = os.path.join(\n",
        "        output_dir,\n",
        "        raster_filename,\n",
        "    )\n",
        "\n",
        "    if not os.path.exists(\n",
        "        raster_path,\n",
        "    ):\n",
        "        raise FileNotFoundError(\n",
        "            f\"Raster not found: {raster_path}\",\n",
        "        )\n",
        "\n",
        "    print(\n",
        "        f\"Reading Quantity Component map for plotting: {raster_path}\",\n",
        "    )\n",
        "\n",
        "    # 2. Data Loading, Dynamic Downsampling, and Masking\n",
        "    with rasterio.open(\n",
        "        raster_path,\n",
        "    ) as src:\n",
        "        max_dim = max(src.width, src.height)\n",
        "        scale_factor = max(1.0, max_dim / max_display_size)\n",
        "\n",
        "        new_height = max(1, int(src.height / scale_factor))\n",
        "        new_width = max(1, int(src.width / scale_factor))\n",
        "\n",
        "        print(f\"Dynamic downsample factor: {scale_factor:.2f}\")\n",
        "\n",
        "        data = src.read(\n",
        "            1,\n",
        "            out_shape=(\n",
        "                new_height,\n",
        "                new_width,\n",
        "            ),\n",
        "            resampling=rasterio.enums.Resampling.nearest,\n",
        "        )\n",
        "\n",
        "        # Force masking using the provided variable\n",
        "        data_masked = np.ma.masked_equal(\n",
        "            data,\n",
        "            nodata_val,\n",
        "        )\n",
        "\n",
        "        src_crs = src.crs\n",
        "        transform = src.transform\n",
        "        orig_height = src.height\n",
        "        orig_width = src.width\n",
        "\n",
        "    pixel_size_km = compute_display_pixel_size_km(\n",
        "        raster_path=raster_path,\n",
        "        downsample_divisor=scale_factor,\n",
        "    )\n",
        "\n",
        "    # Determine the integer range for the color scale\n",
        "    data_max = int(\n",
        "        data_masked.max(),\n",
        "    )\n",
        "\n",
        "    if data_max == 0:\n",
        "        data_max = 1\n",
        "\n",
        "    # 3. Discrete Colormap Configuration\n",
        "    original_cmap = plt.get_cmap(\"viridis\")\n",
        "    colors_list = [\"#c0c0c0\"] + [\n",
        "        original_cmap(i) for i in np.linspace(0, 1, data_max)\n",
        "    ]\n",
        "\n",
        "    cmap = ListedColormap(\n",
        "        colors_list,\n",
        "    )\n",
        "\n",
        "    # 4. Define Boundaries (Bins) for Integers\n",
        "    bounds = np.arange(\n",
        "        0,\n",
        "        data_max + 2,\n",
        "    )\n",
        "    norm = BoundaryNorm(\n",
        "        bounds,\n",
        "        cmap.N,\n",
        "    )\n",
        "\n",
        "    # 5. Plotting the Figure\n",
        "    fig, ax = plt.subplots(\n",
        "        figsize=(\n",
        "            6,\n",
        "            8,\n",
        "        ),\n",
        "        dpi=300,\n",
        "    )\n",
        "\n",
        "    im = ax.imshow(\n",
        "        data_masked,\n",
        "        cmap=cmap,\n",
        "        norm=norm,\n",
        "        interpolation=\"nearest\",\n",
        "    )\n",
        "\n",
        "    # 6. Legend Configuration\n",
        "    legend_elements = []\n",
        "    for i in range(0, data_max + 1):\n",
        "        legend_elements.append(\n",
        "            Patch(\n",
        "                facecolor=cmap(norm(i)),\n",
        "                edgecolor=\"black\",\n",
        "                linewidth=0.5,\n",
        "                label=str(i),\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    ax.legend(\n",
        "        handles=legend_elements,\n",
        "        title=\"Quantity\",\n",
        "        loc=\"center left\",\n",
        "        bbox_to_anchor=(1.02, 0.5),\n",
        "        frameon=False,\n",
        "        fontsize=8,\n",
        "        title_fontsize=10,\n",
        "        alignment=\"left\",\n",
        "    )\n",
        "\n",
        "    # 7. Cartographic Elements\n",
        "    scalebar = ScaleBar(\n",
        "        pixel_size_km,\n",
        "        units=\"km\",\n",
        "        length_fraction=0.35,\n",
        "        location=\"lower right\",\n",
        "        box_alpha=0.0,\n",
        "        scale_formatter=lambda value, _: f\"{int(value)} km\",\n",
        "    )\n",
        "    ax.add_artist(\n",
        "        scalebar,\n",
        "    )\n",
        "\n",
        "    # 8. North Arrow\n",
        "    try:\n",
        "        north_arrow(\n",
        "            ax,\n",
        "            location=\"upper left\",\n",
        "            shadow=False,\n",
        "            rotation={\n",
        "                \"degrees\": 0,\n",
        "            },\n",
        "        )\n",
        "    except NameError:\n",
        "        print(\"Note: north_arrow function not found, skipping north arrow.\")\n",
        "\n",
        "    # 9) Axes styling (with Lat/Lon labels)\n",
        "    ax.set_title(\n",
        "        \"Quantity Component\",\n",
        "        fontsize=18,\n",
        "        pad=5,\n",
        "    )\n",
        "    ax.set_aspect(\n",
        "        \"equal\",\n",
        "    )\n",
        "\n",
        "    # Initialize Transformer (Native CRS -> Lat/Lon)\n",
        "    to_latlon = Transformer.from_crs(\n",
        "        src_crs,\n",
        "        \"EPSG:4326\",\n",
        "        always_xy=True,\n",
        "    )\n",
        "\n",
        "    def format_lon(x, pos):\n",
        "        x_orig = np.clip(x * scale_factor, 0, orig_width - 1)\n",
        "        x_proj, y_proj = rasterio.transform.xy(transform, orig_height // 2, x_orig)\n",
        "        lon, lat = to_latlon.transform(x_proj, y_proj)\n",
        "        return f\"{lon:.1f}°\"\n",
        "\n",
        "    def format_lat(y, pos):\n",
        "        y_orig = np.clip(y * scale_factor, 0, orig_height - 1)\n",
        "        x_proj, y_proj = rasterio.transform.xy(transform, y_orig, orig_width // 2)\n",
        "        lon, lat = to_latlon.transform(x_proj, y_proj)\n",
        "        return f\"{lat:.1f}°\"\n",
        "\n",
        "    # Apply the custom formatters\n",
        "    ax.xaxis.set_major_formatter(FuncFormatter(format_lon))\n",
        "    ax.yaxis.set_major_formatter(FuncFormatter(format_lat))\n",
        "\n",
        "    # Limit ticks to avoid overcrowding\n",
        "    ax.xaxis.set_major_locator(mticker.MaxNLocator(nbins=4))\n",
        "    ax.yaxis.set_major_locator(mticker.MaxNLocator(nbins=6))\n",
        "\n",
        "    # Final styling for ticks\n",
        "    ax.tick_params(\n",
        "        axis=\"both\",\n",
        "        which=\"major\",\n",
        "        labelsize=7,\n",
        "        pad=4,\n",
        "    )\n",
        "    plt.setp(\n",
        "        ax.get_yticklabels(),\n",
        "        rotation=90,\n",
        "        va=\"center\",\n",
        "    )\n",
        "\n",
        "    output_figure_path = os.path.join(\n",
        "        output_dir,\n",
        "        \"map_quantity_component.png\",\n",
        "    )\n",
        "\n",
        "    plt.savefig(\n",
        "        output_figure_path,\n",
        "        dpi=300,\n",
        "        format=\"png\",\n",
        "        bbox_inches=\"tight\",\n",
        "        pad_inches=0.5,\n",
        "    )\n",
        "    plt.show()\n",
        "\n",
        "    print(\n",
        "        f\"Map successfully saved to: {output_figure_path}\",\n",
        "    )\n",
        "\n",
        "# Execution of the plotting function\n",
        "plot_quantity_component_map(\n",
        "    output_dir=output_path,\n",
        "    nodata_val=noData_value,\n",
        "    max_display_size=1500,\n",
        ")"
      ],
      "metadata": {
        "id": "G_vqn57YxL00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnSQvyD4Lk3h"
      },
      "source": [
        "### 9.2 Alternation Exchange"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIoC8RwcTbEd"
      },
      "source": [
        "#### 9.2.1 Compute Alternation Exchange Map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQq2qrOjHY6-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import rasterio\n",
        "from rasterio.windows import Window\n",
        "from tqdm import tqdm\n",
        "from numba import jit\n",
        "\n",
        "@jit(nopython=True)\n",
        "def _compute_exchange_block_numba(\n",
        "    stack_data,\n",
        "    input_nodata,\n",
        "    output_nodata,\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculate the Alternation Exchange metric pixel-by-pixel using JIT compilation.\n",
        "\n",
        "    This function runs at C-speed, avoiding the slow Python loops. The mathematical\n",
        "    logic remains strictly identical to the original specification.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    stack_data : np.ndarray\n",
        "        3D array of shape (Height, Width, Time) containing the pixel trajectories.\n",
        "    input_nodata : int or float\n",
        "        The NoData value present in the input rasters.\n",
        "    output_nodata : int or float\n",
        "        The value to assign to invalid pixels in the output.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        2D array (Height, Width) inheriting the input dtype containing the exchange values.\n",
        "    \"\"\"\n",
        "    rows, cols, time_steps = stack_data.shape\n",
        "\n",
        "    # Initialize output array using the EXACT same data type as the input stack.\n",
        "    # This ensures that negative NoData values (like -9999) fit perfectly.\n",
        "    output_array = np.full(\n",
        "        (\n",
        "            rows,\n",
        "            cols,\n",
        "        ),\n",
        "        output_nodata,\n",
        "        dtype=np.uint8,\n",
        "    )\n",
        "\n",
        "    # Pre-allocate a small buffer for unique classes to avoid dynamic allocation in loop\n",
        "    classes_buffer = np.zeros(\n",
        "        50,\n",
        "        dtype=np.int16,\n",
        "    )\n",
        "\n",
        "    for r in range(rows):\n",
        "        for c in range(cols):\n",
        "            # Extract trajectory for the current pixel\n",
        "            traj = stack_data[\n",
        "                r,\n",
        "                c,\n",
        "                :,\n",
        "            ]\n",
        "\n",
        "            # 1. Check for Input NoData (Strict Masking)\n",
        "            is_valid = True\n",
        "            for t in range(time_steps):\n",
        "                if traj[t] == input_nodata:\n",
        "                    is_valid = False\n",
        "                    break\n",
        "\n",
        "            if not is_valid:\n",
        "                continue\n",
        "\n",
        "            # 2. Identify unique classes manually\n",
        "            n_classes = 0\n",
        "            for t in range(time_steps):\n",
        "                val = traj[t]\n",
        "                found = False\n",
        "                for k in range(n_classes):\n",
        "                    if classes_buffer[k] == val:\n",
        "                        found = True\n",
        "                        break\n",
        "\n",
        "                if not found:\n",
        "                    if n_classes < 50:\n",
        "                        classes_buffer[n_classes] = val\n",
        "                        n_classes += 1\n",
        "\n",
        "            # If no change possible (0 or 1 class), result is 0\n",
        "            if n_classes <= 1:\n",
        "                output_array[\n",
        "                    r,\n",
        "                    c,\n",
        "                ] = 0\n",
        "                continue\n",
        "\n",
        "            # 3. Build Local Transition Matrix\n",
        "            mat = np.zeros(\n",
        "                (\n",
        "                    n_classes,\n",
        "                    n_classes,\n",
        "                ),\n",
        "                dtype=np.int16,\n",
        "            )\n",
        "\n",
        "            for t in range(time_steps - 1):\n",
        "                s_val = traj[t]\n",
        "                e_val = traj[t + 1]\n",
        "\n",
        "                if s_val == e_val:\n",
        "                    continue\n",
        "\n",
        "                # Find indices in our local buffer\n",
        "                s_idx = -1\n",
        "                e_idx = -1\n",
        "                for k in range(n_classes):\n",
        "                    if classes_buffer[k] == s_val:\n",
        "                        s_idx = k\n",
        "                    if classes_buffer[k] == e_val:\n",
        "                        e_idx = k\n",
        "\n",
        "                if s_idx >= 0 and e_idx >= 0:\n",
        "                    mat[\n",
        "                        s_idx,\n",
        "                        e_idx,\n",
        "                    ] += 1\n",
        "\n",
        "            # 4. Calculate Alternation Exchange: Sum(min(M, M.T))\n",
        "            exchange_sum = 0\n",
        "            for i in range(n_classes):\n",
        "                for j in range(n_classes):\n",
        "                    v1 = mat[\n",
        "                        i,\n",
        "                        j,\n",
        "                    ]\n",
        "                    v2 = mat[\n",
        "                        j,\n",
        "                        i,\n",
        "                    ]\n",
        "                    if v1 < v2:\n",
        "                        exchange_sum += v1\n",
        "                    else:\n",
        "                        exchange_sum += v2\n",
        "\n",
        "            # Assign value to output\n",
        "            output_array[\n",
        "                r,\n",
        "                c,\n",
        "            ] = exchange_sum\n",
        "\n",
        "    return output_array\n",
        "\n",
        "# --- Main Processing Function ---\n",
        "\n",
        "def generate_alternation_exchange_map_optimized(\n",
        "    file_list,\n",
        "    output_dir,\n",
        "    input_nodata_val,\n",
        "    output_filename=\"map_alternation_exchange.tif\",\n",
        "    block_size=1024,\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate the Alternation Exchange map using Numba acceleration and\n",
        "    optimized I/O block processing (keeping files open).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    file_list : list of str\n",
        "        List of paths to the raster files (sorted by time).\n",
        "    output_dir : str\n",
        "        Directory to save the output raster.\n",
        "    input_nodata_val : int or float\n",
        "        The NoData value used in the input rasters.\n",
        "    output_filename : str, optional\n",
        "        Name of the output file.\n",
        "    block_size : int, optional\n",
        "        Size of the processing window (default is 1024).\n",
        "    \"\"\"\n",
        "    output_path = os.path.join(\n",
        "        output_dir,\n",
        "        output_filename,\n",
        "    )\n",
        "\n",
        "    # We use the user's NoData variable directly\n",
        "    output_nodata_val = input_nodata_val\n",
        "\n",
        "    print(\n",
        "        f\"Starting optimized generation of Alternation Exchange map: {output_path}\",\n",
        "    )\n",
        "\n",
        "    # 1. Open all datasets ONCE to avoid I/O overhead during the loop\n",
        "    srcs = [\n",
        "        rasterio.open(\n",
        "            path,\n",
        "        ) for path in file_list\n",
        "    ]\n",
        "\n",
        "    # 2. Extract metadata from the first source\n",
        "    height = srcs[0].height\n",
        "    width = srcs[0].width\n",
        "    profile = srcs[0].profile.copy()\n",
        "\n",
        "    # 3. Update profile for the output raster\n",
        "    # Notice: 'dtype' is intentionally omitted here to inherit the input's dtype,\n",
        "    # ensuring the custom NoData value fits perfectly.\n",
        "    profile.update(\n",
        "        {\n",
        "            \"driver\": \"GTiff\",\n",
        "            \"dtype\": rasterio.uint8,\n",
        "            \"count\": 1,\n",
        "            \"nodata\": output_nodata_val,\n",
        "            \"compress\": \"lzw\",\n",
        "            \"tiled\": True,\n",
        "            \"blockxsize\": 256,\n",
        "            \"blockysize\": 256,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # 4. Create output file and iterate blocks\n",
        "    with rasterio.open(\n",
        "        output_path,\n",
        "        \"w+\",\n",
        "        **profile,\n",
        "    ) as dst:\n",
        "\n",
        "        rows = range(\n",
        "            0,\n",
        "            height,\n",
        "            block_size,\n",
        "        )\n",
        "        cols = range(\n",
        "            0,\n",
        "            width,\n",
        "            block_size,\n",
        "        )\n",
        "        total_blocks = len(rows) * len(cols)\n",
        "\n",
        "        with tqdm(\n",
        "            total=total_blocks,\n",
        "            desc=\"Processing Blocks\",\n",
        "            unit=\"block\",\n",
        "        ) as pbar:\n",
        "\n",
        "            for r_off in rows:\n",
        "                for c_off in cols:\n",
        "                    window = Window(\n",
        "                        c_off,\n",
        "                        r_off,\n",
        "                        min(\n",
        "                            block_size,\n",
        "                            width - c_off,\n",
        "                        ),\n",
        "                        min(\n",
        "                            block_size,\n",
        "                            height - r_off,\n",
        "                        ),\n",
        "                    )\n",
        "\n",
        "                    # 5. Read the block from the ALREADY OPEN datasets\n",
        "                    stack_list = []\n",
        "\n",
        "                    for src in srcs:\n",
        "                        data = src.read(\n",
        "                            1,\n",
        "                            window=window,\n",
        "                        )\n",
        "                        stack_list.append(\n",
        "                            data,\n",
        "                        )\n",
        "\n",
        "                    # 6. Prepare data for Numba (Height, Width, Time)\n",
        "                    stack_data = np.array(\n",
        "                        stack_list,\n",
        "                    )\n",
        "                    stack_data = np.moveaxis(\n",
        "                        stack_data,\n",
        "                        0,\n",
        "                        -1,\n",
        "                    )\n",
        "\n",
        "                    # 7. Execute Numba Optimized Computation\n",
        "                    output_block = _compute_exchange_block_numba(\n",
        "                        stack_data,\n",
        "                        input_nodata_val,\n",
        "                        output_nodata_val,\n",
        "                    )\n",
        "\n",
        "                    # 8. Write result to disk\n",
        "                    dst.write(\n",
        "                        output_block,\n",
        "                        1,\n",
        "                        window=window,\n",
        "                    )\n",
        "\n",
        "                    pbar.update(1)\n",
        "\n",
        "    # 9. Close all datasets to free up file descriptors\n",
        "    for src in srcs:\n",
        "        src.close()\n",
        "\n",
        "    print(\n",
        "        f\"Process completed successfully. Alternation Exchange map saved to: {output_path}\",\n",
        "    )\n",
        "\n",
        "# Execution using the optimized function\n",
        "\n",
        "generate_alternation_exchange_map_optimized(\n",
        "    file_list=image_paths,\n",
        "    output_dir=output_path,\n",
        "    input_nodata_val=noData_value,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmKoYydsy_Oz"
      },
      "source": [
        "#### 9.2.2 Plot Alternation Exchange map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgO6zgWAy_Oz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mticker\n",
        "import rasterio\n",
        "from rasterio.enums import Resampling\n",
        "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
        "from matplotlib.patches import Patch\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "from matplotlib_scalebar.scalebar import ScaleBar\n",
        "from pyproj import Transformer\n",
        "\n",
        "def plot_alternation_exchange_discrete_map(\n",
        "    output_dir: str,\n",
        "    nodata_val: int,\n",
        "    raster_filename: str = \"map_alternation_exchange.tif\",\n",
        "    max_display_size: int = 1500, # Added dynamic downsampling parameter\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Plot the Alternation Exchange map using a discrete integer scale,\n",
        "    incorporating dynamic downsampling to prevent memory overflow.\n",
        "\n",
        "    The visualization uses a specific color logic where the value 0 is\n",
        "    represented in gray (indicating stability or absence of exchange),\n",
        "    while values greater than or equal to 1 are represented by a\n",
        "    gradient color scale (e.g., from pink to dark red) to indicate\n",
        "    the intensity of the exchange.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    output_dir : str\n",
        "        The directory path containing the input raster file and where\n",
        "        the output image will be saved.\n",
        "    nodata_val : int\n",
        "        Value representing NoData pixels to be masked.\n",
        "    raster_filename : str, optional\n",
        "        The filename of the raster to be plotted (default is\n",
        "        \"map_alternation_exchange.tif\").\n",
        "    max_display_size : int, optional\n",
        "        Maximum width/height in pixels for the generated plot to prevent\n",
        "        RAM overflow. Default is 1500.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "        The function saves a PNG image to the output directory and\n",
        "        displays the plot.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    FileNotFoundError\n",
        "        If the specified raster file does not exist in the output directory.\n",
        "    \"\"\"\n",
        "    # 1. Input Validation and Path Setup\n",
        "    raster_path = os.path.join(\n",
        "        output_dir,\n",
        "        raster_filename,\n",
        "    )\n",
        "\n",
        "    if not os.path.exists(\n",
        "        raster_path,\n",
        "    ):\n",
        "        raise FileNotFoundError(\n",
        "            f\"Raster not found: {raster_path}\",\n",
        "        )\n",
        "\n",
        "    print(\n",
        "        f\"Reading Alternation Exchange map for plotting: {raster_path}\",\n",
        "    )\n",
        "\n",
        "    # Calculate dynamic scale factor based on max display size\n",
        "    with rasterio.open(raster_path) as src:\n",
        "        max_dim = max(src.width, src.height)\n",
        "        scale_factor = max(1.0, max_dim / max_display_size)\n",
        "\n",
        "        src_crs = src.crs\n",
        "        transform = src.transform\n",
        "\n",
        "    # Get the exact pixel size in km for the scalebar using the scale_factor\n",
        "    pixel_size_km = compute_display_pixel_size_km(\n",
        "        raster_path=raster_path,\n",
        "        downsample_divisor=scale_factor,\n",
        "    )\n",
        "\n",
        "    # 2. Data Loading and Masking with Downsampling\n",
        "    with rasterio.open(\n",
        "        raster_path,\n",
        "    ) as src:\n",
        "\n",
        "        # Calculate new shape safely\n",
        "        new_height = max(1, int(src.height / scale_factor))\n",
        "        new_width = max(1, int(src.width / scale_factor))\n",
        "\n",
        "        data = src.read(\n",
        "            1,\n",
        "            out_shape=(\n",
        "                new_height,\n",
        "                new_width,\n",
        "            ),\n",
        "            resampling=Resampling.nearest,\n",
        "        )\n",
        "\n",
        "        # Force masking using the provided variable\n",
        "        data_masked = np.ma.masked_equal(\n",
        "            data,\n",
        "            nodata_val,\n",
        "        )\n",
        "\n",
        "    # Determine the integer range for the color scale\n",
        "    data_max = int(\n",
        "        data_masked.max(),\n",
        "    )\n",
        "\n",
        "    # Ensure at least 1 to prevent errors if the map is empty/flat\n",
        "    if data_max == 0:\n",
        "        data_max = 1\n",
        "\n",
        "    # 3. Discrete Colormap Configuration\n",
        "    original_cmap = plt.get_cmap(\"viridis\")\n",
        "    # Define the color for value 0 (Background/Gray)\n",
        "    colors_list = [\"#c0c0c0\"] + [\n",
        "        original_cmap(i) for i in np.linspace(0, 1, data_max)\n",
        "    ]\n",
        "\n",
        "    # Create the custom discrete colormap\n",
        "    cmap = ListedColormap(\n",
        "        colors_list,\n",
        "    )\n",
        "\n",
        "    # 4. Define Boundaries (Bins) for Integers\n",
        "    # Create boundaries for every integer from 0 to max + 1\n",
        "    bounds = np.arange(\n",
        "        0,\n",
        "        data_max + 2,\n",
        "    ) - 0.5\n",
        "    norm = BoundaryNorm(\n",
        "        bounds,\n",
        "        cmap.N,\n",
        "    )\n",
        "\n",
        "    # 5. Plotting the Figure\n",
        "    fig, ax = plt.subplots(\n",
        "        figsize=(\n",
        "            6,\n",
        "            8,\n",
        "        ),\n",
        "        dpi=300,\n",
        "    )\n",
        "\n",
        "    im = ax.imshow(\n",
        "        data_masked,\n",
        "        cmap=cmap,\n",
        "        norm=norm,\n",
        "        interpolation=\"nearest\",\n",
        "    )\n",
        "\n",
        "    # 6. Legend Configuration\n",
        "    legend_elements = []\n",
        "    for i in range(0, data_max + 1):\n",
        "        legend_elements.append(\n",
        "            Patch(\n",
        "                facecolor=cmap(norm(i)),\n",
        "                edgecolor=\"black\",\n",
        "                linewidth=0.5,\n",
        "                label=str(i),\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    ax.legend(\n",
        "        handles=legend_elements,\n",
        "        title=\"Exchange\",\n",
        "        loc=\"center left\",\n",
        "        bbox_to_anchor=(1.02, 0.5),\n",
        "        frameon=False,\n",
        "        fontsize=8,\n",
        "        title_fontsize=10,\n",
        "        alignment=\"left\",\n",
        "    )\n",
        "\n",
        "    # 7. Cartographic Elements\n",
        "    scalebar = ScaleBar(\n",
        "        dx=pixel_size_km,\n",
        "        units=\"km\",\n",
        "        length_fraction=0.35,\n",
        "        location=\"lower right\",\n",
        "        box_alpha=0.0,\n",
        "        scale_formatter=lambda value, _: f\"{int(value)} km\",\n",
        "    )\n",
        "    ax.add_artist(\n",
        "        scalebar,\n",
        "    )\n",
        "\n",
        "    # 8. North Arrow\n",
        "    try:\n",
        "        north_arrow(\n",
        "            ax,\n",
        "            location=\"upper left\",\n",
        "            shadow=False,\n",
        "            rotation={\n",
        "                \"degrees\": 0,\n",
        "            },\n",
        "        )\n",
        "    except NameError:\n",
        "        print(\"Note: north_arrow function not found, skipping.\")\n",
        "\n",
        "    # 9) Axes styling (with Lat/Lon labels)\n",
        "    ax.set_title(\n",
        "        \"Exchange\",\n",
        "        fontsize=18,\n",
        "        pad=5,\n",
        "    )\n",
        "    ax.set_aspect(\n",
        "        \"equal\",\n",
        "    )\n",
        "\n",
        "    # Initialize Transformer (Native CRS -> Lat/Lon)\n",
        "    to_latlon = Transformer.from_crs(\n",
        "        src_crs,\n",
        "        \"EPSG:4326\",\n",
        "        always_xy=True,\n",
        "    )\n",
        "\n",
        "    # Get dimensions from the loaded DOWNSAMPLED data\n",
        "    height, width = data_masked.shape\n",
        "\n",
        "    def format_lon(\n",
        "        x,\n",
        "        pos,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Convert a pixel column index to a formatted Longitude string.\n",
        "        \"\"\"\n",
        "        x = np.clip(\n",
        "            x,\n",
        "            0,\n",
        "            width - 1,\n",
        "        )\n",
        "        # Multiply by scale_factor to map back to original pixels\n",
        "        original_x = x * scale_factor\n",
        "        original_y = (height // 2) * scale_factor\n",
        "\n",
        "        x_proj, y_proj = rasterio.transform.xy(\n",
        "            transform,\n",
        "            original_y,\n",
        "            original_x,\n",
        "        )\n",
        "        lon, lat = to_latlon.transform(\n",
        "            x_proj,\n",
        "            y_proj,\n",
        "        )\n",
        "        return f\"{lon:.1f}°\"\n",
        "\n",
        "    def format_lat(\n",
        "        y,\n",
        "        pos,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Convert a pixel row index to a formatted Latitude string.\n",
        "        \"\"\"\n",
        "        y = np.clip(\n",
        "            y,\n",
        "            0,\n",
        "            height - 1,\n",
        "        )\n",
        "        # Multiply by scale_factor to map back to original pixels\n",
        "        original_y = y * scale_factor\n",
        "        original_x = (width // 2) * scale_factor\n",
        "\n",
        "        x_proj, y_proj = rasterio.transform.xy(\n",
        "            transform,\n",
        "            original_y,\n",
        "            original_x,\n",
        "        )\n",
        "        lon, lat = to_latlon.transform(\n",
        "            x_proj,\n",
        "            y_proj,\n",
        "        )\n",
        "        return f\"{lat:.1f}°\"\n",
        "\n",
        "    # Apply the custom formatters\n",
        "    ax.xaxis.set_major_formatter(\n",
        "        FuncFormatter(\n",
        "            format_lon,\n",
        "        ),\n",
        "    )\n",
        "    ax.yaxis.set_major_formatter(\n",
        "        FuncFormatter(\n",
        "            format_lat,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Limit ticks to avoid overcrowding\n",
        "    ax.xaxis.set_major_locator(\n",
        "        mticker.MaxNLocator(\n",
        "            nbins=4,\n",
        "        ),\n",
        "    )\n",
        "    ax.yaxis.set_major_locator(\n",
        "        mticker.MaxNLocator(\n",
        "            nbins=6,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Final styling for ticks\n",
        "    ax.tick_params(\n",
        "        axis=\"both\",\n",
        "        which=\"major\",\n",
        "        labelsize=7,\n",
        "        pad=4,\n",
        "    )\n",
        "    plt.setp(\n",
        "        ax.get_yticklabels(),\n",
        "        rotation=90,\n",
        "        va=\"center\",\n",
        "    )\n",
        "\n",
        "    output_figure_path = os.path.join(\n",
        "        output_dir,\n",
        "        \"map_alternation_exchange.png\",\n",
        "    )\n",
        "\n",
        "    plt.savefig(\n",
        "        output_figure_path,\n",
        "        dpi=300,\n",
        "        format=\"png\",\n",
        "        bbox_inches=\"tight\",\n",
        "        pad_inches=0.5,\n",
        "    )\n",
        "    plt.show()\n",
        "\n",
        "    print(\n",
        "        f\"Map successfully saved to: {output_figure_path}\",\n",
        "    )\n",
        "\n",
        "# Execution of the plotting function\n",
        "plot_alternation_exchange_discrete_map(\n",
        "    output_path,\n",
        "    nodata_val=noData_value,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EmAdmdlTcRX"
      },
      "source": [
        "### 9.3 Alternation Shift"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1enXQwjrT33k"
      },
      "source": [
        "#### 9.3.1 Compute Alternation Shift raster map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGhIUOMLjVMt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import rasterio\n",
        "from rasterio.windows import Window\n",
        "from tqdm import tqdm\n",
        "from numba import jit\n",
        "\n",
        "# --- Numba Optimized Function ---\n",
        "\n",
        "@jit(nopython=True)\n",
        "def _compute_shift_block_numba(\n",
        "    stack_data,\n",
        "    input_nodata,\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculate the Alternation Shift metric pixel-by-pixel using JIT compilation.\n",
        "\n",
        "    Shift is defined as: Total Changes - Net Extension - Total Exchange.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    stack_data : np.ndarray\n",
        "        3D array of shape (Height, Width, Time) containing pixel trajectories.\n",
        "    input_nodata : int or float\n",
        "        The NoData value present in the input rasters.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "        2D array (Height, Width) inheriting the input dtype containing the calculated shift values.\n",
        "    \"\"\"\n",
        "    rows, cols, time_steps = stack_data.shape\n",
        "\n",
        "    # Initialize output array with the user's NoData value\n",
        "    output_array = np.full(\n",
        "        (\n",
        "            rows,\n",
        "            cols,\n",
        "        ),\n",
        "        input_nodata,\n",
        "        dtype=stack_data.dtype,\n",
        "    )\n",
        "\n",
        "    # Pre-allocate buffer for unique classes to avoid dynamic allocation\n",
        "    classes_buffer = np.zeros(\n",
        "        50,\n",
        "        dtype=np.int16,\n",
        "    )\n",
        "\n",
        "    for r in range(rows):\n",
        "        for c in range(cols):\n",
        "            traj = stack_data[\n",
        "                r,\n",
        "                c,\n",
        "                :,\n",
        "            ]\n",
        "\n",
        "            # 1. Check for Input NoData (Strict Masking)\n",
        "            is_valid = True\n",
        "            for t in range(time_steps):\n",
        "                if traj[t] == input_nodata:\n",
        "                    is_valid = False\n",
        "                    break\n",
        "\n",
        "            if not is_valid:\n",
        "                continue\n",
        "\n",
        "            # 2. Identify unique classes manually\n",
        "            n_classes = 0\n",
        "            for t in range(time_steps):\n",
        "                val = traj[t]\n",
        "                found = False\n",
        "                for k in range(n_classes):\n",
        "                    if classes_buffer[k] == val:\n",
        "                        found = True\n",
        "                        break\n",
        "\n",
        "                if not found:\n",
        "                    if n_classes < 50:\n",
        "                        classes_buffer[n_classes] = val\n",
        "                        n_classes += 1\n",
        "\n",
        "            # If no change possible (0 or 1 class), Shift is 0\n",
        "            if n_classes <= 1:\n",
        "                output_array[\n",
        "                    r,\n",
        "                    c,\n",
        "                ] = 0\n",
        "                continue\n",
        "\n",
        "            # 3. Build Local Transition Matrix\n",
        "            mat = np.zeros(\n",
        "                (\n",
        "                    n_classes,\n",
        "                    n_classes,\n",
        "                ),\n",
        "                dtype=np.int16,\n",
        "            )\n",
        "\n",
        "            total_changes = 0\n",
        "\n",
        "            for t in range(time_steps - 1):\n",
        "                s_val = traj[t]\n",
        "                e_val = traj[t + 1]\n",
        "\n",
        "                if s_val == e_val:\n",
        "                    continue\n",
        "\n",
        "                # Increment total changes count directly\n",
        "                total_changes += 1\n",
        "\n",
        "                # Find indices in buffer\n",
        "                s_idx = -1\n",
        "                e_idx = -1\n",
        "                for k in range(n_classes):\n",
        "                    if classes_buffer[k] == s_val:\n",
        "                        s_idx = k\n",
        "                    if classes_buffer[k] == e_val:\n",
        "                        e_idx = k\n",
        "\n",
        "                if s_idx >= 0 and e_idx >= 0:\n",
        "                    mat[\n",
        "                        s_idx,\n",
        "                        e_idx,\n",
        "                    ] += 1\n",
        "\n",
        "            # 4. Calculate Net Extension Amount\n",
        "            # If start != end, extension is 1, else 0\n",
        "            extension_amount = 0\n",
        "            if traj[0] != traj[time_steps - 1]:\n",
        "                extension_amount = 1\n",
        "\n",
        "            # 5. Calculate Total Exchange: Sum(min(M, M.T))\n",
        "            total_exchange = 0\n",
        "            for i in range(n_classes):\n",
        "                for j in range(n_classes):\n",
        "                    v1 = mat[\n",
        "                        i,\n",
        "                        j,\n",
        "                    ]\n",
        "                    v2 = mat[\n",
        "                        j,\n",
        "                        i,\n",
        "                    ]\n",
        "                    if v1 < v2:\n",
        "                        total_exchange += v1\n",
        "                    else:\n",
        "                        total_exchange += v2\n",
        "\n",
        "            # 6. Calculate Scalar Shift\n",
        "            # Formula: Shift = Total Changes - Extension - Exchange\n",
        "            shift_val = total_changes - extension_amount - total_exchange\n",
        "\n",
        "            # Clamp negative values to 0 (safety)\n",
        "            if shift_val < 0:\n",
        "                shift_val = 0\n",
        "\n",
        "            output_array[\n",
        "                r,\n",
        "                c,\n",
        "            ] = shift_val\n",
        "\n",
        "    return output_array\n",
        "\n",
        "# --- Main Processing Function ---\n",
        "\n",
        "def generate_alternation_shift_map_optimized(\n",
        "    file_list,\n",
        "    output_dir,\n",
        "    input_nodata_val,\n",
        "    output_filename=\"map_alternation_shift.tif\",\n",
        "    block_size=1024,\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate the Alternation Shift map using Numba acceleration and\n",
        "    optimized I/O block processing (keeping files open).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    file_list : list of str\n",
        "        List of paths to the raster files (sorted by time).\n",
        "    output_dir : str\n",
        "        Directory to save the output raster.\n",
        "    input_nodata_val : int or float\n",
        "        The NoData value used in the input rasters.\n",
        "    output_filename : str, optional\n",
        "        Name of the output file (default is \"map_alternation_shift.tif\").\n",
        "    block_size : int, optional\n",
        "        Size of the processing window (default is 1024).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "        Saves the GeoTIFF to disk.\n",
        "    \"\"\"\n",
        "    output_path = os.path.join(\n",
        "        output_dir,\n",
        "        output_filename,\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"Starting optimized generation of Alternation Shift map: {output_path}\",\n",
        "    )\n",
        "\n",
        "    # 1. Open all datasets ONCE to avoid I/O overhead during the loop\n",
        "    srcs = [\n",
        "        rasterio.open(\n",
        "            path,\n",
        "        ) for path in file_list\n",
        "    ]\n",
        "\n",
        "    # 2. Read metadata from the first raster\n",
        "    height = srcs[0].height\n",
        "    width = srcs[0].width\n",
        "    profile = srcs[0].profile.copy()\n",
        "\n",
        "    # 3. Update profile for efficient output\n",
        "    # Notice we removed the hardcoded dtype to inherit from the input source\n",
        "    profile.update(\n",
        "        {\n",
        "            \"driver\": \"GTiff\",\n",
        "            \"count\": 1,\n",
        "            \"nodata\": input_nodata_val,\n",
        "            \"compress\": \"lzw\",\n",
        "            \"tiled\": True,\n",
        "            \"blockxsize\": 256,\n",
        "            \"blockysize\": 256,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # 4. Create output file and iterate blocks\n",
        "    with rasterio.open(\n",
        "        output_path,\n",
        "        \"w+\",\n",
        "        **profile,\n",
        "    ) as dst:\n",
        "\n",
        "        rows = range(\n",
        "            0,\n",
        "            height,\n",
        "            block_size,\n",
        "        )\n",
        "        cols = range(\n",
        "            0,\n",
        "            width,\n",
        "            block_size,\n",
        "        )\n",
        "        total_blocks = len(rows) * len(cols)\n",
        "\n",
        "        with tqdm(\n",
        "            total=total_blocks,\n",
        "            desc=\"Processing Shift Blocks\",\n",
        "            unit=\"block\",\n",
        "        ) as pbar:\n",
        "\n",
        "            for r_off in rows:\n",
        "                for c_off in cols:\n",
        "                    # Define current window\n",
        "                    window = Window(\n",
        "                        c_off,\n",
        "                        r_off,\n",
        "                        min(\n",
        "                            block_size,\n",
        "                            width - c_off,\n",
        "                        ),\n",
        "                        min(\n",
        "                            block_size,\n",
        "                            height - r_off,\n",
        "                        ),\n",
        "                    )\n",
        "\n",
        "                    # 5. Load stack for this window only from ALREADY OPEN datasets\n",
        "                    stack_list = []\n",
        "\n",
        "                    for src in srcs:\n",
        "                        data = src.read(\n",
        "                            1,\n",
        "                            window=window,\n",
        "                        )\n",
        "                        stack_list.append(\n",
        "                            data,\n",
        "                        )\n",
        "\n",
        "                    # 6. Prepare data for Numba (Height, Width, Time)\n",
        "                    # moveaxis ensures time is the last dimension for contiguous memory access\n",
        "                    stack_data = np.array(\n",
        "                        stack_list,\n",
        "                    )\n",
        "                    stack_data = np.moveaxis(\n",
        "                        stack_data,\n",
        "                        0,\n",
        "                        -1,\n",
        "                    )\n",
        "\n",
        "                    # 7. Execute Numba Optimized Computation\n",
        "                    output_block = _compute_shift_block_numba(\n",
        "                        stack_data,\n",
        "                        input_nodata_val,\n",
        "                    )\n",
        "\n",
        "                    # 8. Write result to disk\n",
        "                    dst.write(\n",
        "                        output_block,\n",
        "                        1,\n",
        "                        window=window,\n",
        "                    )\n",
        "\n",
        "                    pbar.update(1)\n",
        "\n",
        "    # 9. Close all datasets\n",
        "    for src in srcs:\n",
        "        src.close()\n",
        "\n",
        "    print(\n",
        "        f\"Process completed successfully. Shift Map saved to: {output_path}\",\n",
        "    )\n",
        "\n",
        "# Execution using the optimized function\n",
        "\n",
        "generate_alternation_shift_map_optimized(\n",
        "    file_list=image_paths,\n",
        "    output_dir=output_path,\n",
        "    input_nodata_val=noData_value,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOgV9ey1y_Oz"
      },
      "source": [
        "#### 9.3.2 Plot Alternation Shift Map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnsk7jlgy_Oz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mticker\n",
        "import rasterio\n",
        "from rasterio.enums import Resampling\n",
        "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
        "from matplotlib.patches import Patch\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "from matplotlib_scalebar.scalebar import ScaleBar\n",
        "from pyproj import Transformer\n",
        "\n",
        "def plot_alternation_shift_map(\n",
        "    output_dir: str,\n",
        "    nodata_val: int,\n",
        "    raster_filename: str = \"map_alternation_shift.tif\",\n",
        "    max_display_size: int = 1500, # Added dynamic downsampling parameter\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Plot the Alternation Shift map using a discrete integer scale.\n",
        "\n",
        "    The visualization uses a specific color logic where the value 0 is\n",
        "    represented in gray (indicating stability or absence of shift),\n",
        "    while values greater than or equal to 1 are represented by a\n",
        "    gradient color scale (e.g., from green to dark green) to indicate\n",
        "    the intensity of the shift.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    output_dir : str\n",
        "        The directory path containing the input raster file and where\n",
        "        the output image will be saved.\n",
        "    nodata_val : int\n",
        "        Value representing NoData pixels to be masked.\n",
        "    raster_filename : str, optional\n",
        "        The filename of the raster to be plotted (default is\n",
        "        \"map_alternation_shift.tif\").\n",
        "    max_display_size : int, optional\n",
        "        Maximum width/height in pixels for the generated plot to prevent\n",
        "        RAM overflow. Default is 1500.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "        The function saves a PNG image to the output directory and\n",
        "        displays the plot.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    FileNotFoundError\n",
        "        If the specified raster file does not exist in the output directory.\n",
        "    \"\"\"\n",
        "    # 1. Input Validation and Path Setup\n",
        "    raster_path = os.path.join(\n",
        "        output_dir,\n",
        "        raster_filename,\n",
        "    )\n",
        "\n",
        "    if not os.path.exists(\n",
        "        raster_path,\n",
        "    ):\n",
        "        raise FileNotFoundError(\n",
        "            f\"Raster not found: {raster_path}\",\n",
        "        )\n",
        "\n",
        "    print(\n",
        "        f\"Reading Alternation Shift map for plotting: {raster_path}\",\n",
        "    )\n",
        "\n",
        "    # Calculate dynamic scale factor based on max display size\n",
        "    with rasterio.open(raster_path) as src:\n",
        "        max_dim = max(src.width, src.height)\n",
        "        scale_factor = max(1.0, max_dim / max_display_size)\n",
        "\n",
        "        src_crs = src.crs\n",
        "        transform = src.transform\n",
        "\n",
        "    # Get the exact pixel size in km for the scalebar using the scale_factor\n",
        "    pixel_size_km = compute_display_pixel_size_km(\n",
        "        raster_path=raster_path,\n",
        "        downsample_divisor=scale_factor,\n",
        "    )\n",
        "\n",
        "    # 2. Data Loading and Masking with Downsampling\n",
        "    with rasterio.open(\n",
        "        raster_path,\n",
        "    ) as src:\n",
        "\n",
        "        # Calculate new shape safely\n",
        "        new_height = max(1, int(src.height / scale_factor))\n",
        "        new_width = max(1, int(src.width / scale_factor))\n",
        "\n",
        "        data = src.read(\n",
        "            1,\n",
        "            out_shape=(\n",
        "                new_height,\n",
        "                new_width,\n",
        "            ),\n",
        "            resampling=Resampling.nearest,\n",
        "        )\n",
        "\n",
        "        # Force masking using the provided variable\n",
        "        data_masked = np.ma.masked_equal(\n",
        "            data,\n",
        "            nodata_val,\n",
        "        )\n",
        "\n",
        "    # Determine the integer range for the color scale\n",
        "    data_max = int(\n",
        "        data_masked.max(),\n",
        "    )\n",
        "\n",
        "    # Ensure at least 1 to prevent errors if the map is empty/flat\n",
        "    if data_max == 0:\n",
        "        data_max = 1\n",
        "\n",
        "    # 3. Discrete Colormap Configuration\n",
        "    original_cmap = plt.get_cmap(\"viridis\")\n",
        "\n",
        "    colors_list = [\"#c0c0c0\"] + [\n",
        "        original_cmap(i) for i in np.linspace(0, 1, data_max)\n",
        "    ]\n",
        "\n",
        "    cmap = ListedColormap(\n",
        "        colors_list,\n",
        "    )\n",
        "\n",
        "    # 4. Define Boundaries (Bins) for Integers\n",
        "    # Create boundaries for every integer from 0 to max + 1\n",
        "    # Adding -0.5 centers the integer values perfectly inside the color bins\n",
        "    bounds = np.arange(\n",
        "        0,\n",
        "        data_max + 2,\n",
        "    ) - 0.5\n",
        "\n",
        "    norm = BoundaryNorm(\n",
        "        bounds,\n",
        "        cmap.N,\n",
        "    )\n",
        "\n",
        "    # 5. Plotting the Figure\n",
        "    fig, ax = plt.subplots(\n",
        "        figsize=(\n",
        "            6,\n",
        "            8,\n",
        "        ),\n",
        "        dpi=300,\n",
        "    )\n",
        "\n",
        "    im = ax.imshow(\n",
        "        data_masked,\n",
        "        cmap=cmap,\n",
        "        norm=norm,\n",
        "        interpolation=\"nearest\",\n",
        "    )\n",
        "\n",
        "    # 6. Legend Configuration\n",
        "    legend_elements = []\n",
        "    # Loop from min to max to create a box for each integer\n",
        "    for i in range(0, data_max + 1):\n",
        "        legend_elements.append(\n",
        "            Patch(\n",
        "                facecolor=cmap(norm(i)),\n",
        "                edgecolor=\"black\",\n",
        "                linewidth=0.5,\n",
        "                label=str(i),\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    ax.legend(\n",
        "        handles=legend_elements,\n",
        "        title=\"Shift\",\n",
        "        loc=\"center left\",\n",
        "        bbox_to_anchor=(1.02, 0.5),\n",
        "        frameon=False,\n",
        "        fontsize=8,\n",
        "        title_fontsize=10,\n",
        "        alignment=\"left\",\n",
        "    )\n",
        "\n",
        "    # 7. Cartographic Elements\n",
        "    scalebar = ScaleBar(\n",
        "        dx=pixel_size_km,\n",
        "        units=\"km\",\n",
        "        length_fraction=0.35,\n",
        "        location=\"lower right\",\n",
        "        box_alpha=0.0,\n",
        "        scale_formatter=lambda value, _: f\"{int(value)} km\",\n",
        "    )\n",
        "    ax.add_artist(\n",
        "        scalebar,\n",
        "    )\n",
        "\n",
        "    # 8. North Arrow\n",
        "    try:\n",
        "        north_arrow(\n",
        "            ax,\n",
        "            location=\"upper left\",\n",
        "            shadow=False,\n",
        "            rotation={\n",
        "                \"degrees\": 0,\n",
        "            },\n",
        "        )\n",
        "    except NameError:\n",
        "        print(\"Note: north_arrow function not found, skipping.\")\n",
        "\n",
        "    # 9) Axes styling (with Lat/Lon labels)\n",
        "    ax.set_title(\n",
        "        \"Shift\",\n",
        "        fontsize=18,\n",
        "        pad=5,\n",
        "    )\n",
        "    ax.set_aspect(\n",
        "        \"equal\",\n",
        "    )\n",
        "\n",
        "    # Initialize Transformer (Native CRS -> Lat/Lon)\n",
        "    to_latlon = Transformer.from_crs(\n",
        "        src_crs,\n",
        "        \"EPSG:4326\",\n",
        "        always_xy=True,\n",
        "    )\n",
        "\n",
        "    # Get dimensions from the loaded DOWNSAMPLED data\n",
        "    height, width = data_masked.shape\n",
        "\n",
        "    def format_lon(\n",
        "        x,\n",
        "        pos,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Convert a pixel column index to a formatted Longitude string.\n",
        "        \"\"\"\n",
        "        x = np.clip(\n",
        "            x,\n",
        "            0,\n",
        "            width - 1,\n",
        "        )\n",
        "        # Multiply by scale_factor to map back to original pixels\n",
        "        original_x = x * scale_factor\n",
        "        original_y = (height // 2) * scale_factor\n",
        "\n",
        "        x_proj, y_proj = rasterio.transform.xy(\n",
        "            transform,\n",
        "            original_y,\n",
        "            original_x,\n",
        "        )\n",
        "        lon, lat = to_latlon.transform(\n",
        "            x_proj,\n",
        "            y_proj,\n",
        "        )\n",
        "        return f\"{lon:.1f}°\"\n",
        "\n",
        "    def format_lat(\n",
        "        y,\n",
        "        pos,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Convert a pixel row index to a formatted Latitude string.\n",
        "        \"\"\"\n",
        "        y = np.clip(\n",
        "            y,\n",
        "            0,\n",
        "            height - 1,\n",
        "        )\n",
        "        # Multiply by scale_factor to map back to original pixels\n",
        "        original_y = y * scale_factor\n",
        "        original_x = (width // 2) * scale_factor\n",
        "\n",
        "        x_proj, y_proj = rasterio.transform.xy(\n",
        "            transform,\n",
        "            original_y,\n",
        "            original_x,\n",
        "        )\n",
        "        lon, lat = to_latlon.transform(\n",
        "            x_proj,\n",
        "            y_proj,\n",
        "        )\n",
        "        return f\"{lat:.1f}°\"\n",
        "\n",
        "    # Apply the custom formatters\n",
        "    ax.xaxis.set_major_formatter(\n",
        "        FuncFormatter(\n",
        "            format_lon,\n",
        "        ),\n",
        "    )\n",
        "    ax.yaxis.set_major_formatter(\n",
        "        FuncFormatter(\n",
        "            format_lat,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Limit ticks to avoid overcrowding\n",
        "    ax.xaxis.set_major_locator(\n",
        "        mticker.MaxNLocator(\n",
        "            nbins=4,\n",
        "        ),\n",
        "    )\n",
        "    ax.yaxis.set_major_locator(\n",
        "        mticker.MaxNLocator(\n",
        "            nbins=6,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Final styling for ticks\n",
        "    ax.tick_params(\n",
        "        axis=\"both\",\n",
        "        which=\"major\",\n",
        "        labelsize=7,\n",
        "        pad=4,\n",
        "    )\n",
        "    plt.setp(\n",
        "        ax.get_yticklabels(),\n",
        "        rotation=90,\n",
        "        va=\"center\",\n",
        "    )\n",
        "\n",
        "    output_figure_path = os.path.join(\n",
        "        output_dir,\n",
        "        \"map_alternation_shift.png\",\n",
        "    )\n",
        "\n",
        "    plt.savefig(\n",
        "        output_figure_path,\n",
        "        dpi=300,\n",
        "        format=\"png\",\n",
        "        bbox_inches=\"tight\",\n",
        "        pad_inches=0.5,\n",
        "    )\n",
        "    plt.show()\n",
        "\n",
        "    print(\n",
        "        f\"Map saved successfully: {output_figure_path}\",\n",
        "    )\n",
        "\n",
        "# Execution of the plotting function\n",
        "plot_alternation_shift_map(\n",
        "    output_dir=output_path,\n",
        "    nodata_val=noData_value,\n",
        "    raster_filename=\"map_alternation_shift.tif\",\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}